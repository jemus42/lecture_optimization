\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-mbo}

\newcommand{\titlefigure}{figure_man/noisy_2.png}
\newcommand{\learninggoals}{
\item Noisy surrogate modeling
\item Noisy acquisition functions
\item Final best point
}

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Bayesian Optimization:\\ Noisy Evaluations}
\lecture{Optimization in Machine Learning}


\begin{vbframe}{Noisy Evaluations}

In many real-life applications, we cannot access the true function values $f(\xv)$ but only a \textbf{noisy} version thereof

$$
  f(\xv) + \epsilon. 
$$

For the sake of simplicity, we assume $\epsilon \sim \mathcal{N}\left(0, \sigma_\epsilon^2\right)$.

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/noisy_0.png}
\end{center}

\framebreak 


This raises the following problems: 

\begin{itemize}
  \item \textbf{Surrogate modeling:} So far we used an interpolating GP that is based on noise-free observations; as a consequence, the variance is modeled as $0$
  $$
    s^2(\xvsi) = 0
  $$
  for design points $(\xvsi, \ysi) \in \Dt$. This is problematic. 
  \item \textbf{Acquisition functions:} Most acquisition functions are based on the best observed value $\fmin$ so far. If evaluations are noisy, we do not know this value (it is a random variable).
  \item \textbf{Final best point:} The design point evaluated best is not necessarily the true best point in design (overestimation). 
\end{itemize}


\end{vbframe}

\begin{vbframe}{Surrogate Model}

In case of noisy evaluations, a nugget-effect GP (GP regression) should be used instead of an interpolating GP:

\vspace{+0.45cm}

\begin{minipage}[b]{0.45\textwidth}
  \includegraphics[width = \textwidth]{figure_man/noisy_1.png}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
  \includegraphics[width = \textwidth]{figure_man/noisy_2.png}
\end{minipage}

\end{vbframe}

\begin{vbframe}{Noisy Acquisition Functions: AEI}


\textbf{Augmented Expected Improvement} (\emph{Huang et al., 2006})
$$
  a_{\text{AEI}}(\xv) = a_{\text{EI}_{f_{\min_*}}}(\xv) \Bigg(1- \frac{\sigma_\epsilon}{\sqrt{\sh^2(\xv)+ \sigma_\epsilon^2}}\Bigg).
$$

Here, $a_{\text{EI}_{f_{\min_*}}}$ denotes the \textbf{Expected Improvement with Plugin}.
It uses the \textbf{effective best solution} as a plugin for the (unknown) best observed value $\fmin$

$$
  f_{\min_*} = \min_{\xv \in \{\xvsi[1], ..., \xvsi[t]\}} \fh(\xv) + c \sh(\xv),
$$

where $c > 0$ is a constant that controls the risk aversion.


\framebreak

In addition, it takes into account the nugget effect $\sigma_\epsilon$ by a penalty term

$$
  \Bigg(1- \frac{\sigma_\epsilon}{\sqrt{\sh^2(\xv)+ \sigma_\epsilon^2}}\Bigg)
$$

\begin{itemize}
  \item The penalty is justified to \enquote{account for the diminishing return of additional replicates as the predictions become more accurate} (\emph{Huang et al., 2006})
  \item Designs with small predictive variance $\sh^2(\xv)$ are penalized in favor of more exploration.
  \item If $\sigma_\epsilon^2 = 0$ (noise-free case), the AEI corresponds to the EI with plugin. 
\end{itemize}

\framebreak

\vspace*{1cm}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/noisy_3.png}
\end{center}


\end{vbframe}

\begin{vbframe}{Reinterpolation}

Clean noise from the model and then apply a general acquisition function (EI, PI, LCB, ...)

\lz 

The RP suggests to build \textbf{two models}: a nugget-effect GP (regression model) and then, on the predictions from the first model, an interpolating GP.

\begin{algorithm}[H]
\footnotesize
  \caption{Reinterpolation Procedure}
  \begin{algorithmic}[1]
  \State{Build a nugget-effect GP model based on noisy evaluations}
  \State{Compute predictions for all points in the design $\fh(\xvsi[1]), \ldots, \fh(\xvsi[t])$}
  \State{Train an interpolating GP on $\left\{\left(\xvsi[1], \fh(\xvsi[1])\right), \ldots, \left(\xvsi[t], \fh(\xvsi[t])\right)\right\}$}
  \State{Based on the interpolating model, obtain a new candidate using a noise-free acquisition function}
  \end{algorithmic}
\end{algorithm}

\framebreak

Build a nugget-effect GP on the noisy evaluations

\vspace*{0.5cm}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/noisy_2.png}
\end{center}

\framebreak

Obtain predictions from the nugget-effect GP (grey), fit an interpolating GP (blue) and proceed with EI

\vspace*{0.5cm}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/noisy_4.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Identification of final best point}

Another problem is the identification of a final best point: 

\begin{itemize}
  \item Assume that all evaluations are noisy
  \item The probability is high that \textbf{by chance}
  \begin{itemize}
    \item bad points get overrated 
    \item good points get overlooked
  \end{itemize}
\end{itemize}


\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/noisy_5.png}
\end{center}

\framebreak 

Possibilities to reduce the risk of falsely returning a bad point: 
\begin{itemize}
  \item Return the best predicted point $\argmin_{\xv \in \{\xvsi[1], \ldots, \xvsi[t]\}} \fh(\xv)$
  \item Repeated evaluations of the final point: infer guarantees about final point (however if final point is \enquote{bad} unclear how to find a better one)
  \item Repeated evaluations of all design points: reduce noise during optimization and risk of falsely returning a bad point
  \item More advanced replication strategies, e.g. incumbent strategies: also re-evaluate the \enquote{incumbent} in each iteration
\end{itemize}

\end{vbframe}


\endlecture
\end{document}
