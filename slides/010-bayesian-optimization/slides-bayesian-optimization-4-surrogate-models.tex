\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-mbo}

\newcommand{\titlefigure}{figure_man/surrogate_0.png}
\newcommand{\learninggoals}{
\item Gaussian process vs. random forest
}

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Bayesian Optimization:\\ Types of Surrogate Models}
\lecture{Optimization in Machine Learning}

\begin{frame}{Surrogate Models}

Desiderata:

\begin{itemize}
  \item Regression model with uncertainty estimates
  \item Accurate predictions (especially for small sample size)
  \item Robust
\end{itemize}

\vspace{+0.45cm}

Depending on the application:

\begin{itemize}
  \item Can handle different types of inputs (categorical and continuous)
  \item Can handle dependencies (i.e., hierarchical input)
\end{itemize}

\end{frame}


\begin{vbframe}{Gaussian Process}
The posterior mean and the posterior variance for a GP can be derived analytically: 

\begin{itemize}
  \item Let $\Dts$ be the data we are fitting the GP on
  \item Let $\bm{y}:= \left(\ysi[1], \ldots, \ysi[t]\right)$ be the vector of observed outputs
  \item For a covariance kernel $k(\xv, \xv^{'})$, let $\bm{K} := \left(k(\xvsi, \xvsi[j])\right)_{i,j}$ denote the \textbf{kernel (Gram) matrix} and $k(\xv) \coloneqq \left(k(\xv, \xv^{[1]}), \ldots, k(\xv, \xvsi[t])\right)^\top$
  \item Example: radial basis function kernel (also known as Gauss kernel): $k(\xv, \xv^{'})=\exp (-\frac{||\xv - \xv^{'}||^2}{2 \sigma^2})$
  \item Further, we assume a zero-mean GP prior $^{(*)}$  
\end{itemize}

\vfill

\begin{footnotesize}
 $^{(*)}$ It is common but by no means necessary to consider GPs with a zero mean function. We could also model a GP with a constant trend, for example. Note, however, that using zero-mean priors is not a drastic limitation, since the mean of the posterior process is not confined to be zero. 
\end{footnotesize}

\framebreak 

The posterior predictive distribution for a new test point $\xv \in \mathcal{S}$ under a GP is

\begin{eqnarray*}
  Y(\xv) ~|~ \xv, \Dt \sim \mathcal{N}\left(\fh(\xv), \sh^2(\xv)\right)
\end{eqnarray*}

with 

\begin{eqnarray*}
  \fh(\xv) &=& k(\xv)^\top \bm{K}^{-1} \bm{y} \\
  \sh^2(\xv) &=& k(\xv, \xv) - k(\xv)^\top \bm{K}^{-1} k(\xv)
\end{eqnarray*}

\vfill

\framebreak

Pros:
\begin{itemize}
  \item GPs yield well calibrated uncertainty estimates
  \item The posterior predictive distribution under a GP is normal
\end{itemize}

\vspace{+0.45cm}

\begin{minipage}[b]{0.45\textwidth}
  \includegraphics[width = \textwidth]{figure_man/surrogate_0.png}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
  \includegraphics[width = \textwidth]{figure_man/surrogate_1.png}
\end{minipage}

\framebreak

Cons:
\begin{itemize}
  \item Vanilla GPs scale cubic in the number of data points
  \item GPs can natively only handle numeric features\\
    Categorical features and dependencies require special handling via custom kernel
  \item GPs aren't that robust\\
    In practice, \enquote{white-noise} models can occur where the posterior mean and variance is constant (except for the interpolation of training points)
  \item Performance can be sensitive to the choice of kernel and hyperparameters
\end{itemize}

\end{vbframe}

\begin{frame}{Random Forest}

\begin{itemize}
    \item Bagging ensemble
    \item Fit $B$ decision trees on bootstrap samples using different features sampled at random
\end{itemize}

\begin{center}
  \includegraphics[width = 0.7\textwidth]{slides/010-bayesian-optimization/figure_man/random_forests.jpg}
\end{center}

\end{frame}

\begin{frame}{Random Forest}

\begin{itemize}
    \item Let $\mu_b: \mathcal{S} \to \R$ be the mean prediction of a decision tree $b$ (mean of all data points in the same node as observation $\xv \in \mathcal{S}$)
    \item Let $\sigma_{b}^2: \mathcal{S} \to \R$ be the variance prediction (variance of all data points in the same node as observation $\xv \in \mathcal{S})$
    \item Mean prediction of the forest: $\mu: \mathcal{S} \to \R$,\\
    $\xv \mapsto \frac{1}{B} \sum_{b = 1}^{B} \mu_{b}(\xv)$
    \item Variance prediction of the forest: $\sigma^{2}: \mathcal{S} \to \R$,\\
    $\xv \mapsto  \left( \frac{1}{B} \sum_{b = 1}^{B} \sigma_{b}^2(\xv) + \mu_{b}(\xv)^{2} \right) - \mu(\xv)^{2}$\\
    (law of total variance assuming a mixture of $B$ models;\\
    other variance estimates also exist)
\end{itemize}

\end{frame}


\begin{frame}{Random Forest - Different Choices}

% we always use all data (fraction = 1)
% no bootstrap means we do subsampling (i.e., simply use all data once)
% no random split means we split by variance criterion
% random split means we choose the split point in a feature uniformly at random between feature bounds
\begin{center}
  \includegraphics[width = 0.9\textwidth]{figure_man/surrogate_2.png}\\
\end{center}

\end{frame}

\begin{frame}{Random Forest}
Pros:
\begin{itemize}
 \item Cheap to train
 \item Scales well with the number of data points
 \item Scales well with the number of dimensions
 \item Can easily handle hierarchical mixed spaces
 \item Robust
\end{itemize}
\end{frame}

\begin{frame}{Random Forest}
Cons:
\begin{itemize}
  \item Poor uncertainty estimates
  \item Not really Bayesian (no real posterior predictive distribution)
  \item Poor extrapolation
\end{itemize}
\end{frame}

\endlecture
\end{document}

