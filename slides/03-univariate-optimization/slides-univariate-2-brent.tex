\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/quadratic-title.png}
\newcommand{\learninggoals}{
\item Quadratic interpolation
\item Brent's procedure
}

%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Univariate optimization: Brent's method}
\lecture{Optimization in Machine Learning}
\sloppy



\begin{frame}{Quadratic Interpolation}

Similar to golden ratio procedure. Define $x^{\text{new}}$ as minimum of a parabola fitted through $(x^{\text{left}}, f^{\text{left}})$, $(x^{\text{best}}, f^{\text{best}})$, $(x^{\text{right}}, f^{\text{right}})$. 

\foreach \i in {0, 1, 2, 3, 4}{
  \only<\i>{
  \vspace*{1cm}
  \begin{center}
  \includegraphics{figure_man/quadratic\i.pdf} 
  \end{center}
  }
}

\end{frame}


%Code f체r Erzeugung der Bilder in Ordner figure_man/quadr_int f체r animation
% <<echo = FALSE>>=
% min_func = function(x) cos(x) + 3 *  x^2 + 3*exp(x)
%
% plot_quad = function(minfunc, left = -2.5, right = 2.5, best = -1, max_it = 3, ...) {
%   l = left
%   r = right
%
%   parab = function(x) A * x^2 + B * x + C
%
%   iter = 1
%   curve(minfunc, from = l-0.5, to = r+0.5, ylab = "f(x)", ...)
%   abline(v = c(left, best, right), lty = 2)
%   points(c(left, best, right), minfunc(c(left, best, right)), pch = 15)
%   axis(3, at = c(left, best, right),
%          labels = c(expression("x"[left]), expression("x"[best]),expression("x"[right])), las = 3, cex.axis = 1.5)
%   while (iter <= max_it + 1) {
%     curve(minfunc, from = l-0.5, to = r+0.5, ylab = "f(x)", ...)
%     fl = min_func(left)
%     fr = min_func(right)
%     fb = min_func(best)
%
%     denom = (left - best) * (left - right) * (best - right)
%     A = (right * (fb - fl) + best * (fl - fr) + left * (fr - fb)) / denom
%     B = (right^2 * (fl - fb) + best^2 * (fr - fl) + left^2 * (fb - fr)) / denom
%     C = (best * right * (best - right) * fl + right * left * (right - left) * fb +
%            left * best * (left - best) * fr) / denom
%     xnew = - B / (2 * A)
%
%     points(seq(l - 0.5, r + 0.5, length.out=100),
%            parab(seq(l - 0.5, r + 0.5, length.out=100)), type = "l", lty = 2)
%     abline(v = c(left, best, xnew, right), lty = 2)
%     points(c(left, best, xnew, right), minfunc(c(left, best, xnew, right)),
%            pch = 15)
%     axis(3, at = c(left, best, xnew, right),
%          labels=c(expression("x"[left]), expression("x"[best]),
%                   expression("x"[new]), expression("x"[right])), las = 3, cex.axis = 1.5)
%
%     if (minfunc(xnew) < minfunc(best)) {
%       store = xnew
%       xnew = best
%       best = store
%     }
%     if (best < xnew) {right = xnew}
%     if (best > xnew) {left = xnew}
%
%     curve(minfunc, from = l-0.5, to = r+0.5, ylab = "f(x)", ...)
%     abline(v = c(left, best, right), lty = 2)
%     points(c(left, best, right), minfunc(c(left, best, right)),
%            pch = 15)
%     axis(3, at = c(left, best, right),
%          labels=c(expression("x'"[left]), expression("x'"[best]),
%                   expression("x'"[right])), las = 3, cex.axis = 1.5)
%     iter = iter + 1
%   }
% }
% plot_quad(min_func, l = -2.5, r = 2.5, best = 0, max_it = 3, ylim = c(-10, 90))
% @

% \begin{frame}[t,fragile]{Quadratische Interpolation Beispiel}
% \begin{center}
%   \only<1>{\includegraphics[width=0.8\textwidth]{figure_man/quadr_int/pic-1.png}}
%   \only<2>{\includegraphics[width=0.8\textwidth]{figure_man/quadr_int/pic-2.png}}
%   \only<3>{\includegraphics[width=0.8\textwidth]{figure_man/quadr_int/pic-3.png}}
%   \only<4>{\includegraphics[width=0.8\textwidth]{figure_man/quadr_int/pic-4.png}}
%   \only<5>{\includegraphics[width=0.8\textwidth]{figure_man/quadr_int/pic-5.png}}
% \end{center}
%
%   % \animategraphics[poster=first,autoplay,loop,width=0.65\textwidth]{1}{figure_man/quadr_int/pic-}{1}{9}
% \end{frame}


\begin{vbframe}{Quadratic interpolation comments}
\begin{itemize}
\item Quadratic interpolation \textbf{not robust}. The following may happen:
\begin{itemize}
\item Algorithm suggests the same $x^{\text{new}}$ in each step,
\item $x^{\text{new}}$ outside of search interval,
\item Parabola degenerates to line and no real minimum exists (if 3 points are collinear).
\end{itemize}
\item Algorithm must then abort, finding a global minimum is not guaranteed.
\item In practice, extensively tested numerical libraries should be used.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Brent's method}

In practice a combination of golden ratio and quadratic interpolation is often used, as in the R-function \texttt{optimize()}.

\lz

The fast convergence of the quadratic interpolation is exploited, unstable calculation steps (e.g. parabola degenerated) are stabilized by golden ratio search.

\lz

If the function $f$ has a local minimum, then convergence is guaranteed. 

% \begin{itemize}
% \item Zun채chst wird Suchintervall durch Goldenen Schnitt solange verkleinert, bis Zielfunktion durch Parabel an den letzten 3 Evaluationspunkten gut approximiert wird.
% \item Dann weitere Suche durch Quadratische Interpolation, Ausnutzen der schnelleren Konvergenz.
% \item Bei fr체hzeitiger Terminierung, erneut Goldener Schnitt.
% \end{itemize}
% Dieses Verfahren wird \textbf{Brent'sche Methode} genannt. Funktioniert sehr gut, da sich alle (zweimal stetig diff'baren) Funktionen in einer $\epsilon$ Umgebung um das globale Minimum gut durch eine quadratische Funktion approximieren lassen (vgl. Taylorreihe).
\end{vbframe}


\begin{vbframe}{Example: ML Cauchy}


\begin{columns}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Density Poisson distribution:
\begin{footnotesize}
$$
f(k ~|~ \lambda) := \P(x = k) = \frac{\lambda^k \cdot \exp(-\lambda)}{k!}
$$
\end{footnotesize}
\item Negative log-Likelihood for $n$ observations:
\begin{footnotesize}
\begin{eqnarray*}
- \ell(\lambda, \mathcal{D}) &=& - \log L(\lambda, \mathcal{D}) \\ &=& - \log \prod_{i = 1}^n  f\left(x^{(i)} ~|~ \lambda\right) \\ &=&  - \sum_{i = 1}^n \frac{\lambda^{x^{(i)} } \cdot \exp(-\lambda)}{x^{(i)} !} 
\end{eqnarray*}
\end{footnotesize}

\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}
  \includegraphics[width = 1\textwidth]{figure_man/poisson.pdf}
\end{figure}
\end{column}
\end{columns}



\framebreak 

Both Golden ratio and Brent's method converge against the global optimum at $x^\ast = 0.75$. While the Golden ratio method needs $\approx 93$ iterations, Brent's method only needs $\approx 18$ for the same level of precision. 

\lz
\footnotesize
\begin{verbatim}
library("Rmpfr")
set.seed(1234)

x = rpois(200, lambda = 0.75)

neg_log_lik = function(lambda)- sum(log(dpois(x, lambda)))

optimize(neg_log_lik, lower = 0, upper = 1, trace = TRUE, 
  method = "Brent")

optimize(neg_log_lik, lower = 0, upper = 1, trace = TRUE, 
  method = "GoldenRatio")

\end{verbatim}


%<<warning = FALSE, eval = TRUE, echo = FALSE, message = FALSE, size = "tiny">>=
%library("Rmpfr")
%set.seed(1234)

%x = rpois(200, lambda = 0.75)

%neg_log_lik = function(lambda)- sum(log(dpois(x, lambda)))

%optimizeR(neg_log_lik, lower = 0, upper = 1, trace = TRUE, 
% method = "Brent")

%@

\framebreak 

%<<warning = FALSE, eval = TRUE, echo = FALSE, message = FALSE, size = "tiny">>=
%library("Rmpfr")
%set.seed(1234)

%x = rpois(200, lambda = 0.75)

%neg_log_lik = function(lambda)- sum(log(dpois(x, lambda)))

%optimizeR(neg_log_lik, lower = 0, upper = 1, trace = TRUE, 
% method = "GoldenRatio")

%@

\vspace*{-1cm}

\begin{columns}
\begin{column}{1\textwidth}
\begin{verbatim}
## $minimum
## 1 'mpfr' number of precision 132 bits
## [1] 0.73999999999999999995826058999834279020219
##
## $objective
## 1 'mpfr' number of precision 132 bits
## [1] 228.79065719131130788950898504582093404577
##
## $iter
## [1] 18
##
## $convergence
## [1] TRUE
##
## $estim.prec
## 1 'mpfr' number of precision 132 bits
## [1] 1.336469988199522593390843736470812993265e-20
##
## $method
## [1] "Brent"
\end{verbatim}
\end{column}
\end{columns}

\framebreak

\vspace*{-1cm}

\begin{columns}
\begin{column}{1\textwidth}
\begin{verbatim}
## $minimum
## 1 'mpfr' number of precision 132 bits
## [1] 0.74000000000000000006766164468351954364332
##
## $objective
## 1 'mpfr' number of precision 132 bits
## [1] 228.79065719131130788950898504582093404755
##
## $iter
## [1] 93
##
## $convergence
## [1] TRUE
##
## $estim.prec
## 1 'mpfr' number of precision 132 bits
## [1] 8.653395152839838611457066694127631372854e-21
##
## $method
## [1] "GoldenRatio"
\end{verbatim}

\end{column}
\end{columns}

\end{vbframe}
\endlecture

\end{document}


