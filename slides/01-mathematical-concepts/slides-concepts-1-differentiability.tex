
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/differentiability_multivariate.png}
\newcommand{\learninggoals}{
\item Definition of Smoothness
\item Uni- \& Multivariate Differentiation}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization}
\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Smoothness \& Gradients}
\lecture{Optimization}
\sloppy

% ------------------------------------------------------------------------------

\begin{vbframe}{Smooth vs. non-smooth}

\begin{itemize}
\item The \textbf{smoothness} of a function is a property that is measured by the number of its continuous derivatives. 
\item We call a function $k$-times continuously differentiable, if the $k$-th derivative exists and is continuous. $\mathcal{C}^k$ denotes the class of $k$-times continuously differentiable functions.
\item In this lecture, we will call a function $f: \mathcal{S} \to \R$ \enquote{smooth}, if it is at least one time differentiable, i.e. $f \in \mathcal{C}^k$ with $k \geq$ 1. 
 
\end{itemize}


\begin{center}
\includegraphics[width = 0.4\textwidth]{figure_man/hinge_vs_l2.pdf} \\ 
\begin{footnotesize}
$f_1$ is smooth, $f_2$ is continuous but not differentiable, and $f_3$ is non-continuous. 
\end{footnotesize}
\end{center}


\end{vbframe}

\begin{vbframe}{Differentiability (univariate)}

\textbf{Definition 1:} A function $f: \mathcal{S} \subseteq \R \to \R$ is said to be differentiable in $x \in \mathcal{S}$ if the following limit exists:

$$
f'(x) := \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$

A function $f: \mathcal{S} \subseteq \R \to \R$ is said to be differentiable in $x \in \mathcal{S}$, if $f$ can be locally approximated by a linear function in $x$.

\begin{center}
\includegraphics[width = 0.6\textwidth]{figure_man/tangent.png} \\
\begin{footnotesize}
Geometrically: A tangent can be placed on the graph of $f$ through the point $(x, f(x))$. $m = f'(x)$ then indicates the slope of this tangent. The function on the left is differentiable everywhere; the function on the right is not differentiable at the red point. 
\end{footnotesize}
\end{center}

% \framebreak

% \textbf{Äquivalente Definition}:

% $f$ ist genau dann differenzierbar bei $\tilde x \in I$, wenn sich $f$ lokal durch eine \textbf{lineare Funktion} (Tangente) approximieren lässt. Das heißt, es existieren

% \begin{itemize}
% \item $m_{\tilde x} \in \R$ (Steigung)
% \item eine Funktion $r(\cdot)$ (Fehler der Approximation),
% \end{itemize}

% sodass

% \begin{eqnarray*}
% % f(x) &=& f(\tilde x) + f'(\tilde x)(x - \tilde x) + r(x - \tilde x) \quad \text{bzw.}\\
% f(\tilde x + h) &=& f(\tilde x) + m_{\tilde x} \cdot h + r(h)\\
% \text{mit } && \lim_{h \to 0}\frac{|r(h)|}{|h|} = 0
% \end{eqnarray*}

% Ist $f$ differenzierbar, dann entspricht $m_{\tilde x} = f'(\tilde x)$ (aus 1. Definition).

\end{vbframe}

\begin{vbframe}{Differentiation (multivariate)}

A similar definition of differentiability holds for multivariate functions. 

\lz 

\textbf{Definition: } A function $f: \mathcal{S}\subseteq \R^d \to \R$ is differentiable in $\xv \in \mathcal{S}$ if there exists a (continuous) linear map $\nabla f(\xv): \mathcal{S}\subseteq \R^d \to \R$ with %it can be locally approximated by a linear function in $\xv$. 

$$
\lim_{\bm{h} \to 0} \frac{f(\xv + \bm{h}) - f(\xv) - \nabla f(\xv) \cdot \bm{h}}{||\bm{h}||} = 0
$$

\begin{center}
\includegraphics[width = 0.3\textwidth]{figure_man/differentiability_multivariate.png} \\
\begin{footnotesize}
Geometrically: The function can be locally approximated by a tangent hyperplane. \\
Source: \url{https://github.com/jermwatt/machine_learning_refined}.
\end{footnotesize}
\end{center}

\framebreak 


This local linear approximation is described by the \textbf{gradient}: If $f$ is differentiable in $\xv$, the \textbf{gradient} is defined by

\vspace*{-0.3cm}

$$
\nabla f = \frac{\partial f}{\partial x_1} \bm e_1 + \cdots + \frac{\partial f}{\partial x_n} \bm e_n =
  \biggl(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n}\biggr)^{\top}.
$$

The elements of the gradient are called \textbf{partial derivatives}. 

\begin{center}
	\includegraphics[width = 0.5\textwidth]{figure_man/grad_unit_vectors.png} \\
\end{center}


\framebreak

Consider $\fx = 0.5 x_1^2 + x_2^2 + x_1 x_2$. The gradient is $\nabla \fx = (x_1 + x_2, 2x_2 + x_1)^{\top}$.

\begin{center}
	\includegraphics[width = 0.5\textwidth]{figure_man/grad_unit_vectors.png} ~~~ \includegraphics[width = 0.4\textwidth]{figure_man/gradient2.png}
\end{center}


\framebreak

\textbf{Properties of the gradient:}
\begin{itemize}
	\item The gradient is orthogonal to level curves and level surfaces of a function
	\item The gradient points in the direction of greatest increase of $f$
	\begin{center}
		\includegraphics[width = 0.4\textwidth]{figure_man/gradient3.png} ~~~ \includegraphics[width = 0.4\textwidth]{figure_man/gradient.png}
	\end{center}
	\item The normal vector describing the tangent plane has $n + 1$ components, the first $n$ correspond to $\nabla f$ and the $(n + 1)-$th has the value $-1$
\end{itemize}


\framebreak 

We can also compute the instantaneous rate of change of $f$ at $\xv$ along an arbitrary direction $\bm{v}$: 

$$
D_v \fx = \nabla \fx \cdot \bm v.
$$


$D_v \fx$ is called \textbf{directional derivative}. 

\textbf{Definition:} The directional derivative for direction $\bm{v}$ for $f: \mathcal{S} \to \R, \mathcal{S} \subseteq \R^d$ is defined as
 $$
   D_v \fx = \lim_{h \to 0} \frac{f(\xv + h \bm v) - \fx}{h}.
 $$

\lz 

For example, the slope in the direction $\bm{v} = (1, 1)$ is the sum of the first and the second partial derivative: 

$$
D_v \fx = \nabla \fx^\top \cdot \begin{pmatrix} 1 \\1\end{pmatrix} = \frac{\partial f}{\partial x_1} + \frac{\partial f}{\partial x_2}
$$

\end{vbframe}


\endlecture
\end{document}