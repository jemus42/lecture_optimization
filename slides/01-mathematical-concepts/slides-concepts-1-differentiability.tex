\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}%{figure_man/hinge_vs_l2.pdf}
{figure_man/tangent.png} 
\newcommand{\learninggoals}{
\item Definition of smoothness
\item Uni- \& multivariate differentiation
\item Gradient, partial derivatives 
\item Jacobi-Matrix
\item Hessian Matrix
}


%%%

%%%
%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Mathematical Concepts: \\Differentiation and Derivatives}
\lecture{Optimization in Machine Learning}
\sloppy

% ------------------------------------------------------------------------------


\begin{vbframe}{Univariate differentiability}

\textbf{Definition:} A function $f: \mathcal{S} \subseteq \R \to \R$ is said to be differentiable for each inner point $x \in \mathcal{S}$ if the following limit exists:

$$
f'(x) := \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$

Intuitively: $f$ can be approxed locally by a lin. fun. with slope $m = f'(x)$. 

\begin{center}
\includegraphics[width = 0.8\textwidth]{figure_man/tangent.png} \\
\begin{footnotesize}
Left: Function is differentiable everywhere. Right: Not differentiable at the red point. 
\end{footnotesize}
\end{center}

% \framebreak

% \textbf{Äquivalente Definition}:

% $f$ ist genau dann differenzierbar bei $\tilde x \in I$, wenn sich $f$ lokal durch eine \textbf{lineare Funktion} (Tangente) approximieren lässt. Das heißt, es existieren

% \begin{itemize}
% \item $m_{\tilde x} \in \R$ (Steigung)
% \item eine Funktion $r(\cdot)$ (Fehler der Approximation),
% \end{itemize}

% sodass

% \begin{eqnarray*}
% % f(x) &=& f(\tilde x) + f'(\tilde x)(x - \tilde x) + r(x - \tilde x) \quad \text{bzw.}\\
% f(\tilde x + h) &=& f(\tilde x) + m_{\tilde x} \cdot h + r(h)\\
% \text{mit } && \lim_{h \to 0}\frac{|r(h)|}{|h|} = 0
% \end{eqnarray*}

% Ist $f$ differenzierbar, dann entspricht $m_{\tilde x} = f'(\tilde x)$ (aus 1. Definition).

\end{vbframe}


\begin{vbframe}{Smooth vs. non-smooth}

\begin{itemize}
\item \textbf{Smoothness} of a function  $f: \mathcal{S} \to \R$ is measured by the number of its continuous derivatives
\item $k$-times continuously diff. means: $f^{(k)}$ exists + is continuous for every $\xv \in \mathcal{S}$\\
($f \in \mathcal{C}^k$ class of continuously differentiable functions) 
\item In this lecture, we call $f$ \enquote{smooth}, if at least $f \in \mathcal{C}^1$
\end{itemize}


\begin{center}
\includegraphics[width = 0.5\textwidth]{figure_man/hinge_vs_l2.pdf} \\ 
\begin{footnotesize}
$f_1$ is smooth, $f_2$ is continuous but not differentiable, and $f_3$ is non-continuous. 
\end{footnotesize}
\end{center}


\end{vbframe}

\begin{vbframe}{Multivariate differentiability}

\textbf{Definition: }$f: \mathcal{S}\subseteq \R^d \to \R$ is differentiable in $\xv \in \mathcal{S}$ if there exists a (continuous) linear map $\nabla f(\xv): \mathcal{S}\subseteq \R^d \to \R^d$ with %it can be locally approximated by a linear function in $\xv$. 

$$
\lim_{\bm{h} \to 0} \frac{f(\xv + \bm{h}) - f(\xv) - \nabla f(\xv) \cdot \bm{h}}{||\bm{h}||} = 0
$$

\begin{center}
\includegraphics[width = 0.3\textwidth]{figure_man/differentiability_multivariate.png} \\
\begin{footnotesize}
Geometrically: The function can be locally approximated by a tangent hyperplane. \\
Source: \url{https://github.com/jermwatt/machine_learning_refined}.
\end{footnotesize}
\end{center}

\end{vbframe}

\begin{vbframe}{Gradient}

This linear approximation is given by the \textbf{gradient}: % If $f$ is differentiable in $\xv$, the \textbf{gradient} is defined by
\vspace*{-0.3cm}
$$
\nabla f = \frac{\partial f}{\partial x_1} \bm e_1 + \cdots + \frac{\partial f}{\partial x_n} \bm e_n =
  \biggl(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n}\biggr)^{\top}.
$$
The elements of the gradient are called \textbf{partial derivatives}. 

\vspace*{0.3cm}

\begin{columns}
\begin{column}{0.6\textwidth}
\vspace{1cm} \quad 

To compute a partial derivative in $x_j$, we treat the function as univariate in $x_j$ (and everything else as constant), then compute the derivative in $x_j$.
\end{column}
\begin{column}{0.4\textwidth}
%\begin{center}
\vspace{\topsep}
\includegraphics[width=\columnwidth,trim={2.5cm 0cm 2.0cm 0cm},clip]{figure_man/grad_unit_vectors.png} \\
%\end{center}
\end{column}
\end{columns}


\framebreak

Consider $\fx = 0.5 x_1^2 + x_2^2 + x_1 x_2$. The gradient is $$\nabla \fx = (x_1 + x_2, 2x_2 + x_1)^{\top}.$$

\begin{center}
	\includegraphics[width = 0.5\textwidth]{figure_man/grad_unit_vectors.png} ~~~ \includegraphics[width = 0.45\textwidth]{figure_man/gradient2.png}
\end{center}


\end{vbframe}

\begin{vbframe}{Directional derivative}

The directional derivative tells how fast $f: \mathcal{S} \to \R$ is changing w.r.t. an arbitrary direction $\bm{v}$: 

 $$
   D_v \fx := \lim_{h \to 0} \frac{f(\xv + h \bm v) - \fx}{h} = \nabla \fx^\top \cdot \bm v. 
 $$

 
\textbf{Example: } The directional derivative for $\bm{v} = (1, 1)$ is:

$$
D_v \fx = \nabla \fx^\top \cdot \begin{pmatrix} 1 \\1\end{pmatrix} = \frac{\partial f}{\partial x_1} + \frac{\partial f}{\partial x_2}
$$

NB: Some people require that $||\bm{v}|| = 1$. Then, we can identify $D_v \fx$ with the instantaneous rate of change in direction $\bm{v}$ -- and in our example we would have to divide by \sqrt{2}.

\end{vbframe}

\begin{vbframe}{Properties of the gradient}

\begin{itemize}
	\item Orthogonal to level curves/surfaces of a function
	% \item The normal vector describing the tangent plane is has $n + 1$ components, the first $n$ correspond to $\nabla f$ and the $(n + 1)-$th has the value $-1$
	\item Points in direction of greatest increase of $f$
	\begin{center}
		\includegraphics[width = 0.4\textwidth]{figure_man/gradient3.png} \includegraphics[width = 0.4\textwidth]{figure_man/gradient.png}
	\end{center}
	\begin{footnotesize}
	\textbf{Proof}: Let $\bm{v}$ be a vector of length $1$. Let $\theta$ the angle between $\bm{v}$ and $\nabla \fx$. 
 
	$$
		D_{\bm{v}}\fx = \nabla \fx^\top \bm{v} = \|\nabla \fx\|~\|\bm{v}\|\cos(\theta) = \|\nabla \fx\| \cos(\theta)
	$$
	using the cosine formula for dot products and because $\|v\| = 1$ by assumption. $\cos(\theta)$ is maximal if $\theta = 0$, which is if $\bm{v}$ and $\nabla \fx$ point in the same direction. \\
	(Alternative proof: Apply Cauchy-Schwarz to $\nabla \fx^\top \bm{v}$ and show for which $\bm{v}$ the inequality holds with equality.)  
	\end{footnotesize}
\end{itemize}


\framebreak

\begin{itemize}
	\item Negative gradient $- \nabla \fx$ points in direction of greatest decrease
\end{itemize}

\begin{center}
\includegraphics[width=0.8\textwidth]{figure_man/branin.jpg}
\\

(Length of arrow is scaled norm of gradient)

\end{center}



\end{vbframe}




\begin{vbframe}{Jacobi Matrix}

Let $f: \mathcal{S} \to \R^m$ be vector-valued with components $f_1, f_2, ..., f_m$. 

\textbf{Jacobian} generalizes the gradient by placing the $\nabla f_j$ in its rows:

\begin{eqnarray*}
	J_f = \begin{pmatrix} \frac{\partial f_1}{\partial x_1} & \hdots & \frac{\partial f_1}{\partial x_n} \\
	\vdots & \ddots & \vdots \\
	\frac{\partial f_m}{\partial x_1} & \hdots & \frac{\partial f_m}{\partial x_n}
	\end{pmatrix}
\end{eqnarray*}

\begin{center}
	\includegraphics[width = 0.5\textwidth]{figure_man/Jacobian.png} \\
	\begin{footnotesize}
	$f: \R^2 \to \R^2$ sends  a small square (left, red) originating from a given input point to a distorted parallelogram (right, red) . Jacobian gives best linear approximation of distorted parallelogram near that point. Source: Wikipedia.
	\end{footnotesize}
\end{center}


\end{vbframe}



\begin{vbframe}{Definition Hessian Matrix}

The 2nd derivative of a multivariate function $f \in \mathcal{C}^2(\mathcal{S}, \R)$, $\mathcal{S}\subseteq \R^d$ (if it exists) is defined by the \textbf{Hessian} matrix

  \[ H(\xv) =\nabla^2 \fx =
  \Bigl(\frac{\partial^2 \fx}{\partial x_i \partial x_j}\Bigr)_{i,j=1\ldots d}\]
  
\lz 

\textbf{Example}: Let $f(x_1,x_2) = sin(x_1) \cdot cos(2x_2)$. Then:

$$
H(\xv) = \begin{pmatrix}
\text{-cos}(2x_2)\cdot\text{sin}(x_1) & \text{-2cos}(x_1)\cdot\text{sin}(2x_2) 
\\ \text{-2cos}(x_1)\cdot\text{sin}(2x_2) & \text{-4cos}(2x_2)\cdot\text{sin}(x_1) 
\end{pmatrix}
$$

\begin{itemize}
    \item If all 2nd partial derivatives are continuous, $H$ will be symmetric
    \item Many local properties, w.r.t. geometry, convexity, critical points, 
    are encoded by the Hessian and its Eigenspectrum ($\rightarrow$ later).
\end{itemize}

\end{vbframe}


\begin{vbframe}{Hessian describes local curvature} 

\begin{footnotesize}

Let w.l.o.g. $A(\xv) = \{\lambda_{1, \xv}, ..., \lambda_{d, \xv}\}$ be Eigenspectrum with $\lambda_{1, \xv} \le \lambda_{2, \xv} \le ... \le \lambda_{d, \xv}$ of $H(\xv)$; let $\bm{v}_{i, \xv}$ define the respective Eigenvectors. We can read off it: 

\begin{itemize}
  \item $\bm{v}_d$/$\bm{v}_1$ points in the direction of largest/smallest curvature
\end{itemize}

\vspace*{0.1cm}



\textbf{Example (continued):} $H(\xv) = \begin{pmatrix}
\text{-cos}(2x_2)\cdot\text{sin}(x_1) & \text{-2cos}(x_1)\cdot\text{sin}(2x_2) 
\\ \text{-2cos}(x_1)\cdot\text{sin}(2x_2) & \text{-4cos}(2x_2)\cdot\text{sin}(x_1) 
\end{pmatrix}.
$

\begin{itemize}
  \item \textcolor{orange}{$H(a),~ a=(\frac{-\pi}{2},0)$: $\lambda_{1,a}=1, \lambda_{2,a}=4$; $v_{1,a} = (1, 0)^\top$, $v_{2,a} = (0, 1)^\top$}
\end{itemize}

\end{footnotesize}

\vspace*{0.05cm}

\begin{figure}[!tbp]
    \includegraphics[width=0.38\textwidth]{figure_man/hessian_3d.png}
    \hspace{0.5cm}
    \includegraphics[width=0.38\textwidth]{figure_man/hessian_contour.png}
\end{figure} 

\end{vbframe}




\endlecture
\end{document}