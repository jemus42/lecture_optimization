
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/hinge_vs_l2.pdf}
\newcommand{\learninggoals}{
\item Definition of smoothness
\item Uni- \& multivariate differentiation
\item Gradient, partial derivatives 
\item Jacobi-Matrix}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Smoothness \& Gradients}
\lecture{Optimization}
\sloppy

% ------------------------------------------------------------------------------


\begin{vbframe}{Univariate differentiability}

\textbf{Definition:} A function $f: \mathcal{S} \subseteq \R \to \R$ is said to be differentiable for each inner point $x \in \mathcal{S}$ if the following limit exists:

$$
f'(x) := \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$

Intuitively: $f$ can be approximated locally by a linear function with slope $m = f'(x)$. 

\begin{center}
\includegraphics[width = 0.8\textwidth]{figure_man/tangent.png} \\
\begin{footnotesize}
Left: Function is differentiable everywhere. Right: Not differentiable at the red point. 
\end{footnotesize}
\end{center}

% \framebreak

% \textbf{Äquivalente Definition}:

% $f$ ist genau dann differenzierbar bei $\tilde x \in I$, wenn sich $f$ lokal durch eine \textbf{lineare Funktion} (Tangente) approximieren lässt. Das heißt, es existieren

% \begin{itemize}
% \item $m_{\tilde x} \in \R$ (Steigung)
% \item eine Funktion $r(\cdot)$ (Fehler der Approximation),
% \end{itemize}

% sodass

% \begin{eqnarray*}
% % f(x) &=& f(\tilde x) + f'(\tilde x)(x - \tilde x) + r(x - \tilde x) \quad \text{bzw.}\\
% f(\tilde x + h) &=& f(\tilde x) + m_{\tilde x} \cdot h + r(h)\\
% \text{mit } && \lim_{h \to 0}\frac{|r(h)|}{|h|} = 0
% \end{eqnarray*}

% Ist $f$ differenzierbar, dann entspricht $m_{\tilde x} = f'(\tilde x)$ (aus 1. Definition).

\end{vbframe}


\begin{vbframe}{Smooth vs. non-smooth}

\begin{itemize}
\item \textbf{Smoothness} of a function  $f: \mathcal{S} \to \R$ is measured by the number of its continuous derivatives
\item $k$-times continuously diff. means: $f^{(k)}$ exists + is continuous on $\mathcal{S}$\\
($f \in \mathcal{C}^k$ class of continuously differentiable functions) 
\item In this lecture, we call $f$ \enquote{smooth}, if at least $f \in \mathcal{C}^1$
\end{itemize}


\begin{center}
\includegraphics[width = 0.5\textwidth]{figure_man/hinge_vs_l2.pdf} \\ 
\begin{footnotesize}
$f_1$ is smooth, $f_2$ is continuous but not differentiable, and $f_3$ is non-continuous. 
\end{footnotesize}
\end{center}


\end{vbframe}

\begin{vbframe}{Multivariate differentiability}

\textbf{Definition: }$f: \mathcal{S}\subseteq \R^d \to \R$ is differentiable in $\xv \in \mathcal{S}$ if there exists a (continuous) linear map $\nabla f(\xv): \mathcal{S}\subseteq \R^d \to \R$ with %it can be locally approximated by a linear function in $\xv$. 

$$
\lim_{\bm{h} \to 0} \frac{f(\xv + \bm{h}) - f(\xv) - \nabla f(\xv) \cdot \bm{h}}{||\bm{h}||} = 0
$$

\begin{center}
\includegraphics[width = 0.3\textwidth]{figure_man/differentiability_multivariate.png} \\
\begin{footnotesize}
Geometrically: The function can be locally approximated by a tangent hyperplane. \\
Source: \url{https://github.com/jermwatt/machine_learning_refined}.
\end{footnotesize}
\end{center}

\end{vbframe}

\begin{vbframe}{Gradient}

This linear approximation is given by the \textbf{gradient}: % If $f$ is differentiable in $\xv$, the \textbf{gradient} is defined by

\vspace*{-0.3cm}

$$
\nabla f = \frac{\partial f}{\partial x_1} \bm e_1 + \cdots + \frac{\partial f}{\partial x_n} \bm e_n =
  \biggl(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n}\biggr)^{\top}.
$$

The elements of the gradient are called \textbf{partial derivatives}. 

\begin{center}
	\includegraphics[width = 0.5\textwidth]{figure_man/grad_unit_vectors.png} \\
\end{center}


\framebreak

Consider $\fx = 0.5 x_1^2 + x_2^2 + x_1 x_2$. The gradient is $$\nabla \fx = (x_1 + x_2, 2x_2 + x_1)^{\top}.$$

\begin{center}
	\includegraphics[width = 0.5\textwidth]{figure_man/grad_unit_vectors.png} ~~~ \includegraphics[width = 0.45\textwidth]{figure_man/gradient2.png}
\end{center}


\end{vbframe}

\begin{vbframe}{Directional derivative}

The directional derivative tells how fast $f: \mathcal{S} \to \R$ is changing w.r.t. an arbitrary direction $\bm{v}$: 

 $$
   D_v \fx := \lim_{h \to 0} \frac{f(\xv + h \bm v) - \fx}{h} = \nabla \fx \cdot \bm v. 
 $$

 
\textbf{Example: } The instantaneous rate of change in direction $\bm{v} = (1, 1)$ is:

$$
D_v \fx = \nabla \fx^\top \cdot \begin{pmatrix} 1 \\1\end{pmatrix} = \frac{\partial f}{\partial x_1} + \frac{\partial f}{\partial x_2}
$$

\end{vbframe}

\begin{vbframe}{Properties of the gradient}

\begin{itemize}
	\item Orthogonal to level curves / surfaces of a function
	% \item The normal vector describing the tangent plane is has $n + 1$ components, the first $n$ correspond to $\nabla f$ and the $(n + 1)-$th has the value $-1$
	\item Points in direction of greatest increase of $f$
	\begin{center}
		\includegraphics[width = 0.4\textwidth]{figure_man/gradient3.png} \includegraphics[width = 0.4\textwidth]{figure_man/gradient.png}
	\end{center}
	\begin{footnotesize}
	\textbf{Proof}: Let $\bm{v}$ be a vector of length $1$. Let $\theta$ the angle between $\bm{v}$ and $\nabla \fx$. 
 
	$$
		D_{\bm{v}}\fx = \nabla \fx^\top \bm{v} = \|\nabla \fx\|~\|\bm{v}\|\cos(\theta) = \|\nabla \fx\| \cos(\theta)
	$$
	using the cosine formula for dot products and	 because $\|v\| = 1$ by assumption. $\cos(\theta)$ is maximal if $\theta = 0$, which is if $\bm{v}$ and $\nabla \fx$ point in the same direction. \\
	(Alternative proof: Apply Cauchy-Schwarz to $\nabla \fx^\top \bm{v}$ and show for which $\bm{v}$ the inequality holds with equality.)  
	\end{footnotesize}
\end{itemize}


\framebreak

\begin{itemize}
	\item Negative gradient $- \nabla \fx$ points in direction of greatest decrease
\end{itemize}

\begin{center}
\includegraphics[width=0.85\textwidth]{figure_man/negative_gradients.png}\\
\end{center}



\end{vbframe}




\begin{vbframe}{Jacobi Matrix}

Let $f: \mathcal{S} \to \R^m$ be vector-valued with components $f_1, f_2, ..., f_m$. 

\textbf{Jacobian} matrix as generalization of gradient:  

\begin{eqnarray*}
	J_f = \begin{pmatrix} \frac{\partial f_1}{\partial x_1} & \hdots & \frac{\partial f_1}{\partial x_n} \\
	\vdots & \ddots & \vdots \\
	\frac{\partial f_m}{\partial x_1} & \hdots & \frac{\partial f_m}{\partial x_n}
	\end{pmatrix}
\end{eqnarray*}

\begin{center}
	\includegraphics[width = 0.5\textwidth]{figure_man/Jacobian.png} \\
	\begin{footnotesize}
	$f: \R^2 \to \R^2$ sends  a small square (left, red) to a distored parallelogram (right, red). Jacobian gives best linear aproximation of di
	storted parallelogram near that point. Source: Wikipedia.
	\end{footnotesize}
\end{center}


\end{vbframe}

\endlecture
\end{document}