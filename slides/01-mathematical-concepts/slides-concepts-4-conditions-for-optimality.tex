\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/local_global_min_2D.png}
\newcommand{\learninggoals}{
\item Local and global
\item First \& second order conditions}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Mathematical Concepts: \\Conditions for optimality}
\lecture{Optimization in Machine Learning}
\sloppy

\begin{vbframe}{Definition local and global minimum}
  Given $\mathcal{S} \subseteq \R^d$, $f: \mathcal{S} \to \R$:
  \begin{itemize}
  \item $f$ has \textbf{global minimum} in $\xv^\ast \in \mathcal{S}$, if $f(\xv^\ast) \leq \fx$ for all $\xv \in \mathcal{S}$
  \item $f$ has a \textbf{local minimum} in $\xv^\ast$, if $f(\xv^\ast) \leq \fx$ for all $\xv \in B_\eps(\xv^\ast)$, with $B_\eps(\xv^\ast) := \{\xv \in \mathcal{S} ~|~\|\xv - \xv^\ast\| < \eps\}$ (\enquote{$\epsilon$}-ball round $\xv^\ast$).  
  \end{itemize}
  
  \vspace*{-0.3cm}
  
  \begin{center}
  \includegraphics[width = 0.48\textwidth]{figure_man/local_global_min.png} \quad \includegraphics[width = 0.48\textwidth]{figure_man/local_global_min_2D.png} \\
  \vspace*{0.3cm}
  \begin{tiny}
    Source (left): \url{https://en.wikipedia.org/wiki/Maxima_and_minima}. \\ Source (right): \url{https://wngaw.github.io/linear-regression/}. 
  \end{tiny}
  \end{center}
  
  \end{vbframe}
  
  
  \begin{vbframe}{Existence of Optima}
  
  $$
  f: \; \mathcal{S} \to \R
  $$
  
  \begin{itemize}
  \item $f$ continous:
  \begin{itemize}
  \item A real-valued function $f$ defined on a \textbf{compact set} must attain a minimum and a maximum (extreme value theorem).
  \end{itemize}
  \item $f$ not continuous:
  \begin{itemize}
  \item In general no statement possible about existence of maximum/minimum.
  \end{itemize}
  \end{itemize}
  
  % \textbf{Note}: From now on, we assume silently that the functions considered are sufficiently smooth (i.e., when we consider a second derivative of a function, we assume that the second derivative exists and is continuous). 
  
  \end{vbframe}
    
  \begin{vbframe}{First Order Condition for optimality}
  
  Let $f \in \mathcal{C}^1$. \textbf{Observation: } At a local minimum (for an interior point) 1st order Taylor series approx is perfectly flat; 1st order derivs are $0$.

  \lz 
  
  \begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/first_order.png} \\
  \begin{footnotesize}
  (Strictly) convex functions (left: univariate; right: multivariate) with unique local minimum, which is the global one. Tangent (hyperplane) is perfectly flat at the optimum. \end{footnotesize}\\
  \begin{tiny}
  Source: Watt, 2020, Machine Learning Refined. 
  \end{tiny}
  \end{center}
  
  \framebreak 
  
  At every (interior) local minimum $\xv^\ast$ the first derivative is necessarily always zero; it is therefore called \textbf{first-order} or \textbf{necessary} condition. 
  
  \lz 

  \begin{itemize}
    \item \textbf{First-order condition (univariate): } Let $\xv^\ast \in \R$ be a local minimum of $f$. Then:
    $$
    f'(\xv^\ast) = 0
    $$
    \item \textbf{First-order condition (multivariate): } Let $\xv^\ast \in \R^d$ be a local minimum of $f$. Then:
    $$
    \nabla f(\xv^\ast) = (0, 0, ..., 0)^\top
    $$
  
  \end{itemize}
  
  The points at which the first order derivative is zero are called \textbf{stationary points}. 
  
  \framebreak 
  
  The condition is \textbf{not sufficient}: Not every stationary point ($\nabla f(\xv) = 0$) is a local minimum. 
  
  \begin{center}
  \includegraphics[width = 0.7\textwidth]{figure_man/saddle_points_2.png} \\
  \begin{footnotesize}
  Left: Four points fulfill the necessary conditions; but two of the points are local maxima (not minima). Middle: One point fulfills the necessary condition, but is not a local optimum. Right: Multiple local minima and maxima. 
  \end{footnotesize}\\
  \begin{tiny}
  Source: Watt, 2020, Machine Learning Refined. 
  \end{tiny}
  \end{center}
  
  \end{vbframe}
  
  \begin{vbframe}{Second Order Condition for Optimality}
  
  % Let $f \in \mathcal{C}^2$. A stationary point $\xv$ (i.e., $\fx = 0$) is a local minimum if $f''(\xv) > 0$ (i.e., the function is locally convex). 

  % \begin{center}
  % \includegraphics[width = 0.7\textwidth]{figure_man/saddle_points_2.png} \\
  % \begin{footnotesize}
  % Left / Right: Function has positive curvature in all directions at the minima, and negative curvature around the maxima. Middle: Curvature is positive in one, and negative in the other direction. 
  % \end{footnotesize}\\
  % \begin{tiny}
  % Source: Watt, 2020, Machine Learning Refined. 
  % \end{tiny}
  % \end{center}
  %\framebreak 
  
  Let $f \in \mathcal{C}^2$.  If the function is locally convex, so:

  \begin{itemize}
    \item \textbf{Second-order condition}: A \textbf{stationary} point $\xv^\ast \in \mathcal{S}\subseteq \R$ fulfills
    $$f''(x^\ast) > 0 \quad (d = 1) $$ 
    $$\nabla^2 f(\xv^\ast) \text{ is positive definite} \quad (d > 1) $$
    \begin{footnotesize}
    (all EVs positive), hence curvature is positive in all directions. 
    \end{footnotesize}
    % \item \textbf{Second-order condition (multivariate): } A \textbf{stationary} point $\xv^\ast \in \mathcal{S}\subseteq \R^d$ fulfills $$\nabla^2 f(\xv^\ast) \text{ is positive definite}$$  
  \end{itemize}
  
 Then the second-order condition is \textbf{sufficient} to prove a local minimum. 

  \begin{center}
  \includegraphics[width = 0.3\textwidth]{figure_man/local_global_min.png} \quad \includegraphics[width = 0.3\textwidth]{figure_man/local_global_min_2D.png} \\
  \vspace*{0.3cm}
  \begin{tiny}
    Two functions that are locally convex, but not globally convex. % Source (left): \url{https://en.wikipedia.org/wiki/Maxima_and_minima}. Source (right): \url{https://wngaw.github.io/linear-regression/}. 
  \end{tiny}
  \end{center}
  
 %\textbf{Note:} For a convex function, $\nabla^2 f(\xv)$ is always p.s.d.; therefore, any stationary point is the local (also global) minimum. 
  


  
  % \framebreak 
  
  % \begin{itemize}
  % \item The necessary condition says that at every local minimum the derivative is \textbf{always} $0$. It is non-exclusive: Also at a saddle point (right plot), the derivative can be zero. 
  % \item In order to make sure that a stationary point also corresponds to a local minimum, we have to require that the curvature at this point is positive. 
  % \end{itemize}
  % \begin{center}
  % 	\includegraphics[width=1\textwidth, keepaspectratio]{figure_man/minmaxsaddle.png}
  % \end{center}

%   \framebreak

% \textbf{Example:}
% \footnotesize
% \begin{enumerate}
% \item If [$f$ convex $\Leftrightarrow$ all eigenvalues of $H(\xv)$ positive], then local minimum\\
% \item If [$f$ concave $\Leftrightarrow$ all eigenvalues of $H(\xv)$ negative], then local maximum\\
% \item Some eigenvalues positive and some negative $\Leftrightarrow$ saddle point
% \end{enumerate}

% \vspace{0.5cm}

% \begin{center}
% \includegraphics[scale= 0.5]{figure_man/convex.jpg}
% \end{center}

  
  \end{vbframe}

\begin{vbframe}{Conditions for optimality and convexity}

Let $f:\mathcal{S} \to \R$ be convex on convex set $\mathcal{S}$. Then the following holds:

\begin{itemize}
\item Any local minimum is also global minimum 
\item If $f$ strictly convex, $f$ has exactly one local minimum which is also unique global minimum on $\mathcal{S}$
% \item Sublevel sets $S_1 = \{\xv~|~\fx < a\}$ and $S_2 = \{\xv~|~\fx \leq a \}$, $a\in \R$, form convex sets.
\end{itemize}

\begin{center}
\includegraphics[width = 1\textwidth]{figure_man/hessian-eigenvalues.pdf} \\
% hessian-eigenvalues.R
\vspace*{0.3cm}
\begin{tiny}
  Three different quadratic forms. Left: Hessian has a two positive Eigenvalues ($\lambda_1 = 2, \lambda_2 = 5$). Middle: Hessian has positive and negative Eigenvalue ($\lambda_1 = 2, \lambda_2 = - 5$). Right: Hessian has positive and a zero Eigenvalue ($\lambda_1 = 2, \lambda_2 = 0$).% Source (left): \url{https://en.wikipedia.org/wiki/Maxima_and_minima}. Source (right): \url{https://wngaw.github.io/linear-regression/}. 
\end{tiny}
\end{center}


\framebreak 

\textbf{Example: } Branin function 

\vspace*{-0.5cm}

\begin{columns}
\begin{column}{0.4\textwidth}
\includegraphics[width = 0.8\textwidth]{figure_man/branin3d/branin2D.pdf} \\  \vspace*{-1cm}
\includegraphics[width = 0.8\textwidth]{figure_man/branin3d/branin3D.pdf} 
\end{column}
\hspace*{-2cm}
\begin{column}{0.55\textwidth} 
\includegraphics[width=0.4\columnwidth]{figure_man/branin3d/branin3D-optim-1.pdf}\\ \vspace*{-1.5cm}
\includegraphics[width=0.4\columnwidth]{figure_man/branin3d/branin3D-optim-2.pdf}\\\vspace*{-1.5cm}
\includegraphics[width=0.4\columnwidth]{figure_man/branin3d/branin3D-optim-3.pdf}
\end{column}
\hspace*{-4cm}
\begin{column}{0.3\textwidth}
\begin{tiny}
Gradient and Hessian have been computed numerically at the minima / red points (R package numDeriv). Gradients are 0, function is locally convex. Eigenspectra: 
$\lambda_1 = 22.29, \lambda_2 = 0.96$ (Opt. 1)\\ $\lambda_1 = 11.07, \lambda_2 = 1.73$ (Opt. 2) $\lambda_1 = 11.33, \lambda_2 = 1.69$ (Opt. 3)
\end{tiny}\end{column}
\end{columns}


\framebreak 

Def.: \textbf{Saddle point}
\begin{itemize}
    \item Gradient of 0
    \item If H is indefinite at stationary point, so pos and neg Eigenvalues occur in H, we have a saddle point 
    \item The latter is only a sufficient condition, but not a necessary one
\end{itemize}

\begin{center}
\includegraphics[scale= 0.5]{figure_man/convex.jpg}
\end{center}

% \framebreak 

% \textbf{Example: } Consider $\fx = 2 \cdot $


\end{vbframe}


  \endlecture
  \end{document}
  
  
  