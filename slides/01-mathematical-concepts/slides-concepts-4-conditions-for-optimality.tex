\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/local_global_min_2D.png}
\newcommand{\learninggoals}{
\item Local and global
\item First \& second order conditions}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Mathematical Concepts: \\Conditions for optimality}
\lecture{Optimization in Machine Learning}
\sloppy

\begin{vbframe}{Definition local and global minimum}
  Given $\mathcal{S} \subseteq \R^d$, $f: \mathcal{S} \to \R$:
  \begin{itemize}
  \item $f$ has \textbf{global minimum} in $\xv^\ast \in \mathcal{S}$, if $f(\xv^\ast) \leq \fx$ for all $\xv \in \mathcal{S}$
  \item $f$ has a \textbf{local minimum} in $\xv^\ast$, if $f(\xv^\ast) \leq \fx$ for all $\xv \in B_\eps(\xv^\ast)$, with $B_\eps(\xv^\ast) := \{\xv \in \mathcal{S} ~|~\|\xv - \xv^\ast\| < \eps\}$ (\enquote{$\epsilon$}-ball round $\xv^\ast$).  
  \end{itemize}
  
  \vspace*{-0.3cm}
  
  \begin{center}
  \includegraphics[width = 0.48\textwidth]{figure_man/local_global_min.png} \quad \includegraphics[width = 0.48\textwidth]{figure_man/local_global_min_2D.png} \\
  \vspace*{0.3cm}
  \begin{tiny}
    Source (left): \url{https://en.wikipedia.org/wiki/Maxima_and_minima}. \\ Source (right): \url{https://wngaw.github.io/linear-regression/}. 
  \end{tiny}
  \end{center}
  
  \end{vbframe}
  
  
  \begin{vbframe}{Existence of Optima}
  
  $$
  f: \; \mathcal{S} \to \R
  $$
  
  \begin{itemize}
  \item $f$ continous:
  \begin{itemize}
  \item A real-valued function $f$ defined on a \textbf{compact set} must attain a minimum and a maximum (extreme value theorem).
  \end{itemize}
  \item $f$ not continuous:
  \begin{itemize}
  \item In general no statement possible about existence of maximum/minimum.
  \end{itemize}
  \end{itemize}
  
  % \textbf{Note}: From now on, we assume silently that the functions considered are sufficiently smooth (i.e., when we consider a second derivative of a function, we assume that the second derivative exists and is continuous). 
  
  \end{vbframe}
    
  \begin{vbframe}{First Order Condition for optimality}
  
  Let $f \in \mathcal{C}^1$. \textbf{Observation: } At a local minimum (for an interior point) 1st order Taylor series approx is perfectly flat; 1st order derivs are $0$.

  \lz 
  
  \begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/first_order.png} \\
  \begin{footnotesize}
  (Strictly) convex functions (left: univariate; right: multivariate) with unique local minimum, which is the global one. Tangent (hyperplane) is perfectly flat at the optimum. \end{footnotesize}\\
  \begin{tiny}
  Source: Watt, 2020, Machine Learning Refined. 
  \end{tiny}
  \end{center}
  
  \framebreak 
  
  At every (interior) local minimum $\xv^\ast$ the first derivative is necessarily always zero; it is therefore called \textbf{first-order} or \textbf{necessary} condition. 
  
  \lz 

  \begin{itemize}
    \item \textbf{First-order condition (univariate): } Let $\xv^\ast \in \R$ be a local minimum of $f$. Then:
    $$
    f'(\xv^\ast) = 0
    $$
    \item \textbf{First-order condition (multivariate): } Let $\xv^\ast \in \R^d$ be a local minimum of $f$. Then:
    $$
    \nabla f(\xv^\ast) = (0, 0, ..., 0)^\top
    $$
  
  \end{itemize}
  
  The points at which the first order derivative is zero are called \textbf{stationary points}. 
  
  \framebreak 
  
  The condition is \textbf{not sufficient}: Not every stationary point ($\nabla f(\xv) = 0$) is a local minimum. 
  
  \begin{center}
  \includegraphics[width = 0.7\textwidth]{figure_man/saddle_points_2.png} \\
  \begin{footnotesize}
  Left: Four points fulfill the necessary conditions; but two of the points are local maxima (not minima). Middle: One point fulfills the necessary condition, but is not a local optimum. Right: Multiple local minima and maxima. 
  \end{footnotesize}\\
  \begin{tiny}
  Source: Watt, 2020, Machine Learning Refined. 
  \end{tiny}
  \end{center}
  
  \end{vbframe}
  
  \begin{vbframe}{Second Order Condition for Optimality}
  
  Let $f \in \mathcal{C}^2$. A stationary point $\xv$ (i.e., $\fx = 0$) is a local minimum if $f''(\xv) > 0$ (i.e., the function is locally convex). 

  \begin{center}
  \includegraphics[width = 0.7\textwidth]{figure_man/saddle_points_2.png} \\
  \begin{footnotesize}
  Left / Right: Function has positive curvature in all directions at the minima, and negative curvature around the maxima. Middle: Curvature is positive in one, and negative in the other direction. 
  \end{footnotesize}\\
  \begin{tiny}
  Source: Watt, 2020, Machine Learning Refined. 
  \end{tiny}
  \end{center}
  
  
  \framebreak 
  
  Let $f \in \mathcal{C}^2$.  If the following holds:

  \begin{itemize}
    \item \textbf{Second-order condition (univariate)}: A \textbf{stationary} point $x^\ast \in \mathcal{S}\subseteq \R$ fulfills
    $$f''(x^\ast) > 0.$$ 
    \item \textbf{Second-order condition (multivariate): } A \textbf{stationary} point $\xv^\ast \in \mathcal{S}\subseteq \R^d$ fulfills $$\nabla^2 f(\xv^\ast) \text{ is positive semi-definite}$$  (all eigenvalues are positive). This means the curvature is positive in all directions. 
  
  \end{itemize}
  
 Then second-order condition is \textbf{sufficient} to prove a local minimum. 

 \lz 

 \textbf{Note:} For a convex function, $\nabla^2 f(\xv)$ is always p.s.d.; therefore, any stationary point is the local (also global) minimum. 
  


  
  % \framebreak 
  
  % \begin{itemize}
  % \item The necessary condition says that at every local minimum the derivative is \textbf{always} $0$. It is non-exclusive: Also at a saddle point (right plot), the derivative can be zero. 
  % \item In order to make sure that a stationary point also corresponds to a local minimum, we have to require that the curvature at this point is positive. 
  % \end{itemize}
  % \begin{center}
  % 	\includegraphics[width=1\textwidth, keepaspectratio]{figure_man/minmaxsaddle.png}
  % \end{center}

%   \framebreak

% \textbf{Example:}
% \footnotesize
% \begin{enumerate}
% \item If [$f$ convex $\Leftrightarrow$ all eigenvalues of $H(\xv)$ positive], then local minimum\\
% \item If [$f$ concave $\Leftrightarrow$ all eigenvalues of $H(\xv)$ negative], then local maximum\\
% \item Some eigenvalues positive and some negative $\Leftrightarrow$ saddle point
% \end{enumerate}

% \vspace{0.5cm}

% \begin{center}
% \includegraphics[scale= 0.5]{figure_man/convex.jpg}
% \end{center}

  
  \end{vbframe}

\begin{vbframe}{Conditions for optimality and convexity}

Let $f:\mathcal{S} \to \R$ be convex on convex set $\mathcal{S}$. Then the following holds:

\begin{itemize}
\item Any local minimum is also global minimum 
\item If $f$ strictly convex, $f$ has exactly one local minimum which is also unique global minimum on $\mathcal{S}$
% \item Sublevel sets $S_1 = \{\xv~|~\fx < a\}$ and $S_2 = \{\xv~|~\fx \leq a \}$, $a\in \R$, form convex sets.
\end{itemize}

\framebreak

Def.: \textbf{Saddle point}
\begin{itemize}
    \item all gradients of $x$ are zero but still, $x$ is no optimum
    \item Hessian is indefinite
\end{itemize}

\begin{center}
\includegraphics[scale= 0.5]{figure_man/convex.jpg}
\end{center}




\end{vbframe}


  \endlecture
  \end{document}
  
  
  