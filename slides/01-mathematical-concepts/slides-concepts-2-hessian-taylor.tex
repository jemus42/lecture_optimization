
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\usepackage{graphicx}


\newcommand{\titlefigure}{figure_man/Taylor2D/Taylor2D_1st100.png}
\newcommand{\learninggoals}{
\item Taylor series (Univariate)
\item Hessian Matrix
\item Taylor series (Multivariate)}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization}
\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Hessian Matrix \& Taylor Series}
\lecture{Optimization}
\sloppy
   
\begin{vbframe}{Taylor series (univariate)}

    \textbf{Definition (Taylor's Theorem)}: Let $I \subseteq \R$ an open interval and $a, x \in I$. Further, let $f \in \mathcal{C}^{m+1}(I,\R)$ for $m \in \N$. Then
    $$f(x) = T_{m}(x,a) + R_{m}(x,a).$$
    We call
    $$T_{m}(x,a) = \sum_{k=}^{m} \frac{f^{k}(a)}{k!}(x-a)^{k} = $$
    $$f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^{2} + ... + \frac{f^{(m)}(a)}{m!}(x-a)^{m}$$ the $m$-th Taylor polynomial.
    $$R_{m}(x,a)$$ is called \textbf{remainder term}. We will cover this term later.    

    \framebreak 
    
    \begin{figure}[htp]
        \centering
        \includegraphics[height=0.9\textheight]{figure_man/taylor_univariate.png}
    \end{figure}
    
\end{vbframe}

\begin{vbframe}{Differentiation (Multivariate)}

The second derivative of a multivariate function is defined by its Hessian (if it exists). 

\lz 


  \textbf{Definition (Hessian)}: Let $f \in \mathcal{C}^2(\mathcal{S}, \R)$, $\mathcal{S}\subseteq \R^d$. The \textbf{Hessian matrix} is defined as

  \[ H(\xv) =\nabla^2 \fx =
  \Bigl(\frac{\partial^2 \fx}{\partial x_i \partial x_j}\Bigr)_{i,j=1\ldots d}\]
  
\lz 

\textbf{Example}: Let $f(x_1,x_2) = sin(x_1) \cdot cos(x_2)$. Then:

$$
H(\xv) = \begin{pmatrix}
\text{-cos}(x_2)\cdot\text{sin}(x_1) & \text{-cos}(x_1)\cdot\text{cos}(x_2) 
\\ \text{-cos}(x_1)\cdot\text{sin}(x_2) & \text{-cos}(x_2)\cdot\text{sin}(x_1) 
\end{pmatrix}
$$


\end{vbframe}

\begin{vbframe}{Hessian describes local curvature} 

Local curvature can be analyzed via Eigenspecturm of $H(\xv)$:

\begin{itemize}
  \item The Eigenvector $\bm{v}_\text{max}$ belonging to the largest Eigenvalue $\lambda_\text{max}$ (in absolute terms) points into the the direction of largest curvature. 
  \item Respectively: $\bm{v}_\text{min}$ points into direction of smallest curvature.
  \item Eigenvalues tell us about the definiteness and therefore the curvature of the matrix at a given point. \begin{description}
    \item[positive-definite:] all eigenvalues positive -> concave up
    \item[positive-definite:] all eigenvalues positive -> concave down
    \item[saddle point:] if eigenvalues are positive \& negative.
    \item[no statement possible:] if either eigenvalue is $=0$.
    \end{description}
\end{itemize}

\framebreak

\textbf{Example (continued):} $f(x_1,x_2) = sin(x_1) \cdot cos(x_2)$ and 
\begin{footnotesize}
$$
H(\xv) = \begin{pmatrix}
\text{-cos}(x_2)\cdot\text{sin}(x_1) & \text{-cos}(x_1)\cdot\text{cos}(x_2) 
\\ \text{-cos}(x_1)\cdot\text{sin}(x_2) & \text{-cos}(x_2)\cdot\text{sin}(x_1) 
\end{pmatrix}
$$
The Eigenvalues at $a=(\frac{-\pi}{2},0)$, $b=(0,\frac{-\pi}{2})$ and $c=(\frac{-\pi}{2},0)$ are $-1$ \& $-1$, $1$ \& $-1$ and $1$ \& $1$, respectively. The signs can each be retraced in the following plots:
\end{footnotesize}

\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{figure_man/hessian_3d.png}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{figure_man/hessian_contour.png}
  \end{minipage}
\end{figure} 
\begin{footnotesize}
Therefore, the hessian at $a$ is negative-definite, has a saddle point at $b$ and is positive-definite at $c$.
\end{footnotesize}

\end{vbframe}
\framebreak 

\begin{vbframe}{Multivariate Taylor series}

\textbf{Taylor's theorem (1st order)}: 

$$
  f(\xv) = \underbrace{f(\bm{a}) + \nabla f(\bm{a})^\top (\xv - \bm{a})}_{T_1(\xv, \bm{a})} + R_1(\xv, a) 
$$

\vspace*{-0.3cm}

\begin{footnotesize} \textbf{Example: } $\fx = \text{sin}(2x_1) + \text{cos}(x_2)$, $\bm{a} = (1, 1)^\top$. Since $\nabla \fx = \begin{pmatrix} 2\cdot\text{cos}(2x_1) \\ -\text{sin}(x_2) \end{pmatrix}$

\vspace*{-0.3cm}

\begin{eqnarray*}
  \fx &=& T_1(\xv) + R_1(\xv, \bm{a}) = f(\bm{a}) + \nabla f(\bm{a})^\top (\xv - \bm{a}) + R_1(\xv, \bm{a})\\ &=& \sin(2) + \cos(2) + (2 \cdot \cos(2), - \sin(1))\begin{pmatrix} x_1 - 1 \\ x_2 - 1\end{pmatrix} + R_1(\xv, \bm{a})
\end{eqnarray*}

\end{footnotesize}

\vspace*{-0.3cm}

\begin{columns}
  \begin{column}{0.4\textwidth}
%   \animategraphics[loop,controls,width=\linewidth]{7}{figure_man/Taylor2D/Taylor2D_1st}{0}{359}
    \includegraphics[width = \textwidth]{figure_man/Taylor2D/Taylor2D_1st100.png}
  \end{column}
  \begin{column}{0.4\textwidth}
    \includegraphics[width = \textwidth]{figure_man/Taylor2D/Taylor2D_1st301.png}
  \end{column}
\end{columns}

\framebreak 

\textbf{Taylor's theorem (2nd order)}: 

\vspace*{-0.7cm}

$$
  f(\xv) = \underbrace{f(\bm{a}) + \nabla f(\bm{a})^\top (\xv - \bm{a}) + \frac{1}{2}(\xv - \bm{a})^\top\bm{H}(\bm{a})(\xv - \bm{a})}_{T_2(\xv, \bm{a})} + R_2(\xv, a) 
$$

\begin{footnotesize}
\textbf{Example (continued):} $\fx = \text{sin}(2x_1) + \text{cos}(x_2)$, $\bm{a} = (1, 1)^\top$. Since

$$\nabla \fx = \begin{pmatrix} 2\cdot\text{cos}(2x_1) \\ -\text{sin}(x_2) \end{pmatrix}\text{ and } H(\xv) = \begin{pmatrix} -4 \text{sin}(2x_1) & 0 \\ 0 & -\text{cos}(x_2) \end{pmatrix} $$
 
we get 

\vspace*{-0.8cm}

  \begin{eqnarray*}
    \fx &=& T_1(\xv, \bm{a}) + \frac{1}{2}\begin{pmatrix}x_1 - 1 \\ x_2 - 1 \end{pmatrix}^\top \begin{pmatrix} -4 \text{sin}(2) & 0 \\ 0 & -\text{cos}(1) \end{pmatrix} \begin{pmatrix}x_1 - 1 \\ x_2 - 1 \end{pmatrix} + R_2(\xv, \bm{a})
  \end{eqnarray*}
  \vspace*{-0.2cm}
  \end{footnotesize}
  \begin{columns}
    \begin{column}{0.3\textwidth}
  %   \animategraphics[loop,controls,width=\linewidth]{7}{figure_man/Taylor2D/Taylor2D_2nd-}{0}{359}
      \includegraphics[width = \textwidth]{figure_man/Taylor2D/Taylor2D_2nd-100.png}
    \end{column}
    \begin{column}{0.3\textwidth}
      \includegraphics[width = \textwidth]{figure_man/Taylor2D/Taylor2D_2nd-301.png}
    \end{column}
  \end{columns}  


\framebreak  

\textcolor{red}{@Julia: Update notation}

Formally, Taylor's theorem states: 
    
\textbf{Definition (Taylor's Theorem)}: Let $G \subseteq \R^{n}$ open, $f \in \mathcal{C}^{m+1}(G,\R)$ for $m \in \mathbb{N}_0$.
If $a \in G, h \in \R^{n}$ so that the line segment $S_{a, a+h}$ between $a$ and $a+h \in G$, then
$$f(\bm{a+h}) = \sum_{k=0}^{m} \frac{(\bm{h}\nabla^{k}(f)(\bm{a})}{k!} + R_{m}(\bm{a}) = $$
$$f(\bm{a}) + \frac{(\bm{h}\nabla)(f)(\bm{a})}{1!} + \frac{(\bm{h}\nabla)^{2}(f)(\bm{a})}{2!} + ... + \frac{(\bm{h}\nabla)^{m}(f)(\bm{a})}{m!} + R_{m}(\bm{a})$$

\framebreak 

\textcolor{red}{@Julia: Update notation}

\textbf{Definition (Mutlivariate Directional derivative)}: $\bm{h} = (h_{1}, ..., h_{n})$ $\in \R^{n}$, $G \subseteq \R^{n}$ open, $f : G \rightarrow \R^{n}$ differentiable, $a \in G$.
$$(\bm{h}\nabla(f)(\bm{a}) := \frac{\partial f}{\partial h}(\bm{a}) = \sum_{j=1}^{n} h_{j}\partial_{j}f(\bm(a)) = h_{1}\partial_{j}f(\bm(a)) + ... + h_{n}\partial_{n}f(\bm(a))$$
with ($\bm{h}\nabla$) called \textbf{Differential operator}.

\vspace*{0.2cm}

If $f \in \mathcal{C}^{2}(G,\R)$, we can apply ($\bm{h}\nabla$) again:
$$(\bm{h}\nabla)(f)^{2}(\bm{a}) := (\bm{h}\nabla)(\bm{h}\nabla)(f)(\bm{a}) = $$
$$ = \sum_{j=1}^{n}(\bm{h}\nabla)(\bm{h}_{j}\partial_{j}(f)(\bm{a}) = \sum_{k,j=1}^{n}h_{k}h_{j}\partial_{k}\partial_{j}f(\bm{a})$$


\end{vbframe}


\begin{vbframe}{Taylor approximation}

\textcolor{red}{@Julia: Update notation}

The remainder term $R_m(\bm{a})$ can be written down quantitatively

$$
R_{m}(\bm{a}) := \int_{0}^{1}\frac{(1-t)^{m}}{m!}(\bm{h}\nabla^{m+1})(f)(\bm{a}+t\bm{h}) \text{d}t
$$

\begin{footnotesize}
(this is called the integral form; alternative formulas for the remainder term exist as well, but are not covered here.)
\end{footnotesize}

\lz 

or qualitatively

$$
R_{m}(\bm{a}) \in \order(\|\xv - \bm{a}\|^m) \text{ for } \xv \to \bm{a}
$$

\lz 

If we don't care too much about the approximation error, we just write: 

$$
  \fx \approx T_m(\xv, \bm{a}). 
$$

\framebreak 

\textbf{Note}: 
\begin{itemize}
  \item Higher $m$ gives a better approximation
  \item The $m^{th}$ order taylor series is the best $m^{th}$ order approximation to $\fx$ near $\bm{a}$
\end{itemize}


\begin{columns}
\begin{column}{0.48\textwidth}
  \includegraphics[width = \textwidth]{figure_man/Taylor2D/Taylor2D_1st100.png}
\end{column}
\begin{column}{0.48\textwidth}
  \includegraphics[width = \textwidth]{figure_man/Taylor2D/Taylor2D_2nd-100.png}
\end{column}
\end{columns}

\begin{footnotesize}
Consider $T_2(\xv, \bm{a}) = f(\bm{a}) + \nabla f(\bm{a})^\top (\xv - \bm{a}) + \frac{1}{2}(\xv - \bm{a})^\top\bm{H}(\bm{a})(\xv - \bm{a})$. The first term ensures the \textbf{value} of $T_2$ and $f$ match at $\bm{a}$. The second term ensures the \textbf{slopes} of $T_2$ and $f$ match at $\bm{a}$. The third term ensures the \textbf{curvature} of $T_2$ and $f$ match at $\bm{a}$. 
\end{footnotesize}


\end{vbframe}



  \endlecture
\end{document}