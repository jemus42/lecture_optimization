\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/sa-probs-iter4.png}
\newcommand{\learninggoals}{
\item Motivation
\item Metropolis algorithm
\item Simulated Annealing
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Simulated Annealing}
\lecture{Optimization in Machine Learning}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Motivation}
\textbf{Heuristics} for the optimization of complex (multivariate, non-linear, non-convex) objective functions \\
\lz
Heuristics:
\begin{itemize}
\item Procedure for finding good solutions to complex problems.
\item Does not guarantee optimal/best result (global optimum), but usually good solutions.
\item Goal for complex optimization problems: avoid \enquote{getting stuck} in local optima.
\item Is often used for difficult discrete problems as well.
\end{itemize}

\framebreak

\begin{itemize}
\item \textbf{Simulated annealing} draws analogy between a cooling process (e.g. a metal or liquid) and an optimization problem.
\item If cooling of a liquid material (amount of atoms) is too fast, it solidifies in suboptimal configuration, slow cooling produces crystals with optimal structure (minimum energy stage).
\item Consider atoms of the liquid as a system with many degrees of freedom, analogy to optimization problem of a multivariate function
\item Minimum energy stage corresponds to optimum of objective function.
\item Mathematically it is a local search strategy, with a random option to accept even worse values (sometimes).

\end{itemize}
\end{vbframe}


\begin{vbframe}{Stochastic local search strategy}
\vspace{-0.2cm}
\begin{small}
\begin{itemize}
\item Given is a multivariate objective function $f(\bm{x})$
\item Define a local neighborhood area $V(\bm{x})$ for a given $\bm{x}$
\item Stochastic local search produces a new solution $\bm{x}^{[t+1]}$ from neighbor- hood $V(\bm{x}^{[t]})$ of the solution $\bm{x}^{[t]}$ by sampling of a uniform distribution. 
\item Calculate $f(\bm{x}^{[t+1]})$
\item If $\Delta f = f(\bm{x}^{[t+1]}) - f(\bm{x}^{[t]}) < 0$, $\bm{x}^{[t+1]}$ is accepted as new solution, otherwise a new candidate solution from neighborhood is selected.
\end{itemize}
\end{small}
\vspace{-0.5cm}
\begin{center}
\includegraphics[width=0.75\textwidth]{figure_man/local-search.png}
\end{center}
\vspace{-0.5cm}
%\begin{figure}
%<<echo=FALSE, fig.height=4.25>>=
%int = seq(0,4, length.out = 500)
%f = function(x) x^1.1 * sin(x-2) + 1 - 2.5*sin(3*x-1.5)
%plot(int, f(int), type="l", xlab = "", ylab = "f(x)", xaxt="n")
%x = 2.5
%rad = 0.5
%lines(c(x, x), c(-3, f(x)), lty = 2)
%area = seq(x - rad, x+rad, length.out=200)
%col_vec = c("red", "green")
%points(area, rep(f(x), 200), col = col_vec[(f(area)<f(x))+1], pch=15, cex=0.5)

%x = 2.8
%rad = 0.5
%lines(c(x, x), c(-3, f(x)), lty = 2)
%area = seq(x - rad, x+rad, length.out=200)
%col_vec = c("red", "green")
%points(area, rep(f(x), 200), col = col_vec[(f(area)<f(x))+1], pch=15, cex=0.5)


%lines(c(3.05,3.05), c(-3, f(3.05)), lty=2)
%axis(1, c(2.5, 2.8, 3.05), labels = c(expression(x[0]), expression(x[1]), expression(x[2])),tick=F)
%@
%\caption{local search; green: acceptance range, red: rejection range}
%\end{figure}
\footnotesize{Stochastic local search; green: acceptance range, red: rejection range}
\end{vbframe}

\begin{vbframe}{Metropolis algorithm}
\begin{itemize}
\item Stochastic local search strongly depends on the initialization of $x^{[0]}$ and the neighborhood.
\item Danger of ending up in local minima.
\item Sensible: temporarily allow worse candidate combinations.
\item Metropolis: accept candidate solutions from previous rejection range ($\Delta f > 0$) with probability $\P(\text{acceptance} | \Delta f) = exp(-\frac{\Delta f}{T}).$
\item $T$ denotes the \textbf{temperature}
\end{itemize}

\vspace{-0.5cm}
\begin{center}
\includegraphics[width=0.85\textwidth]{figure_man/metropolis-algorithm.png}
\end{center}
\vspace{-0.6cm}

%\begin{figure}
%<<echo=FALSE, fig.height=4.25>>=
%PDelta = function(Delta_f, Temp) {
%  ret = exp(- Delta_f / (Temp))
%  ret[ret>1] = 1
%  ret
%}


%layout(matrix(1:2, nrow = 1), width = c(5,1.5), height = rep(1,2))


%int = seq(0,4, length.out = 500)
%f = function(x) x^1.1 * sin(x-2) + 1 - 2.5*sin(3*x-1.5)
%plot(int, f(int), type="l", xlab = "", ylab = "f(x)", xaxt="n")
%x = 2.5
%rad = 0.5
%lines(c(x, x), c(-3, f(x)), lty = 2)
%area = seq(x - rad, x+rad, length.out=200)
%col_vec = c("yellow", "green")

%rbPal <- colorRampPalette(c('red','orange', "yellow", "green"))
%col_smooth <- rbPal(50)[as.numeric(cut(seq(0,1,0.02),breaks = 50))]

%points(area, rep(f(x), 200),
%       col = col_smooth[findInterval(PDelta(f(area)-f(x), Temp=3), seq(0,1,0.02))],
%       pch=15, cex=0.5)

%x = 2.75
%rad = 0.5
%lines(c(x, x), c(-3, f(x)), lty = 2)
%area = seq(x - rad, x+rad, length.out=200)
%col_vec = c("orange", "green")
%points(area, rep(f(x), 200),
%       col = col_smooth[findInterval(PDelta(f(area)-f(x), Temp=1.5), seq(0,1,0.02))],
%       pch=15, cex=0.5)
%x = 2.3
%rad = 0.5
%lines(c(x, x), c(-3, f(x)), lty = 2)
%area = seq(x - rad, x+rad, length.out=200)
%points(area, rep(f(x), 200),
%       col = col_smooth[findInterval(PDelta(f(area)-f(x), Temp=0.15), seq(0,1,0.02))],
%       pch=15, cex=0.5)

%x = 1.81
%rad = 0.5
%lines(c(x, x), c(-3, f(x)), lty = 2)
%area = seq(x - rad, x+rad, length.out=200)
%points(area, rep(f(x), 200),
%       col = col_smooth[findInterval(PDelta(f(area)-f(x), Temp=0.15), seq(0,1,0.02))],
%       pch=15, cex=0.5)

%axis(1, c(2.5, 2.75, 2.3, 1.83), labels = c(expression(x[0]), expression(x[1]), expression(x[2]) , expression(x[3])),tick=F)

%#legend
%legend_image <- as.raster(matrix(col_smooth, ncol=1))
%plot(c(0,1),c(0,1), type = 'n', axes = F, xlab = '', ylab = '', main = "")
%rasterImage(legend_image, 0, 0, 1, 1)
%axis(2, at = seq(0, 1, l = 6), labels = seq(1, 0, l = 6), tick = T, las=1,
%     lwd = 0, lwd.ticks = 1)
%@
%\caption{Simulated Annealing schematic, colors: P(acceptance)}
%\end{figure}
\footnotesize{Simulated annealing schematic, colors: $\P$(acceptance)}

\framebreak

\begin{small}
\begin{itemize}
\item New parameter $T$ describes temperature/progress of the system.
\item The higher $T$, the higher the probability to accept worse $\bm{x}$.
\item Atomical view: individual atoms (solution points) of the system can move more freely
\item Local minima can be escaped again, but no convergence can be achieved at constant temperature
\item We come across an important principle of optimization:\\
  \textbf{exploration (high T) vs. exploitation (low T)}
\end{itemize}
\end{small}
\vspace{-0.3cm}
\begin{center}
\includegraphics[width=0.6\textwidth]{figure_man/metropolis-algorithm2.png}
\end{center}


%\begin{figure}
%<<echo=FALSE>>=
%library("RColorBrewer")
%Delta = seq(-5, 50, length.out= 500)

%colvec = brewer.pal(5, "Accent")
%plot(Delta, PDelta(Delta, 250), type = "l", ylim = c(-0.1,1.3), xlab = expression(Delta[f]), ylab = "P(Acceptance)", col = colvec[1], lwd = 2, yaxt="n")
%axis(2, seq(0,1, by=.2))
%abline(h=c(0,1), lty = 2)
%lines(c(0,0), c(0,1), lty=2)
%points(Delta, PDelta(Delta, 125), type = "l", col = colvec[2], lwd = 2)
%points(Delta, PDelta(Delta, 50), type = "l", lwd = 2, col = colvec[3])
%points(Delta, PDelta(Delta, 10), type = "l", lwd = 2,col = colvec[4])
%points(Delta, PDelta(Delta, 2), type = "l", lwd = 2, col = colvec[5])
%legend("top", paste("T =", c(250,125,50,10,2)), fill=colvec, ncol=5)
%@
%\caption{Representation of acceptance probability}
%\end{figure}
%Representation of acceptance probability

\end{vbframe}

\begin{vbframe}{Simulated Annealing}
\begin{itemize}
\item Approach now: start with high temperature to search the whole system (\textbf{exploration})
\item Slowly lower temperature to reach a minimum \\
$\Rightarrow$ sequence of temperatures $T^{[t]}, t \in \N$
\item If temperature depends on simulation time, the procedure is called \textbf{simulated annealing}.
\item Temperature is often kept constant several iterations at a time to search the space of candidate solutions, then multiplied by coefficient $0<c<1$:

$$
T^{[t+1]} = c \cdot T^{[t]}
$$
\end{itemize}

\framebreak


\framebreak
(Choice of) optimization parameters \\
\begin{itemize}
\item Temperature $T$: for any optimization problem, the initial temperature can be the average of a number of random function values.
\vspace{0.1cm}
\item Temperature coefficient $c$: typically between 0.6 and 0.9 ($c<1$)
\vspace{0.1cm}
\item Iterations at the same temperature: typically between 50-100
\vspace{0.1cm}
\item Range $\gamma$: defines area around $\bm{x}^{[t]}$ in which next candidate solution set $\bm{x}^{[t+1]}$ is searched (depends strongly on objective function)
\end{itemize}

\end{vbframe}

% \begin{vbframe}{Simulated Annealing: Algorithmus}
% \begin{algorithm}[H]
%   \begin{footnotesize}
%   \begin{center}
%   \caption{Simulated Annealing}
%    \begin{algorithmic}[1]
%     \State Generiere zufällige Startlösung $x$
%     \State Setze $x_{best} = x$
%     \State Wähle eine monoton fallende Folge von positiven Temperaturwerten $(T_{k})_{k \in N}$
%     \State Wähle Minimale Tmperatur $T_{min}$ als Abbruchkriterium
%     \State Setze $k = 1$
%        \While{$T_{k} >  T_{min}$}
%         \State Wähle in Umgebung von x einen zufälligen Punkt $x_{new}$ aus
%         \If{$f(x_{new}) < f(x)$}  {setze $x = x_{new}$}
%         \ElsIf{
%         {{setze $x = x_{new}$ mit Wahrscheinlichkeit $exp(- \frac{f(x_{new})-f(x)}{T_k})$}}}
%         \EndIf
%         \If{$f(x) < f(x_{best})$} {update $x_{best} = x$}
%         \EndIf
%         \State $k = k + 1$
%       \EndWhile
%     \end{algorithmic}
%     \end{center}
%     \end{footnotesize}
% \end{algorithm}
% \end{vbframe}

\begin{frame}
\frametitle{Example: simulated annealing}


  \begin{center}
  \only<1>{
  \includegraphics[width=0.54\textwidth]{figure_man/sa-himmelblauFun3D.png}
  \includegraphics[width=0.44\textwidth]{figure_man/sa-himmelblauFun2D.png}\\

          \begin{itemize}
            \item Himmelblau's function has several local optima.
            \item We perform $100$ iterations of simulated annealing with the following settings: 
            \begin{itemize}
            \item Proposal points are sampled from a normal distribution ($\sigma = 1.5$) around the current point
            \item Initial temperature of $T^{[0]} = 200$
            \item Constant temperature for the first $50$ iterations
            \item Afterwards, temperature drops by a multiplicative factor of $c = 0.8$ in every iteration
          \end{itemize}
          \end{itemize}}%

  \only<2>{

    \begin{figure}
      \includegraphics[width=0.45\textwidth]{figure_man/sa-iter2.png}    \includegraphics[width=0.45\textwidth]{figure_man/sa-probs-iter2.png} \\
    \end{figure}
          \begin{tiny}
      Left: Optimization surface of the Himmelblau function. Right: Acceptance probability $\P(\text{acceptance})$. 
      \end{tiny} 

    \begin{itemize}
          \item The blue dot is the starting point $(0, 0)$ \\ of the optimization.
          \item For the first $50$ iterations, the temperature is set to $T = 200$. 
          \item A point is proposed (orange)
          \item In the beginning, almost every point is accepted (exploration)
    \end{itemize}}%

  \only<3>{

    \begin{figure}
      \includegraphics[width=0.45\textwidth]{figure_man/sa-iter3.png}    \includegraphics[width=0.45\textwidth]{figure_man/sa-probs-iter3.png} \\
    \end{figure}
          \begin{tiny}
      Left: Optimization surface of the Himmelblau function. Right: Acceptance probability $\P(\text{acceptance})$. 
      \end{tiny} 

    \begin{itemize}
          \item The blue dot is the starting point $(0, 0)$ \\ of the optimization.
          \item For the first $50$ iterations, the temperature is set to $T = 200$. 
          \item A point is proposed (orange)
          \item In the beginning, almost every point is accepted (exploration)
    \end{itemize}}%

  \only<4>{

    \begin{figure}
      \includegraphics[width=0.45\textwidth]{figure_man/sa-iter4.png}    \includegraphics[width=0.45\textwidth]{figure_man/sa-probs-iter4.png} \\
    \end{figure}
          \begin{tiny}
      Left: Optimization surface of the Himmelblau function. Right: Acceptance probability $\P(\text{acceptance})$. 
      \end{tiny} 

    \begin{itemize}
          \item The blue dot is the starting point $(0, 0)$ \\ of the optimization.
          \item For the first $50$ iterations, the temperature is set to $T = 200$. 
          \item A point is proposed (orange)
          \item In the beginning, almost every point is accepted (exploration)
          \item Sometimes we end up with a point that is rejected (e.g., iteration 4) 
    \end{itemize}}%

  \only<5>{

    \begin{figure}
      \includegraphics[width=0.45\textwidth]{figure_man/sa-iter70.png}    \includegraphics[width=0.45\textwidth]{figure_man/sa-probs-iter70.png} \\
    \end{figure}
          \begin{tiny}
      Left: Optimization surface of the Himmelblau function. Right: Acceptance probability $\P(\text{acceptance})$. 
      \end{tiny} 

    \begin{itemize}
          \item From iteration $50$ on, temperature starts $T$ to drop by the multiplicative factor of $c = 0.8$ in every iteration
          \item We see that we cannot move as freely anymore. 
    \end{itemize}}%

  % \only<3>{
  %   \includegraphics[width=0.49\textwidth]{figure_man/sa-iter1.png}
  %   \includegraphics[width=0.49\textwidth]{figure_man/sa-probs-iter1.png}\\

  %   \begin{itemize}
  %         \item Left: The point $\xv^{[2]} = (x,y)$ was sampled in iteration 2, and accepted. 
  %         \item Right: Acceptance probability $P = \exp(-\frac{\Delta f}{T})$ for $\xv^{[2]} = (x,y)$ is $m$. 
  %   \end{itemize}}%

  % \only<4>{
  %   \includegraphics[width=0.49\textwidth]{figure_man/sa-iter1.png}
  %   \includegraphics[width=0.49\textwidth]{figure_man/sa-probs-iter1.png}\\

  %   \begin{itemize}
  %         \item Left: We repeat the procedure for $50$ iterations without changing the temperature.  
  %   \end{itemize}}%

  % \only<4>{
  %   \includegraphics[width=0.49\textwidth]{figure_man/sa-iter1.png}
  %   \includegraphics[width=0.49\textwidth]{figure_man/sa-probs-iter1.png}\\

  %   \begin{itemize}
  %         \item The next 50 iterations are performed with a temperature of $T = 160$. We see that the sampled points focus more around the (local) optima of the function
  %   \end{itemize}}%

  % \only<4>{
  %   \includegraphics[width=0.49\textwidth]{figure_man/sa-iter1.png}
  %   \includegraphics[width=0.49\textwidth]{figure_man/sa-probs-iter1.png}\\

  %   \begin{itemize}
  %         \item After 100 iterations, the temperature would change again. But we stop here. 
  %   \end{itemize}}%


  % %\only<3>{\includegraphics[height = 5cm, width=5.5cm]{figure_man/sa-iter2.png}}%
  % %\only<3>{\includegraphics[height = 5cm, width=5.5cm]{figure_man/sa-probs-iter2.png}%
  %  % \begin{itemize}
  %   %      \item Left: new point
  %    %     \item Right: probability of acceptance $P = \exp(-\frac{\Delta f}{T})$ regarding the new point
  %   %\end{itemize}}%

  % \only<3>{\includegraphics[width=0.49\textwidth]{figure_man/sa-iter50.png}
  %           \includegraphics[width=0.49\textwidth]{figure_man/sa-probs-iter50.png}\\

  %           \begin{itemize}
  %           \item Left: temperature at the beginning constantly high; points scatter strongly (exploration)
  %           \item Right: Downgrading of the acceptance probability still clearly visible (contour lines can still be clearly differentiated)
  %           \end{itemize}}%
  % \only<4>{\includegraphics[width=0.49\textwidth]{figure_man/sa-iter100.png}
  %           \includegraphics[width=0.49\textwidth]{figure_man/sa-probs-iter100.png}\\

  %           \begin{itemize}
  %           \item Left: temperature has already decreased significantly, points scatter less, \enquote{solidification} begins (exploitation)
  %           \item Right: Downgrading of the acceptance probability hardly perceptible any more (contour lines can hardly be differentiated anymore)
  %           \end{itemize}}%

  \end{center}
  \end{frame}



\endlecture
\end{document}

