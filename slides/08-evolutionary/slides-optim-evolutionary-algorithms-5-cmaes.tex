\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/cmaes/cmaes_generations.png}
\newcommand{\learninggoals}{
\item CMA-ES strategy
\item Estimation of distribution
\item Step size control
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{CMA-ES Algorithm}
\lecture{Optimization in Machine Learning}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{Covariance Matrix Adaptation Evolution Strategy (CMA-ES)}

%\begin{vbframe}{CMA-ES as part of many EDAs}
%\textbf{Estimation of Distribution Algorithms} (EDAs) widely used class of algorithms designed to solve optimization problems of the form
%
%\vspace{-10pt}
%\begin{eqnarray*}
%\xv^* = \argmax_{\xv \in \mathcal{S}} f(\xv), \quad \text{where } f:\mathcal{S}\rightarrow \R.
%\end{eqnarray*}
%
%
%Instead of solving above objective directly, EDAs solve related objective:
%\begin{eqnarray*}
%\thetab^\ast = \argmax_{\thetab} \E_{p(\xv|\thetab)} f(\xv),
%\end{eqnarray*}
%\vspace{-10pt}
%
%where $p(\xv|\thetab)$ is a probability density over $\mathcal{S}$, parameterized by $p$ parameters $\thetab \in \R^p$.
%
%\lz
%
%Reason for later formulation: convenience of derivative-free optimization, enabling of rigorous analysis and leveraging of a probabilistic formulation to incorporate auxiliary information.
%\end{vbframe}
%% \framebreak
\begin{frame}{Estimation of Distribution Algorithm}

\begin{minipage}{0.62\textwidth}
\begin{itemize}
    \item General algorithmic template
    \item Instead of population we maintain parameterized distribution to sample offspring from
\end{itemize}

\vspace{0.5cm}
\begin{enumerate}
\item Draw $\lambda$ offsping $\xv^{(i)}$ from $p(\bm{x}|\thetab^{[t]})$
\item Evaluate fitness $f(\xv^{(i)})$ 
%\item Reduce to $\mu$ best offspring
%where $W(\cdot)$ gives weights for each $\xv^{(i)}$, typically $0$ or $1$ (order-preserving fitness transformation)
\item Update $\thetab^{[t+1]}$ with $\mu$ best offspring 
\end{enumerate}

%This core algorithm is often modified in a variety of ways to improve performance via \textbf{Covariance Matrix Adaptation (CMA-ES)}.
\end{minipage}\hfill
\begin{minipage}{0.35\textwidth}\raggedleft
\begin{figure}
  \includegraphics[width=1\textwidth, height=0.9\textheight]{figure_man/cmaes/cmaes_eda.png}
\end{figure}
\end{minipage}

\end{frame}


% \begin{vbframe}{CMA-ES}

% Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is

% \begin{itemize}
% \item A state-of-the-art tool in evolutionary computation 
% \item Stochastic/randomized black box optimization algorithm
% \item For usage in continuous domain
% \item For non-linear, non-convex optimization problems
% \item Particularly effective in \enquote{hard}/ill-conditioned settings
% \end{itemize}
% \vspace{0.3cm}
% Detailed information on CMA-ES can be found in

% \begin{enumerate}
% \item Nikolaus Hansen. The CMA Evolution Strategy. 2016
% \item A. Auger, N. Hansen: Tutorial CMA-ES: Evolution Strategies and Covariance Matrix Adaptation. 2012.
% \end{enumerate}

%\end{vbframe}


\begin{vbframe}{Covariance Matrix Adaptation ES}


\begin{itemize}
 \item Sample distribution is multivariate Gaussian
\end{itemize}

\vspace{-10pt}

\begin{eqnarray*}
\xv^{[t+1](i)} \sim \bm{m}^{[t]} + \sigma^{[t]} \normal (\bm{0}, \bm{C}^{[t]}) \quad \text{for } i = 1, \dots, \lambda.
\end{eqnarray*}
\vspace{-20pt}

\begin{figure}
  \includegraphics[width=0.6\textwidth, height=0.45\textheight]{figure_man/cmaes/cmaes_generations.png}
\end{figure}

\framebreak
Sample distribution is multivariate Gaussian
$$
\xv^{[t+1](i)} \sim \bm{m}^{[t]} + \sigma^{[t]} \normal (\bm{0}, \bm{C}^{[t]}) \quad \text{for } i = 1, \dots, \lambda
$$
\vspace{-20pt}
\begin{itemize}
\item $\xv^{[t+1](i)} \in \R^d$ $i$-th offspring; $\lambda \geq 2$ number of offspring
%\item $\normal(\bm{0}, \bm{C}^{[t]})$ is multivariate normal distribution with zero mean, covariance matrix $\bm{C}^{[t]}$. \textit{Note}: $\bm{m}^{[t]} + \sigma^{[t]} \normal(\bm{0}, \bm{C}^{[t]}) \sim \normal(\bm{m}^{[t]}, (\sigma^{[t]})^2 \bm{C}^{[t]})$.
\item $\bm{m}^{[t]} \in \R^d$ mean value and $\bm{C}^{[t]} \in \R^{d \times d}$ covar matrix
\item $\sigma^{[t]} \in \R_{+}$ \enquote{overall} standard deviation/step size
% Up to the scalar factor $\sigma^{(g)^2}$, $\bm{C}^{(g)}$ is the covariance matrix of the search distribution.
\end{itemize}

\begin{figure}
  \includegraphics[width=0.5\textwidth]{figure_man/cmaes/cmaes_generations.png}
\end{figure}


$\rightarrow$ \textit{How to calculate $\bm{m}^{[t+1]}$, $\bm{C}^{[t+1]}$, $\sigma^{[t+1]}$ for next generation $t+1$?}
\end{vbframe}


% \begin{vbframe}{Recall: Evolution Strategies (ES)}

% New search points are sampled normally distributed as perturbations of $\bm{m}$:
% \begin{eqnarray*}
% \xv_k \sim \bm{m} + \sigma \normal_k (\bm{0}, \bm{C}) \quad \text{for } i = 1, \dots, \lambda
% \end{eqnarray*}

% where $\xv_k$, $\bm{m} \in \R^{n}$, $\sigma \in \R_{+}$, $\bm{C} \in \R^{n \times n}$

% \begin{itemize}
% \item Mean vector $\bm{m} \in \R^d$ represents the favorite solution
% \item Step-size $\sigma \in \R_{+}$ controls the step length
% \item Covariance matrix $\bm{C} \in \R^{n \times n}$ determines the shape of the distribution ellipsoid.
% \end{itemize}

% Remaining question: How to update $\bm{m}$, $\bm{C}$ and $\sigma$?
% \end{vbframe}



% \begin{vbframe}{CMA-ES: Basic Method}
% \begin{enumerate}
% \item \textbf{Sample maximum entropy} distribution
% \item[] $x_i = m + \sigma \normal_i(\bm{0}, \bm{C})$ multivariate normal distribution
% \item \textbf{Ranking} solutions according to their fitness
% \item[] Invariance to order-preserving transformations
% \item \textbf{Update mean and covariance matrix} by natural gradient ascend, improving the \enquote{expected fitness} and the likelihood for good steps
% \item[] PCA $\rightarrow$ variable metric, new problem representation, invariant under changes of the coordinate system
% \item \textbf{Update step-size} based on non-local information
% \item[] Exploit correlations in the history of steps.
% \end{enumerate}
% \end{vbframe}



\begin{frame}
% \begin{eqnarray*}
% \bm{m} \leftarrow \bm{m} + \sigma \bm{y}_w, \quad \bm{y}_w = \sum_{i=1}^{\mu} w_i \bm{y}_{i:\lambda}, \quad \bm{y}_i\sim \normal_i(\bm{0}, \bm{C})
% \end{eqnarray*}

\begin{figure}
\begin{overprint}

\centering
\only<1>{
\frametitle{CMA-ES: Basic Method - Iteration 1}
\begin{enumerate}
\addtocounter{enumi}{-1}
\item Initialize $\bm{m}^{[0]},\sigma^{[0]}$ problem-dependent and $\bm{C}^{[0]}=\bm{\I}_{d}$
\item \textbf{Sample} from distribution
\item[] $\xv^{[1](i)} = \bm{m}^{[0]} + \sigma^{[0]} \normal(\bm{0}, \bm{C}^{[0]})$ multivariate Gaussian
\item[]
\end{enumerate}
%\vspace{-0.5cm}
\scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_1.png}}
%\vspace{-0.5cm}\\

Initial distribution $\normal(\bm{m}^{[0]}, (\sigma^{[0]})^2 \bm{\I}_2)$ of generation $t=0$.
}

\only<2>{
\frametitle{CMA-ES: Basic Method - Iteration 1}
\begin{enumerate}
\item \textbf{Sample} from distribution
\item[] $\xv^{[1](i)} = \bm{m}^{[0]} + \sigma^{[0]} \normal(\bm{0}, \bm{C}^{[0]})$ multivariate normal distribution.
\item[]
\item[]
\end{enumerate}
%\vspace{-0.5cm}
\scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_2.png}}
%\vspace{-0.5cm}\\

Initial distribution $\normal(\bm{m}^{[0]}, (\sigma^{[0]})^2 \bm{\I}_2)$  of generation $t=0$, $\lambda = 7$.
}

\only<3>{
\frametitle{CMA-ES: Basic Method - Iteration 1}
\begin{enumerate}
\addtocounter{enumi}{1}
\item \textbf{Selection and recombination} of $\mu<\lambda$ best-performing offspring using fixed weights $w_1\geq\ldots\geq w_{\mu}>0,\sum_{i=1}^{\mu} w_i = 1$. %solutions according to their fitness (\textit{Selection} of $\mu$ best)
\item[] $\xv_{i:\lambda}$ is $i$-th ranked solution, ranked by $f(\xv_{i:\lambda})$.
\item[]
\end{enumerate}
%\vspace{-0.5cm}
\scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_21.png}}
\vspace{-0.5cm}\\
Calculation of auxiliary variables ($\mu=3$ points) $\bm{y}_w^{[1]} := \sum_{i=1}^{\mu} w_i (\xv_{i:\lambda}^{[1]}-\bm{m}^{[0]})/\sigma^{[0]} := \sum_{i=1}^{\mu} w_i \bm{y}_{i:\lambda}^{[1]}$%, using $\mu = 3$ points. %(high fitness $\rightarrow$ high weights)

%Movement to new population mean $\bm{m}^{[1]}$ (disregarding $\sigma$) of the $\mu = 3$ selected points (high fitness $\rightarrow$ high weights, $\bm{y}_w := \sum_{i=1}^{\mu} w_i \xv_{i:\lambda}$).
}

\only<4>{
\frametitle{CMA-ES: Basic Method - Iteration 1}
\begin{enumerate}
\addtocounter{enumi}{2}
\item \textbf{Update mean}
\item[]
\item[]
\item[]
\end{enumerate}
%\vspace{-0.5cm}
\scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_3.png}}
\vspace{-0.5cm}\\
Movement towards the new distribution with mean $
\bm{m}^{[1]} = \bm{m}^{[0]} + \sigma^{[0]} \bm{y}_{w}^{[1]}$.
}

\only<5>{
\frametitle{CMA-ES: Basic Method - Iteration 1}
\begin{enumerate}
\addtocounter{enumi}{3}
\item \textbf{Update covariance matrix} %(\textit{Recombination}),
\item[] Roughly: elongate density ellipsoid in direction of successful steps.
\item[] $\bm{C}^{[1]}$ reproduces successful points with higher probability than $\bm{C}^{[0]}$. %Improving \enquote{expected fitness} and likelihood for successful steps.
\item[]
\end{enumerate}
%\vspace{-0.5cm}
\scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_4.png}}
\vspace{-0.5cm}\\

Update $\bm{C}$ using sum of outer products and learning rate $c_{\mu}$ (simplified): $\bm{C}^{[1]} = (1-c_{\mu}) \bm{C}^{[0]} + c_{\mu} \sum_{i=1}^{\mu} w_i \bm{y}_{i:\lambda}^{[1]}(\bm{y}_{i:\lambda}^{[1]})^{\top}$ (rank-$\mu$ update).
%\bm{y}_w^{[1]} (\bm{y}_w^{[1]})^\top$ (Rank 1 update).
}

\only<6>{
\frametitle{CMA-ES: Basic Method - Iteration 2}
\begin{enumerate}
\item \textbf{Sample} from distribution for new generation
\item[] %(\textit{Mutation}) with $\xv^{[2](i)} = \bm{m}^{[1]} + \sigma^{[1]} \normal(\bm{0}, \bm{C}^{[1]})$ form multivariate
\item[] %normal distribution.
\item[]
\end{enumerate}
%\vspace{-0.5cm}
\scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_5.png}}

%New Distribution $\normal^{[1]}\sim (\bm{m}^{[1]}, \bm{C}^{[1]})$ (disregarding $\sigma$) \\
%of generation $t=1$, $\lambda = 6$.
}

\only<7>{
\frametitle{CMA-ES: Basic Method - Iteration 2}
\begin{enumerate}
\addtocounter{enumi}{1}
\item \textbf{Selection and recombination} of $\mu<\lambda$ best-performing offspring
\item \textbf{Update mean}
\item[] %$\xv_{i:\lambda}$ as $i$-th ranked solution point, such that $f(\xv_{1:\lambda}) \leq \dots \leq f(\xv_{\lambda:\lambda})$.
\item[]
\end{enumerate}
%\vspace{-0.5cm}
\scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_6.png}}

%Movement to new population mean $\bm{m}^{[2]}$ (disregarding $\sigma$) of the $\mu = 3$ selected points (high fitness $\rightarrow$ high weights, $\bm{y}_w := \sum_{i=1}^{\mu} w_i \xv_{i:\lambda}$).
}

\only<8>{
\frametitle{CMA-ES: Basic Method - Iteration 2}
\begin{enumerate}
\addtocounter{enumi}{3}
\item \textbf{Update covariance matrix} %(\textit{Recombination})
\item[] %improving \enquote{expected fitness} and likelihood for good steps.
\item[]
\item[]
\end{enumerate}
%\vspace{-0.5cm}
\scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_7.png}}

%Blue circle as a mixture of $\bm{C}$ and step $\bm{y}_w$: $\bm{C}^{[2]} \leftarrow 0.8 \bm{C}^{[1]} + 0.2  \bm{y}_w^{[1]} (\bm{y}_w^{[1]})^\top$.
}

\only<9>{
\frametitle{CMA-ES: Basic Method - Iteration 2}
\begin{enumerate}
\addtocounter{enumi}{4}
\item \textbf{Update step-size} exploiting correlation in history of steps.
\item[] steps point in similar direction $\implies$ increase step-size
\item[] steps cancel out $\implies$ decrease step-size
\item[]
\end{enumerate}
%\vspace{-0.5cm}
\scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_8.png}}

%Movement towards the new distribution (disregarding $\sigma$) with mean $
%\bm{m}^{[2]} = \bm{m}^{[1]} + \sigma^{[1]} \bm{y}_{w}^{[1]}$.
}
\end{overprint}
\end{figure}
\end{frame}



%\begin{vbframe}{Updating \MakeLowercase{$\bm{m}$}: The $(\mu/\mu_W, \lambda)$-ES}
%
%$(\mu/\mu_W, \lambda)$-ES marks the \textbf{E}volution \textbf{S}trategy with $\bm{\mu}$ parents, \textbf{W}eighted recombination of all $\bm{\mu}$ parents and $\bm{\lambda}$ offspring.
%
%\lz
%
%Let $\xv_{i:\lambda}$ be the $i$-th ranked solution point, the new mean vector is
%% such that $f(\xv_{1:\lambda}) \leq \dots \leq f(\xv_{\lambda:\lambda})$.
%
%\vspace{-10pt}
%\begin{eqnarray*}
%\bm{m}^{[t+1]} = \bm{m}^{[t]} + \sigma^{[t]} \underbrace{\sum_{i=1}^{\mu} w_i \xv_{i:\lambda}^{[t]}}_{=: \bm{y}_w^{[t]}}
%\end{eqnarray*}
%
%where $w_1 \geq \dots \geq w_{\mu} > 0$, $\sum_{i=1}^{\mu} w_i = 1$ and $\frac{1}{\sum_{i=1}^{\mu} w_i^2} =: \mu_w \approx \frac{\lambda}{4}$.
%
%\lz
%
%The best $\mu$ points are selected from the new solutions (non-elitistic) and weighted intermediate recombination is applied.
%
%\lz
%
%If $w_{i=1:\mu} = 1/\mu$ then $\bm{y}_w$ is equal to the mean of the $\mu$ best points.
%\end{vbframe}


%\begin{vbframe}{Updating \MakeLowercase{$\bm{m}$}: The $(\mu/\mu_W, \lambda)$-ES}
%
%Remarks on weights $w_i$
%
%\begin{itemize}
%\item $w_{i=1:\mu} \in \R_{>0}$ are positive weight coefficients for recombination
%\item Typically chosen as weighted average of $\mu$ selected points $w_{i=1:\mu} = 1/\mu$
%\item Assigning different weights $w_i$ should be interpreted as a selection mechanism
%\item Approaches exist, which give the remaining $\lambda-\mu$ points negative weights, such that $\lambda$ weights in total are used (e.g active covariance matrix adaptation)
%\item Weights depend only on the ranking, not on the function values directly $\rightarrow$ renders the algorithm invariant under order-preserving transformation of the objective function
%\end{itemize}
%\end{vbframe}

\begin{vbframe}{Updating $\bm{C}$: Full Update}
Full CMA update of $\bm{C}$ combines rank-$\mu$ update with a rank-$1$ update using exponentially smoothed evolution path $\bm{p}_c \in \mathbb{R}^{d}$ of successive steps and learning rate $c_1$:
$$\bm{p}_{c}^{[0]}=\bm{0}, \quad \bm{p}_{c}^{[t+1]} = (1-c_1)\bm{p}_{c}^{[t]} + \sqrt{\frac{c_1(2-c_1)}{\sum_{i=1}^{\mu}w_i^2}}\bm{y}_w$$
Final update of $\bm{C}$ is
$$\bm{C}^{[t+1]}=(1-c_1-c_{\mu}{\scriptstyle\sum} w_j)\bm{C}^{[t]}+c_1 \underbrace{\bm{p}_{c}^{[t+1]}(\bm{p}_{c}^{[t+1]})^{\top}}_{\text{rank-$1$}}+c_{\mu}\underbrace{\sum_{i=1}^{\mu}w_i \bm{y}_{i:\lambda}^{[t+1]}(\bm{y}_{i:\lambda}^{[t+1]})^{\top}}_{\text{rank-$\mu$}}$$
\vspace{-0.4cm}
\begin{itemize}
    \item Correlation between generations used in rank-$1$ update
    \item Information from entire population is used in rank-$\mu$ update
\end{itemize}
\end{vbframe}

%\begin{vbframe}{Updating $C$: CMA - Rank-One Update}
%Initialize $\bm{m} \in \R^d$ and $\bm{C} = \bm{\I}$, set $\sigma = 1$, learning rate $c_{cov} \approx 2/d^2$. While not terminate

%\begin{align*}
%\xv^{(i)} &= \bm{m} + \sigma \normal_i(\bm{0}, \bm{C}) \\
%\bm{m} &\leftarrow \bm{m} + \sigma \bm{y}_w, \quad \text{where } \bm{y}_w = \sum_{i=1}^\mu \bm{w}_i(\xv_{i:\lambda}-\bm{m})/\sigma \\
%\bm{C} &\leftarrow (1-c_{cov}) \bm{C} + c_{cov}\mu_w \underbrace{\bm{y}_w\bm{y}_w^\top}_{\text{rank-one}}, \quad \text{where } \mu_w = \frac{1}{\sum_{i=1}^\mu w_i^2}
%\end{align*}

%The rank-one update was developed in several domains independently, conducting a \textbf{principle component analysis} (PCA) of steps $\bm{y}_w$ sequentially in time and space.

% \lz

% \textit{In principle}: the adaptation increases the likelihood of successful steps $\bm{y}_w$ to appear again.

% \textit{Different viewpoint}: the adaptation follows a natural gradient approximation of the expected fitness.

% \framebreak

% \begin{eqnarray*}
% \bm{C} &\leftarrow (1-c_{cov}) \bm{C} + c_{cov}\mu_w \bm{y}_w\bm{y}_w^\top
% \end{eqnarray*}

% \begin{itemize}
% \item Conducting a \textbf{principle component analysis} (PCA) of steps $\bm{y}_w$ sequentially in time and space
% \item Approximation of the \textbf{inverse Hessian} on quadratic functions
% \item Learning of a new \textbf{rotated problem representation}
% \item[] Components only independent in the new representation
% \item Learning of all \textbf{pairwise dependencies} between variables
% \item[] Dependencies reflected by off-diagonal entries in the covariance matrix
% \item Learning of a \textbf{new} (Mahalanobis) \textbf{metric}
% \item For $\mu = 1$: Conducting a \textbf{natural gradient ascent} on the normal distribution $\normal$ (independent of the given coordinate system).
% \end{itemize}

%\end{vbframe}

%\begin{vbframe}{Updating $C$: CMA - Cumulation}
%\enquote{Cumulation} as a widely used technique and known under various names (\textit{exponential smoothing} in forecasting and time series, exponentially weighted \textit{moving average}, \textit{iterate averaging} in stochastic approximation, etc.).
%
%\lz
%
%Using cumulation / an evolution path for the rank-one update of the covariance matrix reduces the number of function evaluations to adapt to a straight ridge from about $\order(d^2)$ to $\order(d)$.
%
%\lz
%
%For the evolution/search path taken over a number of generation steps an exponentially weighted sum of steps $\bm{y}_w$ is used:
%
%\begin{eqnarray*}
%\bm{p}_c \propto \sum_{t = 0}^T \underbrace{(1-c_{\bm{c}})^{T-i}}_{\substack{\text{exponentially} \\ \text{fading weights}}} \bm{y}_w^{[t]}
%\end{eqnarray*}
%
%\framebreak
%
%Cumulation as \textit{recursive construction of the evolution path}:
%
%\begin{eqnarray*}
%\bm{p}_c^{[t+1]} = \underbrace{(1- c_{\bm{c}})}_{\text{decay factor}} \bm{p}_c^{[t]} + \underbrace{\sqrt{1-(1-c_{\bm{c}})^2}}_{\text{normalization factor}} \sqrt{\mu_w} \underbrace{\bm{y}_w^{[t]}}_{input},
%\end{eqnarray*}
%
%where $\bm{y}_{w}^{[t]} = \frac{\bm{m}^{[t+1]} - \bm{m}^{[t]}}{\sigma^{[t]}}$ and $\mu_w = \frac{1}{\sum w_i^2}$, $c_{C} << 1$. %FIXME: \ll
%History information is accumulated in the evolution path.
%
%\begin{figure}
%  \includegraphics[width=0.8\textwidth, height=0.4\textheight]{figure_man/cmaes/cmaes_cumulation_1.png}
%\end{figure}
%
%\framebreak
%
%
%$\bm{y}_w \bm{y}_w^\top$ was used for updating $\bm{C}$ and because $\bm{y}_w \bm{y}_w^\top = -\bm{y}_w (-\bm{y}_w)^\top$ the sign of $\bm{y}_w$ is lost.
%
%\lz
%
%The \textbf{sign information} (signifying correlation between steps) is (re-)introduced by using the \textit{evolution path}.
%
%\begin{align*}
%\bm{p}_c^{[t+1]} &= \underbrace{(1- c_{\bm{c}})}_{\text{decay factor}} \bm{p}_{\bm{c}}^{[t]} + \underbrace{\sqrt{1-(1-c_{\bm{c}})^2}}_{\text{normalization factor}} \sqrt{\mu_w} \bm{y}_w^{[t]} \\
%\bm{C}^{[t+1]} &= (1- c_{cov}) \bm{C}^{[t]} + c_{cov} \underbrace{\bm{p}_c^{[t+1]} (\bm{p}_c^{[t+1]})^\top}_{\text{rank-one}},
%\end{align*}
%
%where $\mu_w = \frac{1}{\sum w_i^2}, c_{cov} << c_{\bm{C}} << 1$, such that $1/{c_{\bm{c}}}$ is the \enquote{backward time horizon}. %FIXME: \ll
%
%\framebreak
%
%\begin{align*}
%\textcolor{green}{\bm{p}_c^{[t+1]}} &= \underbrace{(1- \textcolor{blue}{c_{\bm{c}}})}_{\text{decay factor}} \textcolor{green}{\bm{p}_{\bm{c}}^{[t]}} + \underbrace{\sqrt{1-(1-\textcolor{blue}{c_{\bm{c}}})^2}}_{\text{normalization factor}} \sqrt{\mu_w} \bm{y}_w^{[t]} \\
%\textcolor{green}{\bm{C}^{[t+1]}} &= (1- \textcolor{blue}{c_{cov}})\textcolor{green}{\bm{C}^{[t]}} + \textcolor{blue}{c_{cov}} \underbrace{\textcolor{green}{\bm{p}_c^{[t+1]} (\bm{p}_c^{[t+1]}})^\top}_{\text{rank-one}},
%\end{align*}
%\vspace{-10pt}
%
%where $\mu_w = \frac{1}{\sum \textcolor{blue}{w_i}^2}, \textcolor{blue}{c_{cov}} << \textcolor{blue}{c_{\bm{C}}} << 1$, such that $1/{c_{\bm{c}}}$ is the \enquote{backward time horizon}. %FIXME: \ll
%
%\begin{figure}
%  \includegraphics[width=0.8\textwidth, height=0.37\textheight]{figure_man/cmaes/cmaes_cumulation_2.png}
%\end{figure}
%
%\end{vbframe}

%
%\begin{vbframe}{Updating $C$: CMA - Rank-$\mu$ Update}
%In case of \textit{large population sizes $\lambda$} the \textbf{rank-$\mu$ update} extends the update rule using $\mu > 1$ vectors to update$\bm{C}$ at each generation step.
%
%\lz
%
%The weighted empirical covariance matrix computes a weighted mean of the outer products of the best $\mu$ steps and has rank $\min(\mu, d)$ with probability 1.
%
%\vspace{-10pt}
%
%\begin{eqnarray*}
%\bm{C}_\mu^{[t+1]} = \sum_{i=1}^\mu \bm{w}_i \xv_{i:\lambda}^{[t+1]} (\xv_{i:\lambda}^{[t+1]})^\top
%\end{eqnarray*}
%
%The rank-$\mu$-update then reads
%\begin{eqnarray*}
%\bm{C}^{[t+1]} = (1-c_{cov}) \bm{C}^{[t]} + c_{cov} \bm{C}_{\mu}^{[t+1]},
%\end{eqnarray*}
%
%where $c_{cov} \approx \mu_w/d^2$ and $c_{cov} \leq 1$.
%
%\framebreak
%
%\begin{figure}
%  \includegraphics[width=1\textwidth, height=0.3\textheight]{figure_man/cmaes/cmaes_rankmu.png}
%\end{figure}
%
%\begin{enumerate}
%  \item Sampling $\lambda = 150$ solutions, where $\bm{C}^{[t]} = \bm{\I}$ and $\sigma^{[t]} = 1$.
%  \item[] $\xv^{[t+1](i)} = \bm{m}^{[t]} + \sigma^{[t]} \normal(\bm{0}, \bm{C}^{[t]})$
%  \item Calculation $\bm{C}$, where $\mu = 50$, $w_1 = \dots = w_{\mu} = 1/\mu$ and $c_{cov}=1$
%  \item[] $\bm{C}_{\mu}^{[t+1]} = 1/\mu \sum \xv_{1:\lambda}^{[t]} (\xv_{1:\lambda}^{[t]})^\top$ and $\bm{C}^{[t+1]} = (1-1) \times \bm{C}^{[t]} + 1 \times \bm{C}_{\mu}^{[t+1]}$
%  \item New distribution
%  \item[] $\bm{m}^{[t+1]} = \bm{m}^{[t]} + 1/\mu \sum \xv_{1:\lambda}^{[t]}$.
%\end{enumerate}
%
%\end{vbframe}
%
%
%\begin{vbframe}{Updating $C$: Rank-One and Rank-$\mu$ Update}
%\textbf{Rank-one update}
%
%\begin{itemize}
%\item Uses the evolution path
%\item Can reduce the number of \textit{function evaluations} to adapt to straight ridges from about $\order(d^2)$ to $\order(d)$.
%\end{itemize}
%
%\textbf{Rank-$\mu$ update}
%
%\begin{itemize}
%\item Increases the learning rate in large populations and therefore the primary mechanism for large populations (thumb rule: $\lambda \geq 3d + 10$)
%\item Can reduce the number of \textit{generations} from about $\order(d^2)$ to $\order(d)^{(12)}$, given $\mu_w\propto \lambda \propto d$.
%\end{itemize}
%
%
%\textbf{Hybrid Update}: rank-one and rank-$\mu$ update can be combined.
%\end{vbframe}



\begin{vbframe}{Updating $\sigma$: Methods Step-Size Control}
\begin{itemize}
\setlength\itemsep{1.0em}
\item \textbf{$1/5$-th success rule}: increases the step-size if more than 20 \% of the new solutions are successful, decrease otherwise
\item \textbf{$\sigma$-self-adaptation}: mutation is applied to the step-size and the better - according to the objective function value - is selected
\item \textbf{Path length control via cumulative step-size adaptation (CSA)}\\ Intuition:
\begin{itemize}
    \item Short cumulative step-size $\triangleq$ steps cancel $\to$ decrease $\sigma^{[t+1]}$ 
    \item Long cumulative step-size $\triangleq$ corr. steps $\to$ increase $\sigma^{[t+1]}$ 
\end{itemize}
\vspace{0.3cm}
\begin{center}
\includegraphics[width=0.7\textwidth]{figure_man/cmaes/cumulative-step-size.png}
\end{center}

%\item Alternative step-size adaptation mechanism: two-point step-size adaptation, median success rule, population success rule.
\end{itemize}
\end{vbframe}

%\begin{vbframe}{Updating $\sigma$: Path Length Control (CSA)}
%Measure the length of the evolution path with informal steps:
%\begin{itemize}
%\item perpendicular under random selection (in expectation)
%\item perpendicular in the desired solution (to be most effective)
%\end{itemize}
%
%\begin{figure}
%  \includegraphics[width=1\textwidth, height=0.33\textheight]{figure_man/cmaes/cmaes_path.png}
%\end{figure}
%
%Pathway of mean vector $\bm{m}$ in the generation sequence of above pictures
%\begin{enumerate}
%\item decreases $\sigma$ as single steps cancel each other off
%\item ideal case as single steps are uncorrelated
%\item increases $\sigma$ as single steps point in same direction
%\end{enumerate}
%
%\framebreak
%
%Initialize $\bm{m} \in \R^d$, $\sigma \in \R_+$, evolution path $p_\sigma = \bm{0}$.
%
%Set $c_\sigma \approx 4/d$, $d_\sigma \approx 1$.
%
%\begin{align*}
%\bm{m}^{[t+1]} &= \bm{m}^{[t]} + \sigma^{[t]} \bm{y}_w^{[t]}\\
%\bm{p}_\sigma^{[t+1]} &= (1- c_\sigma) \bm{p}_\sigma^{[t]} + \underbrace{\sqrt{1-(1-c_\sigma)^2}}_{\text{accounts for } 1-c_\sigma} \underbrace{\sqrt{\mu_w}}_{\text{account for} w_i} \bm{y}_w^{[t]} \\
%\sigma^{[t+1]} &= \sigma^{[t]} \times \underbrace{\exp\biggl( \frac{c_\sigma}{d_\sigma}\Bigl(\frac{||\bm{p}_\sigma^{[t+1]}||}{\E||\normal(\bm{0}, \bm{\I})||} - 1 \Bigl)\biggl)}_{>1 \Longleftrightarrow ||\bm{p}_\sigma|| \text{ is greater than its expectation}}
%\end{align*}
%
%\framebreak



% \begin{vbframe}{Updating $\sigma$: Path Length Control (CSA)}
% \begin{figure}
%   \includegraphics[width=1\textwidth, height=0.7\textheight]{figure_man/cmaes/cmaes_step-size.png}
% \end{figure}

% CSA effective and robust for $\lambda \leq n$.

% \end{vbframe}


% \begin{frame}{Updating $C$: CMA - Rank-One Update}
% \begin{eqnarray*}
% \bm{m} \leftarrow \bm{m} + \sigma \bm{y}_w, \quad \bm{y}_w = \sum_{i=1}^{\mu} w_i \bm{y}_{i:\lambda}, \quad \bm{y}_i\sim \normal_i(\bm{0}, \bm{C})
% \end{eqnarray*}

% \begin{figure}
% \begin{overprint}
% \centering
% \only<1>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_1.png}}

% Initial distribution with $\bm{C} = 1$.}
% \only<2>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_2.png}}

% Initial distribution with $\bm{C} = 1$.}
% \only<3>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_3.png}}

% Movement to the population mean $m$ (disregarding $\sigma$) with $\bm{y}_w$.}
% \only<4>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_4.png}}

% Blue circle as a mixture of $\bm{C}$ and step $\bm{y}_w$: $\bm{C} \leftarrow 0.8\times \bm{C} + 0.2 \times \bm{y}_w \bm{y}^\top$.}
% \only<5>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_5.png}}

% Movement towards the new distribution (disregarding $\sigma$).}
% \only<6>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_6.png}}

% New Distribution (disregarding $\sigma$).}
% \only<7>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_7.png}}

% Movement to the population mean $\bm{m}$.}
% \only<8>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_8.png}}

% Green circle as a mixture of $\bm{C}$ and step $\bm{y}_w$: $\bm{C} \leftarrow 0.8\times \bm{C} + 0.2 \times \bm{y}_w \bm{y}^\top$.}
% \only<9>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_9.png}}

% Movement towards the new distribution (disregarding $\sigma$).}
% \end{overprint}
% \end{figure}
% \end{frame}


\endlecture
\end{document}

