
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/ml_linreg_example_2.pdf}
\newcommand{\learninggoals}{
\item Definition
\item Max. Likelihood 
\item Normal regression
\item Risk Minimization
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization}
\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Unconstrained problems}
\lecture{Optimization}
\sloppy

\begin{vbframe}{Unconstrained optimization problem}

$$
\min_{\xv \in \mathcal{S}} \fx
$$
with objective function
$$
f: \; \mathcal{S} \to \R.
$$

\lz 

The problem is called

\begin{itemize}
	\item \textbf{unconstrained}, if the domain $\mathcal{S}$ is not restricted: 
	$$
		\mathcal{S} = \R^d
	$$
	\item \textbf{smooth} if $f$ is at least $\in \mathcal{C}^1$
	\item \textbf{univariate} if $d = 1$, and \textbf{multivariate} if $d > 1$.  
	\item \textbf{convex} if $f$ convex function and $\mathcal{S}$ convex set
\end{itemize}

\end{vbframe}

% \begin{vbframe}{Concept: Univariate vs. multivariate}

% If we are optimizing w.r.t. one variable, i.e. $\text{dim}(\mathcal{S}) = 1$, the problem is an univariate optimization problem. If $\mathcal{S}$ is multi-dimensional, we are talking about multivariate optimization. 
	
% \begin{center}
% 	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/ml_poisson_example_2.pdf} \includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/ml_linreg_example_3.png} \\
% 	\begin{footnotesize}
% 	Left (Univariate): The log-likelihood of a Poisson distribution. The optimization problem is an univariate optimization problem with $\mathcal{S} = [0, 1]$. Right (Multivariate): The sum of squared errors with regards to the coefficients $(\theta_0, \theta_1)$ of a regression. 
% 	\end{footnotesize}
% \end{center}

% \end{vbframe}

\begin{vbframe}{Note: A Convention in Optimization}

W.l.o.g., we always \textbf{minimize} functions $f$. 

\lz

Maximization results from minimizing $-f$.

\begin{center}
	\begin{footnotesize}
	\includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/ml_poisson_example_2.pdf} \includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/ml_poisson_example_3.pdf} \\
	Maximizing the log-likelihood (left) is equivalent to minimizing the negative log-likelihood (right). 
	\end{footnotesize}
\end{center}

\end{vbframe}


\begin{vbframe}{Example 1: Maximum Likelihood}

$\D = \left(\xi[1], ..., \xi[n]\right) \overset{\text{i.i.d.}}{\sim} f(\xv ~|~ \mu, \sigma)$ with $\sigma = 1$: 

$$
	f(\xv ~|~ \mu, \sigma) = \frac{1}{\sqrt{2 \mu \sigma^{2}}}~\exp\left(\frac{-(\xv-\mu)^{2}}{2\sigma^{2}}\right)
$$

\textbf{Goal:} Find $\mu \in \R$ which makes observed data most likely. 

\begin{center}
	\begin{footnotesize}
	\includegraphics[width=0.4\textwidth, keepaspectratio]{figure_man/ml_normal_example_dnorm.pdf} 
	\end{footnotesize}
\end{center}

\framebreak 

\begin{itemize}
	\item \textbf{Likelihood:} \vspace*{-0.4cm}
 $$\mathcal{L}(\mu~|~\D)= \prod_{i=1}^{n} f\left(\xi~|~\mu, 1\right) = (2\pi)^{-n/2}\exp\left(-\frac{1}{2} \sum_{i=1}^{n} (\xi-\mu)^{2} \right)$$
	\item \textbf{Neg. log-likelihood:} \vspace*{-0.4cm}
$$- \ell(\mu, \D) = - \log \mathcal{L}(\mu~|~\D) = \frac{n}{2} \log(2\pi) + \frac{1}{2} \sum_{i=1}^{n} (\xi-\mu)^{2}$$
\end{itemize}

\vspace*{-0.3cm}

\begin{center}
	\begin{footnotesize}
	\includegraphics[width=0.4\textwidth, keepaspectratio]{figure_man/ml_normal_example_negloglike_nooptim.pdf} 
	\end{footnotesize}
\end{center}
\framebreak 

$$
	\min_{\mu \in \R} - \ell(\mu, \D).
$$

can be solved analytically (setting the first deriv. to $0$):
\vspace*{-0.5cm}

\begin{footnotesize}
\begin{eqnarray*}
	-\frac{\partial \ell(\mu, \D)}{\partial \mu} = \sumin \left(\xi - \mu\right) &=& 0 \quad \Leftrightarrow \quad \hat \mu = \frac{1}{n} \sumin \xi 
	% \sumin \xi &=& \sumin \mu = n \cdot \mu \\
	% \sumin \xi &=& n \cdot \mu \\
\end{eqnarray*}
\end{footnotesize}

\vspace*{-0.4cm}

\begin{center}
	\begin{footnotesize}
	\includegraphics[width=0.4\textwidth, keepaspectratio]{figure_man/ml_normal_example_negloglike.pdf} 
	\end{footnotesize}
\end{center}

\framebreak 

\textbf{Note: } The problem was \textbf{smooth}, \textbf{univariate}, \textbf{unconstrained}, \textbf{convex}. 

\lz 

If we had optimized for $\sigma$ as well 

$$
	\min_{\mu \in \R, \sigma \in \R^+} - \ell(\mu, \D).
$$


(instead of assuming it is known) the problem would have been: 

\begin{itemize}
	\item bivariate (optimize over $(\mu, \sigma)$)
	\item constrained ($\sigma > 0$)
\end{itemize}

$$
	\min_{\mu \in \R, \sigma \in \R^+} - \ell(\mu, \D).
$$

\end{vbframe}


\begin{vbframe}{Example 2: Normal regression}

Assume a dataset $\D = \Dset$ generated according to

$$
\yi = \bm{\theta}^\top \xi + \epsi, \qquad \epsi \overset{iid}{\sim} \mathcal{N}\left(0, 1\right).
$$

\begin{center}
	\includegraphics[height=0.4\textwidth, keepaspectratio]{figure_man/ml_linreg_example_1.pdf} 
\end{center}



\end{vbframe}

\begin{vbframe}{Example 2: Normal linear regression} 


In normal linear regression the goal is to find a vector $\thetab$ which minimizes the sum of squared errors (SSE; also: L2-loss): 

$$
\min_{\thetab \in \R^d} \sum_{i = 1}^n \left(\thetab^\top \xi - \yi\right)^2
$$

\begin{center}
	\includegraphics[height=0.30\textwidth, keepaspectratio]{figure_man/ml_linreg_example_2.pdf}
\end{center}

\end{vbframe}

\begin{vbframe}{Example 2: Normal regression}

\begin{center}
	\includegraphics[height=0.30\textwidth, keepaspectratio]{figure_man/ml_linreg_example_1.pdf} ~~ \includegraphics[height=0.30\textwidth, keepaspectratio]{figure_man/ml_linreg_example_2.pdf}
\end{center}

\begin{itemize}
	\item \textbf{Smooth}, \textbf{multivariate}, \textbf{unconstrained}, \textbf{convex} problem
	\item Quadratic form: Obtain a geometric interpretation via Eigenspectrum
	\item The problem has a closed-form solution, which is given by $\thetab = (\Xmat^\top \Xmat)^{-1}\Xmat^\top \bm{y}$, where $\Xmat$ is the design matrix
\end{itemize}

\end{vbframe}

\begin{vbframe}{Risk Minimization in ML}
	
In the above example

$$
\min_{\thetab \in \R^d} \sum_{i = 1}^n \textcolor{violet}{\left(\textcolor{cyan}{\thetab^\top \xi} - \yi\right)^2}
$$

\begin{itemize}
	\item the linear model $\textcolor{cyan}{\thetab^\top \xv}$ by an arbitrary model $\textcolor{cyan}{\fxt}$ % (e.g., something more complex and nonlinear)
	\item the L2-loss $\textcolor{violet}{\left(\fxt - y\right)^2}$ by any loss $\textcolor{violet}{\Lxy}$
\end{itemize}

\lz 

This leads us to \textbf{empirical risk minimization} (ERM)

	$$
	\min \risket = \frac{1}{n} \sumin L\left(\yi, \fxit\right),
	$$

a core concept in machine learning (ML). 

\framebreak 

ML models usually consists of the following components: 

\begin{center}

  \textbf{ML} = $\underbrace{\textbf{Hypothesis Space + Risk + Regularization}}_{\text{Formulating the optimization problem}}$ + $\underbrace{\textbf{Optimization}}_{\text{Solving it}}$ 
  
\end{center}

\lz

\begin{itemize}

  \item \textbf{Hypothesis Space:} Define (and restrict!) what kind of model 
  $f$ can be learned from the data.
  
  \item \textbf{Risk:} Define the risk function $\risket$ that quantifies how well a specific model performs on a given 
  data set via a suitable loss function $L$.
  
  \item \textbf{Optimization:} Solve the resulting optimization problem through optimizing the risk $\risket$ over the hypothesis space.

  \item \textbf{Regularization:} Penalize model complexity 
  
\end{itemize}

\end{vbframe}

\begin{vbframe}{Example 3: Regularized LM}

\begin{footnotesize}

ERM with L2-loss, LM, and quadratic (L2) regularization term: 

\vspace*{-0.2cm}

$$
	\min \risket = \frac{1}{n}\sumin \left(\thetab^\top \xi - \yi\right)^2  + \lambda \cdot \|\thetab\|_2^2 \quad \text{(Ridge regr.)}
$$

\vspace*{-0.1cm}


The problem is still \textbf{multivariate}, \textbf{unconstrained}, \textbf{smooth}, \textbf{convex}.

\vspace*{0.1cm}

ERM with L2-loss, LM, and L1 regularization: 

\vspace*{-0.2cm}

$$
	\min \risket = \frac{1}{n}\sumin \left(\thetab^\top \xi - \yi\right)^2  + \lambda \cdot \|\thetab\|_1 \quad \text{(Lasso regr.)}
$$

\vspace*{-0.1cm}

The problem is still \textbf{multivariate}, \textbf{unconstrained}, \textbf{convex}, but \textcolor{violet}{\textbf{not smooth}}. % The L1 penality is not differentiable if $\exists i$ s.t. $\theta_i = 0$.  

\vspace*{-0.3cm}

\begin{figure}
\begin{center}
	\includegraphics[width=0.8\textwidth]{figure_man/lasso-ridge_medium.jpg}
\end{center}
	\caption{Source: Medium, 2022, Visualize the Loss with Regularization}
	% https://medium.com/@jingwang.physics/visualize-the-loss-with-regularization-2f82666dc0f7
\end{figure}

\end{footnotesize}

\end{vbframe}


\begin{vbframe}{Example 4: SVM}

\begin{footnotesize}
\begin{itemize}
	\item $\D = \left(\left(\xi, \yi\right)\right)_{i = 1, ..., n}$ with $\yi \in \{-1, 1\}$ (classification)
	\item $\fxt \in \R$ scoring classifier: Predict $1$ if $\fxt > 0$ and $-1$ otherwise. 
\end{itemize}
\end{footnotesize}

ERM with LM, Hinge loss, and L2 regularization: 

$$
	\min_{\thetab} ~\sumin \max\left(1 - \yi f^{(i)}, 0\right) + \lambda \thetab^\top \thetab, \quad f^{(i)} := \thetab^\top \xi
$$

\vspace*{-0.2cm}

\begin{columns}[T] % align columns
	\begin{column}{.58\textwidth}
		\begin{center}
			\includegraphics[width=0.6\textwidth]{figure_man/svm_geometry.png} 
		\end{center}
	\end{column}
	\begin{column}{.38\textwidth} \vspace*{1.5cm}
		\begin{footnotesize}
		This is one formulation of the \textbf{linear support vector machine}. The problem is still \textbf{multivariate}, \textbf{unconstrained}, \textbf{convex}, but \textcolor{violet}{\textbf{not smooth}}.  
		\end{footnotesize}
	\end{column}
\end{columns}

\framebreak 

Understanding Hinge loss $\Lxy = \max\left(1 - y\cdot f, 0\right)$

\begin{footnotesize}
\begin{center}
\begin{tabular}{ c | c | c | c | c }
$\mathbf{y}$ & $\fx$ &  \textbf{Correct pred.?}  & $\Lxy$ & \textbf{Reason for costs}  \\ \hline
 $1$ & $(- \infty, 0)$  & N & $(1, \infty)$ & Misclassification \\
 $- 1$ & $(0, \infty)$ & N  & $(1, \infty)$ &  Misclassification \\
 $1$ & $(0, 1)$ & Y & $(0, 1)$ & Low confidence / margin \\
 $- 1$ & $(-1, 0)$  & Y  & $(0, 1)$& Low confidence / margin\\
 $1$ & $(1, \infty)$ &  Y & $0$ & -- \\
 $- 1$ & $(- \infty, -1)$ &  Y & $0$ & -- \\
\end{tabular}
\end{center}
\end{footnotesize}

\vspace*{-0.3cm}
\begin{figure}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{figure_man/hinge.pdf}
	\end{center}
	% See hinge_vs_l2.R
\end{figure}

\framebreak 

\textbf{Kernelized} formulation of the SVM for nonlinear $f$: 

\begin{eqnarray*}
	\min_{\thetab}\sumin L\left(\yi, \bm{K}_i ~\thetab\right) + \lambda \thetab^\top \bm{K} \thetab 
\end{eqnarray*}

with $k(\xi, \xi[j])$ positive definite kernel function, and $\bm{K}_{ij} := k(\xi, \xi[j])$, $i, j = 1, ..., n$ kernel matrix, $\bm{K}_i$ is the $i$-th column of the kernel matrix. 

\vspace*{0.2cm}

\begin{columns}[T] % align columns
	\begin{column}{.38\textwidth} 
        Kernelization 
        
        \begin{itemize}
        	\item allows introducing nonlinearity
        	\item without changing characteristics of the optimization problem (convexity!)
        \end{itemize}
	\end{column}
	\begin{column}{.58\textwidth}
		\begin{center}
		    \includegraphics[width=0.4\textwidth]{slides/02-optimization-problems/figure_man/nonlinear-svm-c.pdf}
		    % https://github.com/slds-lmu/lecture_i2ml/blob/master/slides/nonlinear-svm/figure_man/kernels/nonlinear-svm-c.pdf
		\end{center}
	\end{column}

\end{columns}





\end{vbframe}


\begin{vbframe}{Example 5: Neural network}

The (computational) complexity of the optimization problem 

$$
\text{arg} \min_{\thetab} \risket,
$$

(i.e. smoothness, convexity) is influenced by model structure and loss. 		
\vspace*{-0.3cm}
\begin{center}
		\includegraphics[width=0.3\textwidth]{figure_man/ml_landscape.jpg} ~~~ \includegraphics[width=0.3\textwidth]{figure_man/log_reg.png}
	\begin{footnotesize}
		\newline
		Loss landscapes of ML problems. \\ Left: Deep learning model ResNet-56, right: Logistic regression with cross-entropy loss
		\newline
		Source: \url{https://arxiv.org/pdf/1712.09913.pdf}
	\end{footnotesize}
\end{center}	

\end{vbframe}


\endlecture

\end{document}
