
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/ml_linreg_example_2.pdf}
\newcommand{\learninggoals}{
\item Definition
\item Practical examples}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization}
\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Unconstrained problems}
\lecture{Optimization}
\sloppy

\begin{vbframe}{Definition: Optimization Problem}

$$
\min_{\xv \in \mathcal{S}} \fx
$$
with objective function
$$
f: \; \mathcal{S} \to \R.
$$

\lz 

The problem is called

\begin{itemize}
	\item \textbf{unconstrained}, if the domain $\mathcal{S}$ is not restricted: 
	$$
		\mathcal{S} = \R^d
	$$
	\item \textbf{smooth} if $f$ is at least $\in \mathcal{C}^1$
	\item \textbf{univariate} if $d = 1$, and \textbf{multivariate} if $d > 1$.  
\end{itemize}

\end{vbframe}

% \begin{vbframe}{Concept: Univariate vs. multivariate}

% If we are optimizing w.r.t. one variable, i.e. $\text{dim}(\mathcal{S}) = 1$, the problem is an univariate optimization problem. If $\mathcal{S}$ is multi-dimensional, we are talking about multivariate optimization. 
	
% \begin{center}
% 	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/ml_poisson_example_2.pdf} \includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/ml_linreg_example_3.png} \\
% 	\begin{footnotesize}
% 	Left (Univariate): The log-likelihood of a Poisson distribution. The optimization problem is an univariate optimization problem with $\mathcal{S} = [0, 1]$. Right (Multivariate): The sum of squared errors with regards to the coefficients $(\theta_0, \theta_1)$ of a regression. 
% 	\end{footnotesize}
% \end{center}

% \end{vbframe}

\begin{vbframe}{Note: A Convention in Optimization}

W.l.o.g., we always \textbf{minimize} functions $f$. 

\lz

Maximization results from minimizing $-f$.

\begin{center}
	\begin{footnotesize}
	\includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/ml_poisson_example_2.pdf} \includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/ml_poisson_example_3.pdf} \\
	Poisson example: Maximizing the log-likelihood (left) is equivalent to minimizing the negative log-likelihood (right). 
	\end{footnotesize}
\end{center}

\end{vbframe}


\begin{vbframe}{Example 1: Maximum Likelihood}

Let $\D = \left(\xi[1], ..., \xi[n]\right)$ sampled i.i.d. from density $f(\xv ~|~ \mu, \sigma)$, with $\sigma = 1$: 

$$
	f(\xv ~|~ \mu, \sigma) = \frac{1}{\sqrt{2 \mu \sigma^{2}}}~\exp\left(\frac{-(\xv-\mu)^{2}}{2\sigma^{2}}\right)
$$

\textbf{Goal:} Find $\mu \in \R$ which makes observed data most likely. 

\begin{center}
	\begin{footnotesize}
	\includegraphics[width=0.4\textwidth, keepaspectratio]{figure_man/ml_normal_example_dnorm.pdf} 
	\end{footnotesize}
\end{center}

\framebreak 

\begin{itemize}
	\item \textbf{Likelihood:} \vspace*{-0.4cm}
 $$\mathcal{L}(\mu~|~\D)= \prod_{i=1}^{n} f\left(\xi~|~\mu, 1\right) = (2\pi)^{-n/2}\exp\left(-\frac{1}{2} \sum_{i=1}^{n} (\xi-\mu)^{2} \right)$$
	\item \textbf{Neg. log-likelihood:} \vspace*{-0.4cm}
$$- \ell(\mu, \D) = - \log \mathcal{L}(\mu~|~\D) = \frac{n}{2} \log(2\pi) + \frac{1}{2} \sum_{i=1}^{n} (\xi-\mu)^{2}$$
\end{itemize}

\vspace*{-0.3cm}

\begin{center}
	\begin{footnotesize}
	\includegraphics[width=0.4\textwidth, keepaspectratio]{figure_man/ml_normal_example_negloglike_nooptim.pdf} 
	\end{footnotesize}
\end{center}
\framebreak 

$$
	\min_{\mu \in \R} - \ell(\mu, \D).
$$

can be solved analytically (setting the first deriv. to $0$):
\vspace*{-0.5cm}

\begin{footnotesize}
\begin{eqnarray*}
	-\frac{\partial}{\ell(\mu, \D)}{\partial \mu} = \sumin \left(\xi - \mu\right) &=& 0 \quad \Leftrightarrow \quad \hat \mu = \frac{1}{n} \sumin \xi 
	% \sumin \xi &=& \sumin \mu = n \cdot \mu \\
	% \sumin \xi &=& n \cdot \mu \\
\end{eqnarray*}
\end{footnotesize}

\vspace*{-0.4cm}

\begin{center}
	\begin{footnotesize}
	\includegraphics[width=0.4\textwidth, keepaspectratio]{figure_man/ml_normal_example_negloglike.pdf} 
	\end{footnotesize}
\end{center}

\framebreak 

\textbf{Note: } The problem was \textbf{smooth}, \textbf{univariate}, \textbf{unconstrained}. 

\lz 

If we had optimized for $\sigma$ as well (instead of assuming it is known) the problem would have been: 

\begin{itemize}
	\item bivariate (optimize over $(\mu, \sigma)$)
	\item constrained ($\sigma > 0$)
\end{itemize}

\end{vbframe}


\begin{vbframe}{Example 2: Normal regression}

Assume a dataset $\D = \Dset$ generated according to

$$
\yi = \bm{\theta}^\top \xi + \epsi, \qquad \epsi \overset{iid}{\sim} \mathcal{N}\left(0, 1\right).
$$

\begin{center}
	\includegraphics[height=0.4\textwidth, keepaspectratio]{figure_man/ml_linreg_example_1.pdf} 
\end{center}



\end{vbframe}

\begin{vbframe}{Example 2: Normal linear regression} 


In normal linear regression the goal is to find a vector $\thetab$ which minimizes the sum of squared errors (SSE): 

$$
\min_{\thetab \in \R^d} \sum_{i = 1}^n \left(\thetab^\top \xi - \yi\right)^2
$$

\begin{center}
	\includegraphics[height=0.30\textwidth, keepaspectratio]{figure_man/ml_linreg_example_2.pdf}
\end{center}

\end{vbframe}

\begin{vbframe}{Example 2: Normal regression}

\begin{center}
	\includegraphics[height=0.30\textwidth, keepaspectratio]{figure_man/ml_linreg_example_1.pdf} ~~ \includegraphics[height=0.30\textwidth, keepaspectratio]{figure_man/ml_linreg_example_2.pdf}
\end{center}

\begin{itemize}
	\item \textbf{Smooth}, \textbf{multivariate}, \textbf{unconstrained} problem
	\item Quadratic form: Obtain a geometric interpretation via Eigenspectrum
	\item The problem has a closed-form solution, which is given by $\thetab = (\Xmat^\top \Xmat)^{-1}\Xmat^\top \bm{y}$, where $\Xmat$ is the design matrix
\end{itemize}

\end{vbframe}

\begin{frame}{Example 3: Risk Min. in Machine Learning}
	
	
\begin{itemize}
	\item $\D = \Dset$ denotes a dataset 
	where $\fxit$ is a model, parameterized by $\thetab$ (e.g. linear model).
	\item Let $\Lxy$ be the point-wise loss function which measures the error of a prediction $\fx$ compared to the true output $y$.
	\item We want to find the model which minimizes the \textbf{empirical risk}
	
	$$
	\risket = \frac{1}{n} \sumin L\left(\yi, \fxit\right).
	$$
\end{itemize}

\textcolor{red}{Formulate without $\theta$ and then explain why we usually parameterize the hypothesis space. }

\end{frame}	

\begin{vbframe}{Risk Minimization in Machine Learning}
		
Machine learning consists of three components: 

\begin{center}

  \textbf{Machine Learning} = $\underbrace{\textbf{Hypothesis Space + Risk}}_{\text{Formulating the optimization problem}}$ + $\underbrace{\textbf{Optimization}}_{\text{Solving it}}$
  
\end{center}

\lz

\begin{itemize}

  \item \textbf{Hypothesis Space:} Define (and restrict!) what kind of model 
  $f$ can be learned from the data.
  
  \item \textbf{Risk:} Define the risk function $\risket$ that quantifies how well a specific model performs on a given 
  data set via a suitable loss function $L$.
  
  \item \textbf{Optimization:} Solve the resulting optimization problem through optimizing the risk $\risket$ over the hypothesis space.
  
\end{itemize}

\framebreak 

The (computational) complexity of the optimization problem 

$$
\text{arg} \min_{\thetab} \risket
$$

and hence the choice of the numerical optimization algorithm is influenced by the model structure and the choice of the loss function:, i.e., smoothness, convexity. 		
\vspace*{-0.5cm}
\begin{center}
		\includegraphics[width=0.3\textwidth]{figure_man/ml_landscape.jpg} ~~~ \includegraphics[width=0.3\textwidth]{figure_man/log_reg.png}
	\begin{footnotesize}
		\newline
		Loss landscapes of ML problems. \\ Left: ResNet-56, right: Logistic regression with cross-entropy loss
		\newline
		Source: \url{https://arxiv.org/pdf/1712.09913.pdf}
	\end{footnotesize}
\end{center}	

\end{vbframe}




\endlecture
\end{document}
