
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/ml_linreg_example_2.pdf}
\newcommand{\learninggoals}{
\item Definition
\item Practical examples}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization}
\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Unconstrained problems}
\lecture{Optimization}
\sloppy

\begin{vbframe}{General Definition}

Consider the \textbf{optimization problem}
$$
\min_{\xv \in \mathcal{S}\subseteq \R^d} \fx
$$
with objective function
$$
f: \; \mathcal{S} \to \R.
$$

The problem is called

\begin{itemize}
	\item \textbf{unconstrained}, if the domain $\mathcal{S}$ is not restricted: 
	$$
		\mathcal{S} = \R^d
	$$
	\item \textbf{smooth} if $f$ is smooth.
	\item \textbf{univariate} if $d = 1$, and \textbf{multivariate} if $d > 1$.  
\end{itemize}

\end{vbframe}

% \begin{vbframe}{Concept: Univariate vs. multivariate}

% If we are optimizing w.r.t. one variable, i.e. $\text{dim}(\mathcal{S}) = 1$, the problem is an univariate optimization problem. If $\mathcal{S}$ is multi-dimensional, we are talking about multivariate optimization. 
	
% \begin{center}
% 	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/ml_poisson_example_2.pdf} \includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/ml_linreg_example_3.png} \\
% 	\begin{footnotesize}
% 	Left (Univariate): The log-likelihood of a Poisson distribution. The optimization problem is an univariate optimization problem with $\mathcal{S} = [0, 1]$. Right (Multivariate): The sum of squared errors with regards to the coefficients $(\theta_0, \theta_1)$ of a regression. 
% 	\end{footnotesize}
% \end{center}

% \end{vbframe}

\begin{vbframe}{Note: A Convention in Optimization}

W.l.o.g., we always \textbf{minimize} functions $f$. 

\lz

Maximization results from minimizing $-f$.

\begin{center}
	\begin{footnotesize}
	\includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/ml_poisson_example_2.pdf} \includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/ml_poisson_example_3.pdf} \\
	Poisson example: Maximizing the log-likelihood (left) is equivalent to minimizing the negative log-likelihood (right). 
	\end{footnotesize}
\end{center}

\end{vbframe}

\begin{vbframe}{Example 1: Maximum Likelihood Estimation}
	
Assume an i.i.d. sample $\D = \left(x^{(1)}, ..., x^{(n)}\right)$ from a distribution with density $f(x ~|~ \thetab)$. We want to find $\lambda$ which makes the observed data most likely.

\begin{center}
	\includegraphics[width=0.4\textwidth, height=0.4\textwidth]{figure_man/ml_poisson_example_1.pdf} \\
	\begin{footnotesize}
		Example: Histogram of a sample drawn from a Poisson distribution $f(k ~|~ \lambda) := \P(x = k) = \frac{\lambda^k \cdot \exp(-\lambda)}{k!}$. 
	\end{footnotesize}
\end{center}

\framebreak 

 We operationalize this as \textbf{maximizing} the log-likelihood function (or equivalently: minimizing the negative log-likelihood) with respect to $\lambda$:

\begin{footnotesize}
\begin{eqnarray*}
	\hat \lambda  &=& \text{arg min}_\lambda ~ - \ell(\lambda, \mathcal{D}) =\text{arg min}_\lambda  - \log \mathcal{L}(\lambda, \mathcal{D}) = \text{arg min}_\lambda - \log \prod_{i = 1}^n  f\left(\xi ~|~ \lambda\right) \\ &=& \text{arg min}_\lambda - \sumin f\left(x^{(i)} ~|~ \lambda\right) = \text{arg min}_\lambda \sumin \frac{- \lambda^{\xi} \cdot \exp(- \lambda)}{\xi!} 
\end{eqnarray*}
\end{footnotesize}

\begin{center}
	\includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/ml_poisson_example_2.pdf} \\
	\begin{footnotesize}
		Example: The log-likelihood of a Poisson distribution for data example above. The objective function is univariate and differentiable, and the domain is \textcolor{red}{unconstrained}.
	\end{footnotesize}
\end{center}


\end{vbframe}


\begin{vbframe}{Example 2: Normal regression}

Assume a dataset $\D = \Dset$ generated according to

$$
\yi = \bm{\theta}^\top \xi + \epsi, \qquad \epsi \overset{iid}{\sim} \mathcal{N}\left(0, 1\right).
$$

\begin{center}
	\includegraphics[height=0.4\textwidth, keepaspectratio]{figure_man/ml_linreg_example_1.pdf} 
\end{center}

\end{vbframe}

\begin{vbframe}{Example 2: Normal linear regression} 


In normal linear regression the goal is to find a vector $\thetab$ which minimizes the sum of squared errors (SSE): 

$$
\min_{\thetab \in \R^d} \sum_{i = 1}^n \left(\thetab^\top \xi - \yi\right)^2
$$

\begin{center}
	\includegraphics[height=0.30\textwidth, keepaspectratio]{figure_man/ml_linreg_example_2.pdf}
\end{center}

\end{vbframe}

\begin{vbframe}{Example 2: Normal regression}

\begin{center}
	\includegraphics[height=0.30\textwidth, keepaspectratio]{figure_man/ml_linreg_example_1.pdf} ~~ \includegraphics[height=0.30\textwidth, keepaspectratio]{figure_man/ml_linreg_example_2.pdf}
\end{center}

\begin{itemize}
	\item The problem is multivariate, smooth, and unconstrained
	\item Since the problem is a quadratic form, we easily obtain a geometric interpretation of the problem 
	\item The problem has a closed-form solution, which is given by $\thetab = (\Xmat^\top \Xmat)^{-1}\Xmat^\top \bm{y}$, where $\Xmat$ is the design matrix
\end{itemize}

\end{vbframe}



\endlecture
\end{document}
