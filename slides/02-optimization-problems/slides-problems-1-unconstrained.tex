
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/ml_linreg_example_2.pdf}
\newcommand{\learninggoals}{
\item Definition
\item Practical examples}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization}
\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Unconstrained problems}
\lecture{Optimization}
\sloppy

\begin{vbframe}{Definition: Optimization Problem}

$$
\min_{\xv \in \mathcal{S}} \fx
$$
with objective function
$$
f: \; \mathcal{S} \to \R.
$$

\lz 

The problem is called

\begin{itemize}
	\item \textbf{unconstrained}, if the domain $\mathcal{S}$ is not restricted: 
	$$
		\mathcal{S} = \R^d
	$$
	\item \textbf{smooth} if $f$ is at least $\in \mathcal{C}^1$
	\item \textbf{univariate} if $d = 1$, and \textbf{multivariate} if $d > 1$.  
\end{itemize}

\end{vbframe}

% \begin{vbframe}{Concept: Univariate vs. multivariate}

% If we are optimizing w.r.t. one variable, i.e. $\text{dim}(\mathcal{S}) = 1$, the problem is an univariate optimization problem. If $\mathcal{S}$ is multi-dimensional, we are talking about multivariate optimization. 
	
% \begin{center}
% 	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/ml_poisson_example_2.pdf} \includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/ml_linreg_example_3.png} \\
% 	\begin{footnotesize}
% 	Left (Univariate): The log-likelihood of a Poisson distribution. The optimization problem is an univariate optimization problem with $\mathcal{S} = [0, 1]$. Right (Multivariate): The sum of squared errors with regards to the coefficients $(\theta_0, \theta_1)$ of a regression. 
% 	\end{footnotesize}
% \end{center}

% \end{vbframe}

\begin{vbframe}{Note: A Convention in Optimization}

W.l.o.g., we always \textbf{minimize} functions $f$. 

\lz

Maximization results from minimizing $-f$.

\begin{center}
	\begin{footnotesize}
	\includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/ml_poisson_example_2.pdf} \includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/ml_poisson_example_3.pdf} \\
	Poisson example: Maximizing the log-likelihood (left) is equivalent to minimizing the negative log-likelihood (right). 
	\end{footnotesize}
\end{center}

\end{vbframe}

\begin{vbframe}{Example 1.1: Maximum Likelihood Estimation: Poisson Distribution}
	
$\D = \left(x^{(1)}, ..., x^{(n)}\right)$ is sampled i.i.d. from density $f(x ~|~ \thetab)$. We want to find $\lambda$ which makes the observed data most likely.

\begin{center}
	\includegraphics[width=0.4\textwidth, height=0.4\textwidth]{figure_man/ml_poisson_example_1.pdf} \\
	\begin{footnotesize}
		Example: Histogram of a sample drawn from a Poisson distribution $f(k ~|~ \lambda) := \P(x = k) = \frac{\lambda^k \cdot \exp(-\lambda)}{k!}$. 
	\end{footnotesize}
\end{center}

\framebreak 

 We operationalize this as \textbf{maximizing} the log-likelihood function (or equivalently: minimizing the negative log-likelihood) with respect to $\lambda$:

\begin{footnotesize}
\begin{eqnarray*}
	\hat \lambda  &=& \text{arg min}_\lambda ~ - \ell(\lambda, \mathcal{D}) =\text{arg min}_\lambda  - \log \mathcal{L}(\lambda, \mathcal{D}) = \text{arg min}_\lambda - \log \prod_{i = 1}^n  f\left(\xi ~|~ \lambda\right) \\ &=& \text{arg min}_\lambda - \sumin f\left(x^{(i)} ~|~ \lambda\right) = \text{arg min}_\lambda \sumin \frac{- \lambda^{\xi} \cdot \exp(- \lambda)}{\xi!} 
\end{eqnarray*}
\end{footnotesize}

\begin{center}
	\includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/ml_poisson_example_2.pdf} \\
	\begin{footnotesize}
		Example: The log-likelihood of a Poisson distribution for data example above. The objective function is univariate and differentiable, and the domain is \textcolor{red}{unconstrained}.
	\end{footnotesize}
\end{center}


\end{vbframe}



\begin{vbframe}{Example 1.2: Maximum Likelihood Estimation: Normal distribution}

\begin{footnotesize}
$\textbf{Density:}~~f(\textbf{x} ~|~ \mu, \sigma) = \frac{1}{\sqrt{2 \mu \sigma^{2}}}~exp(\frac{-(\textbf{x}-\mu)^{2}}{2\sigma^{2}})$\\
\vspace{0.2cm}
Since we want to have an univariate, unconstrained optimization problem, we set $\sigma=1$ and estimate only $\mu$, which is $\in \R$ and therefore unconstrained.\\
\vspace{0.2cm}

\textbf{Likelihood}:~$\mathcal{L}(\mu, \sigma^{2}~|~\xi)= \sum_{i=1}^{n} f(\xi) = (2\pi \sigma^{2})^{-n/2}exp^{(-\frac{1}{2\sigma^{2}})} \sum_{i=1}^{n} (\xi-\mu)^{2} $\\
\vspace{0.2cm}

\textbf{MLE}$_{\mu}$: 
$\hat \mu = \text{arg min}_\mu -\ell(\mu, \sigma, \mathcal{D}) = \text{arg min}_\mu -\log \mathcal{L}(\mu, \sigma, \mathcal{D}) =$\\
\vspace{0.2cm}
$\text{arg min} -\log \left( \prod_{i=1}^{n} f(\xi ~|~ \mu, \sigma) \right) ~=~ \text{arg min} \sum_{i=1}^{n} f(\xi ~|~ \mu, \sigma) ~=~$\\
\vspace{0.2cm}
$\text{arg min} ~ \frac {n \log(2\pi\sigma^{2})}{2} + \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} (\xi - \mu)^{2}$ \\
\vspace{0.2cm}
$\implies \partial \mu ~ f(\mu, \sigma^{2}) = \frac{1}{\sigma^{2}} \sum_{i=1}^{n} (\xi-\mu) = 0 \implies \hat \mu = \frac{1}{n} \sum_{i=1}^{n} \xi$



\end{footnotesize}





\framebreak
\begin{center}
	\begin{footnotesize}
	\includegraphics[width=0.5\textwidth, keepaspectratio]{figure_man/ml_normal_example_dnorm.pdf} \includegraphics[width=0.5\textwidth, keepaspectratio]{figure_man/ml_normal_example_negloglike.pdf} 
	\end{footnotesize}
\end{center}


\vspace{0.2cm}
\begin{footnotesize}
If we wanted to estimate $\sigma$ as well, we would now have a multi- (/bi-) variate constrained optimization problem, since $\sigma > 0$. We will cover this problem type later in this lecture.
\end{footnotesize}

\end{vbframe}



\begin{vbframe}{Example 2: Normal regression}

Assume a dataset $\D = \Dset$ generated according to

$$
\yi = \bm{\theta}^\top \xi + \epsi, \qquad \epsi \overset{iid}{\sim} \mathcal{N}\left(0, 1\right).
$$

\begin{center}
	\includegraphics[height=0.4\textwidth, keepaspectratio]{figure_man/ml_linreg_example_1.pdf} 
\end{center}

\end{vbframe}

\begin{vbframe}{Example 2: Normal linear regression} 


In normal linear regression the goal is to find a vector $\thetab$ which minimizes the sum of squared errors (SSE): 

$$
\min_{\thetab \in \R^d} \sum_{i = 1}^n \left(\thetab^\top \xi - \yi\right)^2
$$

\begin{center}
	\includegraphics[height=0.30\textwidth, keepaspectratio]{figure_man/ml_linreg_example_2.pdf}
\end{center}

\end{vbframe}

\begin{vbframe}{Example 2: Normal regression}

\begin{center}
	\includegraphics[height=0.30\textwidth, keepaspectratio]{figure_man/ml_linreg_example_1.pdf} ~~ \includegraphics[height=0.30\textwidth, keepaspectratio]{figure_man/ml_linreg_example_2.pdf}
\end{center}

\begin{itemize}
	\item The problem is multivariate, smooth, and unconstrained
	\item Since the problem is a quadratic form, we easily obtain a geometric interpretation of the problem 
	\item The problem has a closed-form solution, which is given by $\thetab = (\Xmat^\top \Xmat)^{-1}\Xmat^\top \bm{y}$, where $\Xmat$ is the design matrix
\end{itemize}

\end{vbframe}

\begin{frame}{Example 3: Risk Min. in Machine Learning}
	
	
\begin{itemize}
	\item $\D = \Dset$ denotes a dataset 
	where $\fxit$ is a model, parameterized by $\thetab$ (e.g. linear model).
	\item Let $\Lxy$ be the point-wise loss function which measures the error of a prediction $\fx$ compared to the true output $y$.
	\item We want to find the model which minimizes the \textbf{empirical risk}
	
	$$
	\risket = \frac{1}{n} \sumin L\left(\yi, \fxit\right).
	$$
\end{itemize}

\textcolor{red}{Formulate without $\theta$ and then explain why we usually parameterize the hypothesis space. }

\end{frame}	

\begin{vbframe}{Risk Minimization in Machine Learning}
		
Machine learning consists of three components: 

\begin{center}

  \textbf{Machine Learning} = $\underbrace{\textbf{Hypothesis Space + Risk}}_{\text{Formulating the optimization problem}}$ + $\underbrace{\textbf{Optimization}}_{\text{Solving it}}$
  
\end{center}

\lz

\begin{itemize}

  \item \textbf{Hypothesis Space:} Define (and restrict!) what kind of model 
  $f$ can be learned from the data.
  
  \item \textbf{Risk:} Define the risk function $\risket$ that quantifies how well a specific model performs on a given 
  data set via a suitable loss function $L$.
  
  \item \textbf{Optimization:} Solve the resulting optimization problem through optimizing the risk $\risket$ over the hypothesis space.
  
\end{itemize}

\framebreak 

The (computational) complexity of the optimization problem 

$$
\text{arg} \min_{\thetab} \risket
$$

and hence the choice of the numerical optimization algorithm is influenced by the model structure and the choice of the loss function:, i.e., smoothness, convexity. 		
\vspace*{-0.5cm}
\begin{center}
		\includegraphics[width=0.3\textwidth]{figure_man/ml_landscape.jpg} ~~~ \includegraphics[width=0.3\textwidth]{figure_man/log_reg.png}
	\begin{footnotesize}
		\newline
		Loss landscapes of ML problems. \\ Left: ResNet-56, right: Logistic regression with cross-entropy loss
		\newline
		Source: \url{https://arxiv.org/pdf/1712.09913.pdf}
	\end{footnotesize}
\end{center}	

\end{vbframe}




\endlecture
\end{document}
