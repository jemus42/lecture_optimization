
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\slvec}{\left(\zeta^{(1)}, \zeta^{(n)}\right)} % slack variable vector
\newcommand{\sli}[1][i]{\zeta^{(#1)}} % i-th slack variable
\newcommand{\scptxi}{\scp{\thetab}{\xi}} % scalar prodct of theta and xi
\newcommand{\alphav}{\bm{\alpha}} % vector alpha (bold) (basis fun coefficients)

\newcommand{\titlefigure}{figure_man/classes_optimization_problems.png}
\newcommand{\learninggoals}{
\item Definition
\item LP, QP, CP
\item Ridge and Lasso
\item Soft-margin SVM 
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization}
\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Constrained problems}
\lecture{Optimization}
\sloppy

\begin{vbframe}{Constrained Optimization Problem }

$$
\min_{\xv \in \mathcal{S}} \fx, \text{ with } f: \; \mathcal{S} \to \R.
$$

\begin{itemize}
	\item Problem is \textbf{constrained}, if domain $\mathcal{S}$ is restricted: $\mathcal{S} \textcolor{blue}{\subsetneq} \R^d.$

	\item Typically $\mathcal{S}$ is defined via functions called \textbf{constraints}

	$$
		\mathcal{S}:= \{\xv \in \R^d ~|~ g_i(\xv) \le 0, h_j(\xv) = 0 ~\forall~ i, j\}, \text{ where }
	$$ 

\begin{itemize}
\item $g_i: \R^d \to \R, i = 1, ..., k$ are called inequality constraints,
\item $h_j: \R^d \to \R, j = 1, ..., l$ are called equality constraints.
\end{itemize}

\end{itemize}

\lz 

Equivalent formulation: 
\begin{eqnarray*}
\min && f(\mathbf{x})  \\
\text{such that} && g_i(\mathbf{x}) \le 0 \qquad \text{for } i=1,\ldots,k  \\
 && h_j(\mathbf{x}) = 0 \qquad \text{for } j=1,\ldots,l. 
\end{eqnarray*}

\framebreak 

The problem is called

\begin{itemize}
	% \item \textbf{smooth}, if $f$, $g_i$, $h_j$ are all smooth
	\item \textbf{linear program (LP)}, if $f$ a (multivariate) subject to affine$^{(*)}$ equality constraints $h_j$
	\item \textbf{quadratic program (QP)}, if $f$ a (multivariate) quadratic function subject to only linear constraints $g_i, h_j$
	\item \textbf{convex program (CP)}, if $f$, $g_i$ are convex functions, and $h_i$ are affine functions$^{(*)}$
\end{itemize}

\vspace*{-0.3cm}
\begin{figure}{}
\includegraphics[width=0.3\textwidth]{figure_man/classes_optimization_problems.png}
% https://docs.google.com/presentation/d/1ebsa5uoWHX25LsSdRpb3x3bCmJatfSdmUYTaotrYVxE/edit#slide=id.p
\end{figure}

\begin{footnotesize}
$^{(*)}$ $h_i(\xv) = \bm{a}_i^\top \xv - b_i$, $\bm{a}_i \in \R^d, b_i \in \R$.
\end{footnotesize}


% \begin{itemize}
% \item \textbf{Box-constrained}:

% $$
% \mathcal{S} = \{\xv \in \R^d: x_i^l \leq x_i \leq x_i^u \text{ for all } i\}
% $$
% \item
% \textbf{Linear or non-linear constraints}
% \begin{eqnarray*}
% \mathcal{S} = \{\xv \in \R^d: g(\xv) \le 0;  h(\xv) = 0\}
% \end{eqnarray*}
% We call $g(\xv)\le 0$ an inequality constraint, and $h(\xv) = 0 $ an equality constraint. 

% \end{itemize}


\end{vbframe}

\begin{vbframe}{Example 1: Unit circle}

\vspace*{-0.3cm} 

\begin{eqnarray*}
  \min && f(x_1, x_2) = x_1 + x_2 \\
  \text{s.t. } && h(x_1,x_2) = x_1^2 + x_2^2 - 1 = 0
\end{eqnarray*}

\begin{center}
  \includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/unit_circle.png} \\
\end{center}

The problem is not \textbf{convex}, since $\mathcal{S}$ is not a convex set. 

\lz 

\textbf{Note: } If the equality constraint is replaced by an inequality constraint ($g(x_1, x_2) = x_1^2 + x_2^2 - 1 \le 0$), the problem is a CP. 

\end{vbframe}

\begin{vbframe}{Example 2: Maximum likelihood}

$\D = \left(\xi[1], ..., \xi[n]\right) \overset{\text{i.i.d.}}{\sim} f(\xv; n; \thetab)$ with $f$ a multinomial distribution

\begin{footnotesize}
  $$
  f(\xv; n; \thetab) = \begin{cases} \binom{n!}{x_1! \cdot x_2! ... x_d!} \theta_1^{x_1} \cdot ... \cdot \theta_d^{x_d} & \text{if } x_1 + ... + x_d = n \\ 0 & \text{else}
  \end{cases}
  $$
\end{footnotesize}

with constraints on $\theta_i$: 
\begin{eqnarray*}
	0 \le \theta_i \le 1 && \text{ for all } i \\
	 \theta_1 + ... + \theta_d = 1. &&
\end{eqnarray*}



\end{vbframe}




% \begin{vbframe}{Example 1.1: Maximum Likelihood Estimation: Poisson Distribution}
	
% $\D = \left(x^{(1)}, ..., x^{(n)}\right)$ is sampled i.i.d. from density $f(x ~|~ \thetab)$. We want to find $\lambda$ which makes the observed data most likely.

% \begin{center}
% 	\includegraphics[width=0.4\textwidth, height=0.4\textwidth]{figure_man/ml_poisson_example_1.pdf} \\
% 	\begin{footnotesize}
% 		Example: Histogram of a sample drawn from a Poisson distribution $f(k ~|~ \lambda) := \P(x = k) = \frac{\lambda^k \cdot \exp(-\lambda)}{k!}$. 
% 	\end{footnotesize}
% \end{center}

% \framebreak 

%  We operationalize this as \textbf{maximizing} the log-likelihood function (or equivalently: minimizing the negative log-likelihood) with respect to $\lambda$:

% \begin{footnotesize}
% \begin{eqnarray*}
% 	\hat \lambda  &=& \text{arg min}_\lambda ~ - \ell(\lambda, \mathcal{D}) =\text{arg min}_\lambda  - \log \mathcal{L}(\lambda, \mathcal{D}) = \text{arg min}_\lambda - \log \prod_{i = 1}^n  f\left(\xi ~|~ \lambda\right) \\ &=& \text{arg min}_\lambda - \sumin f\left(x^{(i)} ~|~ \lambda\right) = \text{arg min}_\lambda \sumin \frac{- \lambda^{\xi} \cdot \exp(- \lambda)}{\xi!} 
% \end{eqnarray*}
% \end{footnotesize}

% \begin{center}
% 	\includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/ml_poisson_example_2.pdf} \\
% 	\begin{footnotesize}
% 		Example: The log-likelihood of a Poisson distribution for data example above. The objective function is univariate and differentiable, and the domain is \textcolor{red}{unconstrained}.
% 	\end{footnotesize}
% \end{center}


% \end{vbframe}

% \begin{vbframe}{Example 2: Maximum Likelihood Estimation}

% \textbf{Example}: Maximum Likelihood Estimation

% \lz

% For data $\left(\xv^{(1)}, ..., \xv^{(n)}\right)$, we want to find the maximum likelihood estimate

% $$
% \max_\theta L(\theta) = \prod_{i = 1}^n f(^{(i)}, \theta)
% $$

% In some cases, $\theta$ can only take \textbf{certain values}. 

% \lz 

% \begin{itemize}
% \item If $f$ is a Poisson distribution, we require the rate $\lambda$ to be non-negative, i.e. $\lambda \ge 0$

%   \item If $f$ is a multinomial distribution

% \begin{footnotesize}
%   $$
%   f(x_1, ..., x_p; n; \theta_1, ..., \theta_p) = \begin{cases} \binom{n!}{x_1! \cdot x_2! ... x_p!} \theta_1^{x_1} \cdot ... \cdot \theta_p^{x_p} & \text{if } x_1 + ... + x_p = n \\ 0 & \text{else}
%   \end{cases}
%   $$
% \end{footnotesize}


% \end{itemize}

% \end{vbframe}

\begin{vbframe}{Example 3: Ridge regression}
		
Ridge regression can be formulated as regularized ERM: 

\begin{eqnarray*}  
	\thetah_{\text{Ridge}} &=& \argmin_{\thetab} \left\{ \sumin \left(\yi - \thetab^\top\xv\right)^2 + \lambda ||\thetab||_2^2 \right\}
\end{eqnarray*}

Equivalently it can be written as constrained optimization problem: 

\vspace*{-0.2cm}
\begin{columns}[T] % align columns
	\begin{column}{.48\textwidth}
		\begin{eqnarray*}
			\min_{\thetab} && \sum_{i = 1}^n \left(\thetab^\top \xi - \yi\right)^2 \\
			\text{s.t. } && \|\thetab\|_2 \le t
		\end{eqnarray*}
		
		%We are looking for the best SSE with $\thetab$ that lies within a $t$-ball around $0$. 
	\end{column}

	\begin{column}{.48\textwidth}
		\begin{center}
			\includegraphics[width=0.45\textwidth, keepaspectratio]{figure_man/ridge.png} 
		\end{center}
	\end{column}
\end{columns}

The problem is a CP: (strictly) convex $f$ and convex $g$.  

% \pause
% These are smooth, (strongly) convex optimization problems in quadratic form. Usually, the unconstrained formulation is used. 
% Again, we can either compute the closed form solution given by  $\thetab = (\Xmat^\top \Xmat + \lambda \bm{I})^{-1}\Xmat^\top \yv$ or use a gradient based optimization method.

\end{vbframe}
	
\begin{vbframe}{Example 4: LASSO Regression}
	
Lasso regression can be formulated as regularized ERM: 

\begin{eqnarray*}  
		\thetah_{\text{Lasso}} &=&  \argmin_{\thetab} \left\{ \sumin \left(\yi - \thetab^\top\xv\right)^2 + \lambda ||\thetab||_1 \right\} 
\end{eqnarray*}

Equivalently it can be written as constrained optimization problem: 

	\vspace*{-0.2cm}
	\begin{columns}[T] % align columns
		\begin{column}{.48\textwidth}
			\begin{eqnarray*}
				\min_{\thetab} && \sum_{i = 1}^n \left(\thetab^\top \xi - \yi\right)^2 \\
				\text{s.t. } && \|\thetab\|_1 \le t
			\end{eqnarray*}
		\end{column}

		\begin{column}{.48\textwidth}
			\begin{center}
				\includegraphics[width=0.45\textwidth, keepaspectratio]{figure_man/lasso.png} 
			\end{center}
		\end{column}
	\end{columns}	

The problem is (still) a CP: (strictly) convex $f$ and convex $g$. However with a non-differentiable constraint $g$.

\end{vbframe}

\begin{vbframe}{Example 5: Support Vector Machines} 

Alternative derivation for the (linear) SVM: 

\begin{itemize}
	\item Find decision boundary which separates classes with \textbf{maximum} safety distance
	\item Distance to points closest to decision boundary (\enquote{safety margin $\gamma$}) should be \textbf{maximized}
\end{itemize}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figure_man/svm-example.pdf}
	\end{center}
\end{figure}

\end{vbframe}

\begin{frame}{Example 5: Support Vector Machines} 

	\only<1>{

	As optimization problem: 

	\begin{eqnarray*}
		& \min\limits_{\thetab, \thetab_0} & \frac{1}{2} \|\thetab\|^2 \\ % + C   \sum_{i=1}^n \sli \\
		& \text{s.t.} & \,\, \yi  \left( \scp{\thetab}{\xi} + \thetab_0 \right) \geq 1 \quad \forall\, i \in \nset
		% & \text{and} & \,\, \sli \geq 0 \quad \forall\, i \in \nset.\\
	\end{eqnarray*}

	\begin{figure}
		\begin{center}
			\includegraphics[width=0.3\textwidth]{figure_man/svm_geometry.png}
		\end{center}
		% Taken from here https://github.com/slds-lmu/lecture_i2ml/tree/master/slides/linear-svm/figure
	\end{figure}


	}

	\only<2>{

	As optimization problem \textcolor{violet}{that tolerates, but minimizes margin violations}: 

	\begin{eqnarray*}
		& \min\limits_{\thetab, \thetab_0,\sli} & \frac{1}{2} \|\thetab\|^2 + \textcolor{violet}{C   \sum_{i=1}^n \sli} \\
		& \text{s.t.} & \,\, \yi  \left( \scp{\thetab}{\xi} + \thetab_0 \right) \geq 1 \textcolor{violet}{- \sli} \quad \forall\, i \in \nset,\\
		& \text{and} & \,\, \textcolor{violet}{\sli \geq 0 \quad \forall\, i \in \nset.}\\
	\end{eqnarray*}

\vspace*{-0.5cm}

	\begin{figure}
		\begin{center}
			\includegraphics[width=0.3\textwidth]{figure_man/boundary_with_violations.png}
		\end{center}
		% Taken from here https://github.com/slds-lmu/lecture_i2ml/tree/master/slides/linear-svm/figure
	\end{figure}
	}

% Taken from here https://github.com/slds-lmu/lecture_i2ml/tree/master/slides/linear-svm/figure

\begin{footnotesize}
	A more thorough derivation of SVMs is given in ``Supervised learning''. 
\end{footnotesize}

\end{frame}	


%	\begin{frame}{Example 6: Support Vector Machines} 
%		
%		Mathematically, the support vector machine problem corresponds to the following optimization problem: 
%		
%		\begin{eqnarray*}
%			& \max\limits_{\thetab, \theta_0} & \gamma \\
%			& \text{s.t.} & \,\, d_f\left(\xi, \yi\right) \ge \gamma \quad \text{for all } i \in \{1, ..., n\}
%		\end{eqnarray*}
%		\pause
%		This is a convex quadratic program based on geometric intuition, but hard to optimize.
%		
%		\begin{center}
%			\includegraphics[width=2.5cm]{figure_man/svm_example.pdf} \\
%		\end{center}
%	\end{frame}	

\begin{frame}{Example 5: Support Vector Machines}

The problem, also called \textbf{soft-margin} SVM in \textbf{primal} form

\begin{eqnarray*}
	& \min\limits_{\thetab, \thetab_0,\sli} & \frac{1}{2} \|\thetab\|^2 + C   \sum_{i=1}^n \sli \\
	& \text{s.t.} & \,\, \yi  \left( \scp{\thetab}{\xi} + \thetab_0 \right) \geq 1 - \sli \quad \forall\, i \in \nset,\\
	& \text{and} & \,\, \sli \geq 0 \quad \forall\, i \in \nset.\\
\end{eqnarray*}

is a CP (even a QP!). It is equivalent to the unconstrained formulation of the SVM via Hinge loss. 
	
\end{frame}

% \begin{frame}{Example 5: Support Vector Machines}

% Another -- equivalent -- formulation is the \textbf{dual} formulation of the SVM: 

% \begin{eqnarray*}
% 	& \max\limits_{\alphav \in \R^n} & \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j\yi y^{(j)} \scp{\xi}{\xv^{(j)}} \\
% 	& \text{s.t. } & 0 \le \alpha_i \le C \quad \forall\, i \in \nset, \\
% 	& \quad & \sum_{i=1}^n \alpha_i \yi = 0,
% \end{eqnarray*}


% \end{frame}

% \begin{frame}{Example 5: Support Vector Machines}
% When applying the kernel trick to the dual (soft-margin) SVM problem by replacing $\scp{\xi}{\xv^{(j)}}$ by kernels $k(\xi, \xv^{(j)})$, we get the non-linear SVM:
% \begin{eqnarray*}
% 	& \max\limits_{\alpha \in \R^n} & \one^\top \alpha - \frac{1}{2} \alpha^\top \diag(\yv) \bm{K} \diag(\yv) \alpha \\
% 	& \text{s.t.} & \alpha^\top \yv = 0, \\
% 	& \quad & 0 \leq \alpha \leq C, 
% \end{eqnarray*}
% where $K_{ij} = k(\xi, \xv^{(j)})$. 
% \pause
% This is still a constrained convex quadratic problem, because $\bm{K} \in \R^{n \times n}$ is positive semi-definite. 
% \end{frame}


\endlecture
\end{document}
