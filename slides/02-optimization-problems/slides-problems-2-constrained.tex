
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\slvec}{\left(\zeta^{(1)}, \zeta^{(n)}\right)} % slack variable vector
\newcommand{\sli}[1][i]{\zeta^{(#1)}} % i-th slack variable
\newcommand{\scptxi}{\scp{\thetab}{\xi}} % scalar prodct of theta and xi
\newcommand{\alphav}{\bm{\alpha}} % vector alpha (bold) (basis fun coefficients)

\newcommand{\titlefigure}{figure_man/unit_circle.png}
\newcommand{\learninggoals}{
\item TODO
\item TODO}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization}
\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Constrained problems}
\lecture{Optimization}
\sloppy

\begin{vbframe}{General Definition}

Consider the \textbf{optimization problem}
$$
\min_{\xv \in \mathcal{S}} \fx
$$
with objective function
$$
f: \; \mathcal{S} \to \R.
$$

The problem is called \textbf{constrained}, if the domain $\mathcal{S}$ is restricted: 

$$
	\mathcal{S} \textcolor{blue}{\subsetneq} \R^d.
$$

$\mathcal{S}$ is typically defined via functions called \textbf{constraints}: 

$$
	\mathcal{S}:= \{\xv \in \R^d ~|~ g_i(\xv) \le 0, h_j(\xv) = 0 ~\forall~ i, j\}
$$ 

where

\begin{itemize}
\item $g_i: \R^d \to \R, i = 1, ..., k$ are called inequality constraints,
\item $h_j: \R^d \to \R, j = 1, ..., l$ are called equality constraints.
\end{itemize}

\framebreak 

We also write the general constrained optimization problem as: 

\begin{eqnarray*}
\min && f(\mathbf{x})  \\
\text{such that} && g_i(\mathbf{x}) \le 0 \qquad \text{for } i=1,\ldots,k  \\
 && h_j(\mathbf{x}) = 0 \qquad \text{for } j=1,\ldots,l. 
\end{eqnarray*}

\textcolor{red}{Types of constraints.... If $f$ and all constraints are \textbf{smooth}, the problem is \textbf{smooth}.}

% \begin{itemize}
% \item \textbf{Box-constrained}:

% $$
% \mathcal{S} = \{\xv \in \R^d: x_i^l \leq x_i \leq x_i^u \text{ for all } i\}
% $$
% \item
% \textbf{Linear or non-linear constraints}
% \begin{eqnarray*}
% \mathcal{S} = \{\xv \in \R^d: g(\xv) \le 0;  h(\xv) = 0\}
% \end{eqnarray*}
% We call $g(\xv)\le 0$ an inequality constraint, and $h(\xv) = 0 $ an equality constraint. 

% \end{itemize}


\end{vbframe}

\begin{vbframe}{Example 1: Unit circle}


\textbf{Example} for a constrained optimization problem: minimization on the unit circle

\begin{eqnarray*}
  \min && f(x_1, x_2) = x_1 + x_2 \\
  \text{s.t. } && g(x_1,x_2) = x_1^2 + x_2^2 - 1 = 0
\end{eqnarray*}

\begin{center}
  \includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/unit_circle.png} \\
\end{center}

\end{vbframe}


\begin{vbframe}{Example 2: Maximum Likelihood Estimation}

\textbf{Example}: Maximum Likelihood Estimation

\lz

For data $\left(\xv^{(1)}, ..., \xv^{(n)}\right)$, we want to find the maximum likelihood estimate

$$
\max_\theta L(\theta) = \prod_{i = 1}^n f(^{(i)}, \theta)
$$

In some cases, $\theta$ can only take \textbf{certain values}. 

\lz 

\begin{itemize}
\item If $f$ is a Poisson distribution, we require the rate $\lambda$ to be non-negative, i.e. $\lambda \ge 0$

  \item If $f$ is a multinomial distribution

\begin{footnotesize}
  $$
  f(x_1, ..., x_p; n; \theta_1, ..., \theta_p) = \begin{cases} \binom{n!}{x_1! \cdot x_2! ... x_p!} \theta_1^{x_1} \cdot ... \cdot \theta_p^{x_p} & \text{if } x_1 + ... + x_p = n \\ 0 & \text{else}
  \end{cases}
  $$
\end{footnotesize}

The probabilities $\theta_i$ must lie between $0$ and $1$ and add up to $1$, i.e. we require 
\begin{eqnarray*}
	0 \le \theta_i \le 1 && \text{ for all } i \\
	 \theta_1 + ... + \theta_p = 1. &&
\end{eqnarray*}

\end{itemize}

\end{vbframe}

	\begin{frame}{Example 3: Ridge regression}
		
		In Ridge regression, we add an $L_2$ penalty on $\thetab$: 
		\vspace*{-0.1cm}
		\begin{eqnarray*}  
			\thetah_{\text{Ridge}} &=& \argmin_{\thetab} \left\{ \sumin \left(\yi - \fxit \right)^2 + \lambda ||\thetab||_2^2 \right\}
		\end{eqnarray*}

		To get a better understanding of the geometry, we reformulate the optimization as a constrained problem:
		\vspace*{-0.2cm}
		\begin{columns}[T] % align columns
			\begin{column}{.48\textwidth}
				\begin{eqnarray*}
					\min_{\thetab} && \sum_{i = 1}^n \left(\thetab^\top \xi - \yi\right)^2 \\
					\text{s.t. } && \|\thetab\|_2 \le t
				\end{eqnarray*}
				
				%We are looking for the best SSE with $\thetab$ that lies within a $t$-ball around $0$. 
			\end{column}

			\begin{column}{.48\textwidth}
				\begin{center}
					\includegraphics[width=0.45\textwidth, keepaspectratio]{figure_man/ridge.png} 
				\end{center}
			\end{column}
		\end{columns}
		\pause
These are smooth, (strongly) convex optimization problems in quadratic form. Usually, the unconstrained formulation is used. 
Again, we can either compute the closed form solution given by  $\thetab = (\Xmat^\top \Xmat + \lambda \bm{I})^{-1}\Xmat^\top \yv$ or use a gradient based optimization method.

\end{frame}
	
\begin{frame}{Example 4: LASSO Regression}
	
	In LASSO regression, we add an $L_1$ penalty on $\thetab$:
	\vspace*{-0.1cm}
	\begin{eqnarray*}
		\thetah_{\text{Lasso}} &=&  \argmin_{\thetab} \left\{ \sumin \left(\yi - \fxit\right)^2 + \lambda ||\thetab||_1 \right\} 
	\end{eqnarray*}
	Analogously, the problem can be reformulated as a constrained optimization problem:
	\vspace*{-0.2cm}
	\begin{columns}[T] % align columns
		\begin{column}{.48\textwidth}
			\begin{eqnarray*}
				\min_{\thetab} && \sum_{i = 1}^n \left(\thetab^\top \xi - \yi\right)^2 \\
				\text{s.t. } && \|\thetab\|_1 \le t
			\end{eqnarray*}
		\end{column}

		\begin{column}{.48\textwidth}
			\begin{center}
				\includegraphics[width=0.45\textwidth, keepaspectratio]{figure_man/lasso.png} 
			\end{center}
		\end{column}
	\end{columns}	

		\pause
These are non-smooth, convex optimization problems. There is no closed form solution and optimization is harder due to the non-differentiability of the constraint. Here, we could use derivative-free optimization methods, e.g. coordinate descent. 	

\end{frame}

\begin{vbframe}{Example 4: LASSO Regression}

\textcolor{red}{Add geometric interpretation for L1 (clipping operator). }

\end{vbframe}

	\begin{frame}{Example 6: Support Vector Machines} 
		
		\begin{itemize}
			\item In a linear support vector machine problem, we want to find a linear decision boundary which separates the classes with a \textbf{maximum} safety distance. 
			\item This means, the distance to the points that are closest to the hyperplane (\enquote{safety margin $\gamma$}) should be \textbf{maximized}. 
			\item We allow violations of the margin constraints via slack variables $\sli \geq 0$
			%\item Let $\gamma > 0$ be the safety margin. 
		\end{itemize}
		
		\vspace*{-0.3cm}
		
		\begin{center}
			\includegraphics[width=3cm]{figure_man/svm_example.pdf} \\
			\begin{footnotesize}
				The safety margin $\gamma$ is indicated by a green arrow. 
			\end{footnotesize}
		\end{center}
	
	\begin{footnotesize}
		A more thorough introduction to SVMs is given in ``Supervised learning''. 
	\end{footnotesize}
	\end{frame}	
%	\begin{frame}{Example 6: Support Vector Machines} 
%		
%		Mathematically, the support vector machine problem corresponds to the following optimization problem: 
%		
%		\begin{eqnarray*}
%			& \max\limits_{\thetab, \theta_0} & \gamma \\
%			& \text{s.t.} & \,\, d_f\left(\xi, \yi\right) \ge \gamma \quad \text{for all } i \in \{1, ..., n\}
%		\end{eqnarray*}
%		\pause
%		This is a convex quadratic program based on geometric intuition, but hard to optimize.
%		
%		\begin{center}
%			\includegraphics[width=2.5cm]{figure_man/svm_example.pdf} \\
%		\end{center}
%	\end{frame}	

\begin{frame}{Example 5: Support Vector Machines}

	This soft-margin SVM optimization problem can be reformulated:
	  \begin{eqnarray*}
		& \min\limits_{\thetab, \thetab_0,\sli} & \frac{1}{2} \|\thetab\|^2 + C   \sum_{i=1}^n \sli \\
		& \text{s.t.} & \,\, \yi  \left( \scp{\thetab}{\xi} + \thetab_0 \right) \geq 1 - \sli \quad \forall\, i \in \nset,\\
		& \text{and} & \,\, \sli \geq 0 \quad \forall\, i \in \nset.\\
	\end{eqnarray*}
The parameter $C$ controls the trade-off between the two conflicting
objectives of maximizing the size of the margin and minimizing the frequency and size of margin
violations.
	\pause
	This is a convex optimization problem -- particularly, a quadratic program with linear constraints and is known as the \textbf{primal} problem.
	
\end{frame}

\begin{frame}{Example 5: Support Vector Machines}

We could directly solve the primal problem, but usually the SVM is solved in the \textbf{dual}:  

\begin{eqnarray*}
	& \max\limits_{\alphav \in \R^n} & \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j\yi y^{(j)} \scp{\xi}{\xv^{(j)}} \\
	& \text{s.t. } & 0 \le \alpha_i \le C, \\
	& \quad & \sum_{i=1}^n \alpha_i \yi = 0,
\end{eqnarray*}
\pause
This is a convex quadratic program with box constraints plus one linear constraint.


\end{frame}

\begin{frame}{Example 5: Support Vector Machines}
When applying the kernel trick to the dual (soft-margin) SVM problem by replacing $\scp{\xi}{\xv^{(j)}}$ by kernels $k(\xi, \xv^{(j)})$, we get the non-linear SVM:
\begin{eqnarray*}
	& \max\limits_{\alpha \in \R^n} & \one^\top \alpha - \frac{1}{2} \alpha^\top \diag(\yv) \bm{K} \diag(\yv) \alpha \\
	& \text{s.t.} & \alpha^\top \yv = 0, \\
	& \quad & 0 \leq \alpha \leq C, 
\end{eqnarray*}
where $K_{ij} = k(\xi, \xv^{(j)})$. 
\pause
This is still a constrained convex quadratic problem, because $\bm{K} \in \R^{n \times n}$ is positive semi-definite. 
\end{frame}


\endlecture
\end{document}
