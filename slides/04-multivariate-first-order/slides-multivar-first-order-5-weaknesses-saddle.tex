\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/multimodal.png}
\newcommand{\learninggoals}{
\item Multimodality
\item Saddle points
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{First order methods: \\Weaknesses of GD -- Saddle points}
\lecture{Optimization in Machine Learning}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Unimodal vs. Multimodal loss surfaces}
\begin{figure}
\centering
\includegraphics[width=12cm]{figure_man/difficult_vs_easy.png}
\footnotesize{In deep learning we often find multimodal loss surfaces. Left: Multimodal loss surface; Right: (Nearly) unimodal loss surface. Source: Hao Li et al. (2017). }
\centering
\vspace{1cm}
\includegraphics[width=.9\textwidth]{figure_man/multimodal.png}
\footnotesize{Potential snippet from a loss surface with many local minima}
\end{figure}

\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {GD: Only locally optimal moves}
\begin{itemize}
\small{
\item GD makes only \textit{locally} optimal moves
\item It may move away from the global optimum
\begin{figure}
	\centering
	\scalebox{0.65}{\includegraphics{figure_man/local_hill.png}}
	\tiny{\\Source: Goodfellow, Ch. 8}
\end{figure}
\item Figure above: initializion on ``wrong'' side of the hill resulting in suboptimal performance
\item In higher dimensions, GD may move around the hill (potentially at the cost of longer trajectory and time to convergence)}
\end{itemize}
\end{frame}

\begin{vbframe} {Local minima}

\begin{itemize}
\item In practice: Only local minima with high value compared to global minimium are problematic.
\begin{figure}
\begin{center}
	\includegraphics[width=.6\textwidth]{figure_man/minima.png}
\end{center}
\tiny{Source: Goodfellow, Ch. 4}
\end{figure}
\item In DL, literature suspects that most local minima have low empirical risk. (\href{https://arxiv.org/abs/1406.2572}{Y. Dauphin et al. (2014)})
\item Simple test: Norm of gradient should get close to zero.
\end{itemize}
\end{vbframe}


\begin{vbframe}{GD at saddle points}
	
	\begin{columns}
		\begin{column}{0.48\textwidth}
			\textbf{Example: }
			\begin{eqnarray*}
				f(x_1, x_2) &=& x_1^2 - x_2^2 \\
				\nabla \fx &=& \left(2 \cdot x_1, - 2 \cdot x_2\right)^\top \\
				\bm{H} &=& \begin{pmatrix} 2 & 0 \\ 0 & - 2\end{pmatrix}
			\end{eqnarray*}

			Along $x_1$, the function curves upwards (pos. eigenvalue belonging to eigenvector $(1, 0)$ of $\bm{H}$). Along $x_2$, the function curves downwards. 
			
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=4cm]{figure_man/saddlepoint.png}
			\end{figure} 
		\end{column}	
	\end{columns}
	%\item Second-order algorithms experience even greater problems when dealing with saddle points. Newtons method for example actively searches for a region with zero gradient. That might be another reason why second-order methods have not succeeded in replacing gradient descent for neural network training. 
	
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	
	\frametitle{Example: Saddle point with GD}
	\begin{itemize}
		\item So how do saddle points impair optimization?
		\item First-order algorithms that use only gradient information \textbf{might} get stuck in saddle points
	\end{itemize}
	\center
	\only<1>{\includegraphics[width=9cm]{figure_man/opt1.png}}%
	\only<2>{\includegraphics[width=9cm]{figure_man/opt2.png}}%
	\only<3>{\includegraphics[width=9cm]{figure_man/opt3.png}}%
	\only<4>{\includegraphics[width=9cm]{figure_man/opt10.png}}%
    \only<5>{\includegraphics[width=9cm]{figure_man/saddle_point_grad_norm.pdf}}%
	
	\begin{itemize}
		
		\only<1>{\item[] \small{Red dot: Starting location}}
		\only<2>{\item[] \small{First step...}}
		\only<3>{\item[] \small{...second step...}}
		\only<4>{\item[] \small{...tenth step got stuck and cannot escape the saddle point!}}
        \only<5>{\item[] \small{...tenth step got stuck and cannot escape the saddle point!}}
		
	\end{itemize}
	
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Cliffs and Exploding Gradients}
%\begin{vbframe}{Cliffs and exploding gradients}
%\begin{itemize}
%\item As a result from the multiplication of several parameters, the emprirical risk for highly nonlinear deep neural networks often contain sharp nonlinearities.
%\begin{itemize}
%\item That may result in very high derivatives in some places.
%\item As the parameters get close to such cliff regions, a gradient descent update can catapult the parameters very far.
%\item Such an occurrence can lead to losing most of the optimization work that had been done.
%\end{itemize}
%\item However, serious consequences can be easily avoided using a technique called \textbf{gradient
%clipping}.
%\item The gradient does not specify the optimal step size, but only the optimal direction
%within an infinitesimal region.
%\framebreak 
%\item Gradient clipping simply caps the step size to be small enough that it is less likely to go outside the region where the gradient indicates the direction of steepest descent.
%\item We simply \enquote{prune} the norm of the gradient at some threshold $h$:
%$$\text{if  } ||\nabla \thetab|| > \text h: \nabla \thetab \leftarrow \frac{h}{||\nabla \thetab||} \nabla \thetab $$
%\end{itemize}
%\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{vbframe}{Example: cliffs and exploding gradients}
%\begin{figure}
%\centering
%\includegraphics[width=8cm]{figure_man/cliff2.png}
%\caption{\enquote{The objective function for highly nonlinear deep neural networks or for
%	recurrent neural networks often contains sharp nonlinearities in parameter space resulting
%	from the multiplication of several parameters. These nonlinearities give rise to very
%	high derivatives in some places. When the parameters get close to such a cliff region, a
%	gradient descent update can catapult the parameters very far, possibly losing most of the
%	optimization work that had been done} (Goodfellow et al. (2016)).}
%\end{figure}
%\end{vbframe}

\begin{vbframe}{Saddle Points}
  \begin{itemize}
    \item In optimization we look for areas with zero gradient.
    \item A variant of zero gradient areas are saddle points.
    \item For the empirical risk $\risk$ of a neural network, the expected ratio of the number of saddle points to local minima typically grows exponentially with $m$ 
    $$\risk: \R^m \rightarrow \R$$ 
    In other words: Networks with more parameters (deeper networks or larger layers) exhibit a lot more saddle points than local minima.
     \item Why is that?
    \item The Hessian at a local minimum has only positive eigenvalues. At a saddle point it is a mixture of positive and negative eigenvalues.
    
\framebreak
    
    \item Imagine the sign of each eigenvalue is generated by coin flipping:
    \begin{itemize}
      \item In a single dimension, it is easy to obtain a local minimum (e.g. \enquote{head} means positive eigenvalue).
      \item In an $m$-dimensional space, it is exponentially unlikely that all $m$ coin tosses will be head.
    \end{itemize}
    \item A property of many random functions is that eigenvalues of the Hessian become more likely to be positive in regions of lower cost.
    \item For the coin flipping example, this means we are more likely to have heads $m$ times if we are at a critical point with low cost.
    \item That means in particular that local minima are much more likely to have low cost than high cost and critical points with high cost are far more likely to be saddle points.
    \item See Dauphin et al. (2014) for a more detailed investigation.
    
\framebreak
    
    \item \enquote{Saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum} (Dauphin et al. (2014)).
  \begin{figure}
    \centering
    \includegraphics[width=8.5cm]{figure_man/cost.png}
    % TAKEN FROM i2dl LECTURE
  \end{figure}
  \end{itemize}
\end{vbframe}

\endlecture
\end{document}

