\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/saddlepoint.png}
\newcommand{\learninggoals}{
\item Definition
\item Max. Likelihood 
\item Normal regression
\item Risk Minimization
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{First order methods: \\Weaknesses of GD -- Saddle points}
\lecture{Optimization in Machine Learning}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{GD at saddle points}
	
	\begin{columns}
		\begin{column}{0.48\textwidth}
			\textbf{Example: }
			\begin{center}
				$f(x_1, x_2) = x_1^2 - x_2^2$
			\end{center}
			Along $x_1$, the function curves upwards (eigenvector of the Hessian with positive eigenvalue). Along $x_2$, the function curves downwards (eigenvector of the Hessian with negative eigenvalue).
			
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=4cm]{figure_man/saddlepoint.png}
			\end{figure} 
		\end{column}	
	\end{columns}
	%\item Second-order algorithms experience even greater problems when dealing with saddle points. Newtons method for example actively searches for a region with zero gradient. That might be another reason why second-order methods have not succeeded in replacing gradient descent for neural network training. 
	
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	
	\frametitle{Example: Saddle point with GD}
	\begin{itemize}
		\item So how do saddle points impair optimization?
		\item First-order algorithms that use only gradient information \textbf{might} get stuck in saddle points.
	\end{itemize}
	\center
	\only<1>{\includegraphics[width=9cm]{figure_man/opt1.png}}%
	\only<2>{\includegraphics[width=9cm]{figure_man/opt2.png}}%
	\only<3>{\includegraphics[width=9cm]{figure_man/opt3.png}}%
	\only<4>{\includegraphics[width=9cm]{figure_man/opt10.png}}%
	
	\begin{itemize}
		
		\only<1>{\item[] \small{Red dot: Starting location}}
		\only<2>{\item[] \small{First step...}}
		\only<3>{\item[] \small{...second step...}}
		\only<4>{\item[] \small{...tenth step got stuck and cannot escape the saddle point!}}
		
	\end{itemize}
	
}

\begin{vbframe}{Unimodal vs. Multimodal loss surfaces}
\begin{figure}
\centering
\includegraphics[width=12cm]{figure_man/difficult_vs_easy.png}
\footnotesize{Left: Multimodal loss surface with saddle points; Right: (Nearly) unimodal loss surface (Hao Li et al. (2017))}
\centering
\vspace{1cm}
\includegraphics[width=.9\textwidth]{figure_man/multimodal.png}
\footnotesize{Potential snippet from a loss surface with many local minima}
\end{figure}

\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Only locally optimal moves}
\begin{itemize}
\small{
\item If the training algorithm makes only \textit{locally} optimal moves (as in gradient descent), it may move away from regions of \textit{much} lower cost.
\begin{figure}
	\centering
	\scalebox{0.65}{\includegraphics{figure_man/local_hill.png}}
	\tiny{\\Source: Goodfellow, Ch. 8}
\end{figure}
\item In the figure above, initializing the parameter on the ``wrong'' side of the hill will result in suboptimal performance.
\item In higher dimensions, however, it may be possible for gradient descent to go around the hill but such a trajectory might be very long and result in excessive training time.}
\end{itemize}
\end{frame}

\begin{vbframe} {Local minima}

\begin{itemize}
\item In practice only local minima with a high value compared to the global minimium are problematic.
\begin{figure}
\begin{center}
	\includegraphics[width=.6\textwidth]{figure_man/minima.png}
\end{center}
\tiny{Source: Goodfellow, Ch. 4}
\end{figure}
\item In DL, literature suspects that most local minima have low empirical risk. (\href{https://arxiv.org/abs/1406.2572}{Y. Dauphin et al. (2014)})
\item Simple test: Norm of gradient should get close to zero.
\end{itemize}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Cliffs and Exploding Gradients}
%\begin{vbframe}{Cliffs and exploding gradients}
%\begin{itemize}
%\item As a result from the multiplication of several parameters, the emprirical risk for highly nonlinear deep neural networks often contain sharp nonlinearities.
%\begin{itemize}
%\item That may result in very high derivatives in some places.
%\item As the parameters get close to such cliff regions, a gradient descent update can catapult the parameters very far.
%\item Such an occurrence can lead to losing most of the optimization work that had been done.
%\end{itemize}
%\item However, serious consequences can be easily avoided using a technique called \textbf{gradient
%clipping}.
%\item The gradient does not specify the optimal step size, but only the optimal direction
%within an infinitesimal region.
%\framebreak 
%\item Gradient clipping simply caps the step size to be small enough that it is less likely to go outside the region where the gradient indicates the direction of steepest descent.
%\item We simply \enquote{prune} the norm of the gradient at some threshold $h$:
%$$\text{if  } ||\nabla \thetab|| > \text h: \nabla \thetab \leftarrow \frac{h}{||\nabla \thetab||} \nabla \thetab $$
%\end{itemize}
%\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{vbframe}{Example: cliffs and exploding gradients}
%\begin{figure}
%\centering
%\includegraphics[width=8cm]{figure_man/cliff2.png}
%\caption{\enquote{The objective function for highly nonlinear deep neural networks or for
%	recurrent neural networks often contains sharp nonlinearities in parameter space resulting
%	from the multiplication of several parameters. These nonlinearities give rise to very
%	high derivatives in some places. When the parameters get close to such a cliff region, a
%	gradient descent update can catapult the parameters very far, possibly losing most of the
%	optimization work that had been done} (Goodfellow et al. (2016)).}
%\end{figure}
%\end{vbframe}

\endlecture
\end{document}

