\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/momentum_illustration_medium.png}
\newcommand{\learninggoals}{
\item Eigendecomp of QF
\item GD steps in eigenspace
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{First order methods: GD on quadratic forms}
\lecture{Optimization in Machine Learning}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{GD on quadratic forms}

\begin{itemize}
	\item We consider the quadratic function $q(\xv) = \xv^\top \bm{A} \xv - \bm{b}^\top \xv$. 
	\item We assume that the Hessian $\bm{H} = \Amat$ is symmetric and invertible 
	\item The optimal solution is $\xv^\ast = \Amat^{-1} b$ 
	\item As $\nabla q(\xv) = \Amat \xv - \bm{b}$, the iterations of gradient descent are
\end{itemize}
	$$
	\xv^{[ t+1 ] } = \xv^{[ t ] } - \alpha (\Amat \xv^{[ t ] } - \bm{b})
	$$

\begin{figure}
	\includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/gd.png} \\
\end{figure}

\begin{footnotesize}
The following slides follow the blogpost by Goh, "Why Momentum Really Works", Distill, 2017. \url{http://doi.org/10.23915/distill.00006}
\end{footnotesize}

\framebreak

For $\Amat$, there exists an eigenvalue decomposition: 

$$
	\Amat = \bm{V} \, \bm{\Lambda} \bm{V}^\top
$$

where the columns of $\bm{V}$ contain the eigenvectors $\boldsymbol{v}_i$ and $\bm{\Lambda} = \text{diag}(\lambda_1, ..., \lambda_n)$ contains the eigenvalues $\lambda_i$ sorted from smallest to biggest. We perform a change of basis $\bm{w}^{[ t ] } = \bm{V}^\top (\xv^{[ t ] } - \xv^\ast)$ to its eigenspace, where all dimensions act independently. 

\begin{figure}
	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/gd_eigenspace.png} \\
\end{figure}

\framebreak 

We get 

\begin{eqnarray*}
    \xv^{[t + 1]} &=& \xv^{[t]} - \alpha \left(\Amat \xv^{[t]} - \bm{b} \right) \\
    \bm{w}^{[ t ] } &=& \bm{V}^\top (\xv^{[ t ] } - \xv^\ast) \\
    \bm{V} \cdot \bm{w}^{[ t ] } + \xv^\ast &=&  \xv^{[ t ] } \\
\end{eqnarray*}

and with $\textcolor{blue}{\xv^{[ t ] } = \bm{V} \cdot \bm{w}^{[ t ] } + \xv^\ast}$ we can write a GD step as: 

\begin{footnotesize}
\begin{alignat*}{2}
    \textcolor{blue}{\xv^{[t + 1]}} &= \textcolor{blue}{\xv^{[t]}} - \alpha \left(\Amat \textcolor{blue}{\xv^{[t]}} - \bm{b} \right) && \\
    \textcolor{blue}{\bm{V} \cdot \bm{w}^{[ t + 1] } + \xv^\ast} &= \textcolor{blue}{\bm{V} \cdot \bm{w}^{[ t ] } + \xv^\ast} - \alpha \left(\Amat \cdot \textcolor{blue}{\bm{V} \cdot \bm{w}^{[ t ] }} + \Amat \cdot \textcolor{blue}{\xv^\ast} - \bm{b} \right) ~\qquad && \bigg|~- \xv^\ast\\
    \bm{V} \cdot \bm{w}^{[ t + 1] } &= \bm{V} \cdot \bm{w}^{[ t ] } - \alpha \left(\Amat \cdot \bm{V} \cdot \bm{w}^{[ t ] } + \Amat \xv^\ast - \bm{b} \right) ~\qquad && \bigg|~\bm{V}^\top (\text{NB: } \bm{V}^\top \bm{V} = \bm{1})\\
    \bm{w}^{[ t + 1] } &= \bm{w}^{[ t ] } - \alpha \bm{V}^\top\left(\Amat \bm{V} \cdot \bm{w}^{[ t ] } + \Amat \xv^\ast - \bm{b} \right) && \bigg|~\Amat \xv^\ast - \bm{b} = 0 \\
    \bm{w}^{[ t + 1] } &= \bm{w}^{[ t ] } - \alpha \bm{V}^\top\Amat \bm{V} \cdot \bm{w}^{[ t ] }&& \bigg|~\bm{V}^\top \bm{A} \bm{V} = \bm{\Sigma} \\
    \bm{w}^{[ t + 1] } &= \bm{w}^{[ t ] } - \alpha \bm{\Sigma} \bm{w}^{[ t ] }&& \\
\end{alignat*}
\end{footnotesize}

Which means: 

$$
w_i^{[ t+1 ] } &=& w_i^{[ t ] } - \alpha \lambda_i w_i^{[ t ] } = (1- \alpha \lambda_i) w_i^{[ t ] } = (1- \alpha \lambda_i)^{t+1 } w_i^{[ 0 ] }
$$


\framebreak
If we now perform GD on $\boldsymbol{w}$, we get

\begin{eqnarray*}
	w_i^{[ t+1 ] } &=& w_i^{[ t ] } - \alpha \lambda_i w_i^{[ t ] } \\
	&=& (1- \alpha \lambda_i) w_i^{[ t ] } = (1- \alpha \lambda_i)^{t+1 } w_i^{[ 0 ] }
\end{eqnarray*}

\begin{figure}
	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/gd_eigenspace.png} \\
\end{figure}

\framebreak

Moving back to the original space, we get

$$
	\xv^{[ t ] } - \xv^\ast = \bm{V} \cdot \boldsymbol{w}^{[ t ] } = \sum_{i = 1}^{d} w_i^{[0]} ( 1 - \alpha \lambda_i)^t \bm{v}_i 
$$

This allows a very intuitive interpretation: each element of $w^{[0]}$ is the component of the error in the initial guess in the eigenbasis and decays with a rate of $1 - \alpha \lambda_i$. 

For most step sizes, the eigenvectors with the largest eigenvalues converge the fastest. 

\framebreak

We now consider the contribution of each eigenvector to the total loss

$$
	q(\xv^{[ t ] }) - q(\xv^\ast) = \frac{1}{2} \sum_{i}^{d} (1 - \alpha \lambda_i)^{2t} \lambda_i (w_i^{[0]})^2 
$$

\begin{figure}
	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/gd_conv.png} \\
\end{figure}

%\framebreak

%The considerations above already gave some guidance on how to choose the optimal step size $\alpha$. In order to converge, $| 1 - \alpha \lambda_i| $ must be strictly less than $1$. 

%\begin{itemize}
%	\item use eigenvectors of $A$
%	\item GD in closed form 
%	\item decomposing the error, plot with convergence
%	\item choosing step size, show dependence on condition number 
%	\item maybe with example of polynomial regression
%\end{itemize}
\end{vbframe}


\endlecture
\end{document}

