\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/SGD_cropped.png}
\newcommand{\learninggoals}{
\item SGD
\item Erratic behavior  
\item Convergence
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{First order methods: SGD}
\lecture{Optimization in Machine Learning}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\begin{vbframe}{Stochastic gradient descent}
	
%	Let us consider GD for empirical risk minimization. The updates are: 
%	
%	$$
%	\thetab^{[t + 1]} = \thetab^{[t]} - \alpha \cdot \frac{1}{n} \cdot \sumin \nabla_\theta L\left(\yi, f(\xi ~|~ \thetab^{[t]})\right)
%	$$
%	
%	
%	\begin{itemize}
%		\item Optimization algorithms that use the entire training set to compute updates in one huge step are called \textbf{batch} or \textbf{deterministic}. This is computationally very costly or often impossible. 
%		\item \textbf{Idea:} Instead of letting the sum run over the whole dataset (\textbf{batch mode}) one can also let it run only over small subsets (\textbf{minibatches}), or only over a single example $i$. 
%		% \item One \textbf{epoch} means one pass of the full training set.
%		\item If the index $i$ of the training example is a random variable with uniform distribution, then its expectation is the batch gradient $\nabla_\theta \risket$
%		\item[$\to$] We have a \textbf{stochastic}, noisy version of the batch gradient
%		
%		\framebreak 
%		
%		\item The gradient w.r.t. a single training observation is fast to compute but not reliable. It can be used simply as a computational trick to deal with large data or to operate on real streams of online data in online learning.
%		\item In contrast, the full
%		batch gradient is costly (or even impossible, e.g., when data does not even fit into memory) to compute, particularly in DL, but it averages out all the noise from sub-sampling.
%		\item Minibatches are in between. The batch size decides upon the compromise
%		between speed and averaging (smoothing).
%		\item In summary: SGD computes an unbiased estimate of the gradient by taking the average gradient over a minibatch (or one sample) to update the parameter $\thetab$ in this direction.
%		% \item Optimization algorithms that use only a single example at a time are called \textbf{stochastic} or \textbf{online}. This can be used simply as a computational trick to deal with large data or to operate on real streams of online data in online learning.
%		% Those methods are called \textbf{minibatch} or \textbf{stochastic}.
%	\end{itemize}
	
	

	% \begin{algorithm}[H]
	% \footnotesize
	%   \caption{Basic SGD pseudo code}
	%   \begin{algorithmic}[1]
	%   \State Initialize parameter vector $\thetab^{[0]}$ 
	%   \State Randomly shuffle data and partition into minibatches $J_1, ..., J_k$ of size $m$
	%   \State $t \leftarrow 0$
	%   \While{stopping criterion not met}
	%   \State Take a minibatch $J$ of $m$ examples from training set, $J \subset \nset$
	%       \State Compute gradient estimate: $\hat{g}^{[t]} \leftarrow \frac{1}{m} \sum_{i \in J} \nabla_\theta L(\yi, f(\xi ~|~ \thetab^{[t]}) $
	%       \State Apply update: $\thetab^{[t]} \leftarrow \thetab^{[t-1]} - \alpha \hat{g}^{[t]}$
	%       \State $t \leftarrow t + 1$
	%     \EndWhile
	%   \end{algorithmic}
	% \end{algorithm}
	% \begin{itemize}
	%   \item Thus, what SGD basically does is computing an unbiased estimate of the gradient by taking the average gradients of a minibatch to update the parameter $\thetab$.
	% \end{itemize}
	
	%\framebreak
	

	% \begin{itemize}
	%   \item Thus, what SGD basically does is computing an unbiased estimate of the gradient by taking the average gradients of a minibatch to update the parameter $\thetab$.
	% \end{itemize}
	
%	\framebreak
%	
%	\vspace*{0.5cm}
%	\begin{itemize}
%		\item With minibatches of size $m$, a full pass over the training set (called an \textbf{epoch}) consists of $\frac{n}{m}$ gradient updates.
%		\item SGD and its modifications are the most used optimization algorithms for ML in general and for deep learning in particular.
%		\item SGD (with one or a few samples per batch) updates have a high variance, even though they are unbiased. 
%		Because of this variance, the learning rate $\alpha$ is typically much smaller than in the full-batch scenario.
%		
%		\framebreak 
%		
%		\vspace*{0.5cm}
%		
%		\item When the learning rate is slowly decreased, SGD converges to a local minimum.
%		\item SGD with minibatches reduces the variance of the parameter updates and utilizes highly optimized matrix operations to efficiently compute gradients.
%		\item Minibatch sizes are typically between 50 and 1000.
%		\item Recent results indicate, that SGD often leads to better generalizing models then GD, and thus may perform some kind of indirect regularization.
%	\end{itemize}
%\end{vbframe}


\begin{vbframe}{Stochastic gradient descent}

NB: We use $g$ instead of $f$ as objective, bc. $f$ is used as model in ML. 

\lz 

$g: \R^d \to \R$ objective, $g$ \textbf{average over functions}: 

$$
	g(\xv) = \frac{1}{n}\sumin g_i(\xv), \qquad g \text{ and } g_i \text{ smooth}
$$

Stochastic gradient descent (SGD) approximates the gradient 

\vspace*{-0.2cm}

\begin{eqnarray*}
	\nabla_\xv~ g(\xv) = \frac{1}{n}\sumin \nabla_{\xv}~g_i(\xv) &:=& \bm{d} \quad \text{ by }\\
	\frac{1}{\textcolor{blue}{|J|}}\sum_{i \in \textcolor{blue}{J}} \nabla_\xv~g_i(\xv) &:=& \bm{\hat d}, 
\end{eqnarray*}

with random subset $J \subset \{1, 2, ..., n\}$ of gradients called \textbf{mini-batch}. This is done e.g. when computing the true gradient is \textbf{expensive}. 

\framebreak 

	\begin{algorithm}[H]
		\footnotesize
		\caption{Basic SGD pseudo code}
		\begin{algorithmic}[1]
			\State Initialize $\xv^{[0]}$, $t = 0$ 
			\While{stopping criterion not met}
			\State Randomly shuffle data and partition into minibatches $J_1, ..., J_K$ of size $m$
			\For{$k\in\{1,...,K\}$} 
			\State $t \leftarrow t + 1$ 
			\State Compute gradient estimate with $J_k$: $\bm{\hat{d}}^{[t]} \leftarrow \frac{1}{m} \sum_{i \in J_k} \nabla_\xv g_i(\xv^{[t - 1]}) $
			\State Apply update: $\xv^{[t]} \leftarrow \xv^{[t-1]} - \alpha \cdot \bm{\hat{d}}^{[t]}$
			
			\EndFor		
			
			\EndWhile
		\end{algorithmic}
	\end{algorithm}

\vspace*{0.2cm}

\begin{footnotesize}
	\begin{itemize}
		\item Instead of drawing batches randomly we might want to go through the $g_i$ sequentially (unless $g_i$ are sorted in any way)
		\item Updates are computed faster, but also more stochastic: 
		\begin{itemize} 
			\begin{footnotesize}	
			\item In the simplest case, batch-size $m := |J_k|$ is set to $m = 1$
			\item If $n$ is a billion, computation of update is a billion times faster
			\item \textbf{But} (we will see later): Convergence rates suffer from stochasticity!
			\end{footnotesize}
		\end{itemize} 
	\end{itemize}
\end{footnotesize}

\end{vbframe}

\begin{vbframe}{SGD in ML}

	In ML, we perform ERM:  

	$$
		\riskt = \frac{1}{n}\sumin \underbrace{\Lxyit}_{g_i(\theta)}
	$$

	\begin{itemize}
		\item for a data set 
		$$\D = \Dset$$
		\item a loss function $\Lxy$,
		e.g. L2 loss $\Lxy = \left(y - \fx\right)^2$,
		\item and a model class $f$, e.g. the linear model $\fxit = \thetab^\top \xv$. 
	\end{itemize}

	\framebreak 

	For large data sets, computing the exact gradient 
	$$
		\bm{d} = \frac{1}{n}\sum_{i=1}^n \nabla_{\thetab} \Lxyit \quad \text{(also: score)}
	$$ 
	may be expensive or infeasible to compute and is approximated by 
	$$
	\bm{\hat d} = \frac{1}{m}\sum_{i \in J} \nabla_{\thetab} \Lxyit,
	$$
        for $J \subset{1, 2, ..., n}$ random subset. 

        \lz 
        
	\textbf{NB:} Often, the max. size of $J$ technically limited by how much data fits in to the memory. 
\end{vbframe}


\begin{vbframe}{Stochasticity of SGD}

	% The iterations of SGD are \textbf{stochastic}, as they depend on randomly drawn observations. % It is assumed that the above equation behaves exactly like its expectation.
	
	\vspace*{0.2cm}

	\begin{figure}
		\scalebox{0.7}{\includegraphics{figure_man/SGD.png}}
		\begin{tiny}\\ 
		Source: Shalev-Shwartz, Ben-David. Understanding machine learning. Cambridge University Press, 2014. 
		\end{tiny}\\
        Minimize $g(x_1, x_2) = 1.25(x_1 + 6)^2 + (x_2 - 8)^2)$. \\ Left: GD. Right: SGD, the black line depicts the averaged value of $\xv$.
	\end{figure}

	\framebreak 

        Assume batch size $m = 1$ (statements also apply for higher batch size).

	\begin{itemize}

		\item \textbf{(Possibly) suboptimal direction: }Approx. gradient $\bm{\hat d} = \nabla_\xv g_i(\xv)$ might point in suboptimal (possibly not even descent!) direction
		\item \textbf{Unbiased estimate: } If $J$ drawn i.i.d., approx. gradient $\bm{\hat d}$ is an unbiased estimate of gradient $\bm{d} = \nabla_\xv g(\xv) = \sumin \nabla_\xv g_i(\xv)$: 

		\vspace*{-0.5cm}

		\begin{eqnarray*}
			\mathbb{E}_{i}\left[\nabla_\xv ~g_i(\xv)\right] &=&\sumin \nabla_\xv ~g_i(\xv) \cdot \P(i = i) = \sumin \nabla_\xv ~ g_i(\xv) \cdot \frac{1}{n}\\ &=& \frac{1}{n} \sumin \nabla_\xv~g_i(\xv) = \nabla_\xv~ g(\xv).
		\end{eqnarray*}
	\end{itemize}
\textbf{Conclusion:} SGD might perform single suboptimal moves, but moves in \enquote{right direction} \textbf{on average}. 

\end{vbframe}

\begin{frame}{Erratic behavior of SGD}

\textbf{Example:}	$g(\xv) = \sum_{i = 1}^5 g_i(\xv)$, $g_i$ quadratic. We run SGD with $m = 1$. 

	\only<1>{
	\begin{figure}
			\includegraphics[width = 0.8\textwidth]{figure_man/sgd_example_iter_1.png}
	\end{figure}}
	\only<2>{
	\begin{figure}
			\includegraphics[width = 0.8\textwidth]{figure_man/sgd_example_iter_2.png}
	\end{figure}}
	\only<3>{
	\begin{figure}
			\includegraphics[width = 0.8\textwidth]{figure_man/sgd_example_iter_3.png}
	\end{figure}}
	\only<4>{
	\begin{figure}
			\includegraphics[width = 0.8\textwidth]{figure_man/sgd_example_iter_4.png}
	\end{figure}}
	\only<5>{
	\begin{figure}
			\includegraphics[width = 0.8\textwidth]{figure_man/sgd_example_iter_5.png} \\
			In iteration $5$, SGD performs a suboptimal move away from the optimum. 
	\end{figure}}
\end{frame}

\begin{vbframe}{Erratic behavior of SGD}

 	\begin{figure}
 		\vspace{-0.3cm}
 		\centering
 		\includegraphics[width = 0.7\textwidth]{figure_man/sgd_example_confusion_areas.png} \newline
        \begin{footnotesize}
  $\xv$ in \textbf{blue area}: $- \nabla g_i(\xv)$ will point towards optimum, no matter which $i$ we sample. \\ $\xv$ in \textbf{red area} (confusion area): We might sample $i$ for which $- \nabla g_i(\xv)$ points away from optimum and perform suboptimal moves close to optimum. 
        \end{footnotesize}

  \end{figure}



 	\framebreak 

	\begin{itemize}
		\item At $\xv$, \enquote{confusion} is captured variance of gradients
		$$
			\frac{1}{n}\sumin \|\nabla_\xv~ g_i(\xv) - \nabla_\xv ~ g(\xv)\|^2
		$$
		\item If this variance is $0$, the next step will be in the right direction (independent of the sampled $i$)
		\item If the term is small, the next step will likely go in the right direction
		\item If the term is large, the step will likely go in the wrong direction (middle of confusion area, where $\xv^\ast$ lives)
	\end{itemize}

\end{vbframe}

\begin{vbframe}{Convergence of SGD}

	As a consequence, SGD has worse convergence properties than GD. However, this can be controlled via \textbf{increasing the batch size} or \textbf{reducing the step size}. 

	\begin{blocki}{The larger the batch size $m$}
		\item the better the approximation to $\nabla_\xv g(\xv)$
		\item the lower the variance
		\item the lower the risk of performing steps in the wrong direction
	\end{blocki}

	\begin{blocki}{The smaller the step size $\alpha$}
		\item the smaller a step in a potentially wrong direction 
		\item the lower the effect of high variance
	\end{blocki}


As maximum batch size is usually limited by computational resources (memory), choosing the step size is crucial. 

\end{vbframe}


\endlecture
\end{document}

