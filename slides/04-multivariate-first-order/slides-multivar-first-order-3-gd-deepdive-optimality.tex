\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\Hess}{\mathbf{H}}

\newcommand{\titlefigure}{figure_man/gdes_1.png}
\newcommand{\learninggoals}{
\item Convergence of GD 
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Deep dive: \\Gradient descent and optimality}
\lecture{Optimization in Machine Learning}
\sloppy
	
	\begin{vbframe}{Gradient Descent and Optimality}
	
	\begin{itemize}
		\item GD is a greedy algorithm: In every iteration, it makes locally optimal moves.
		\vspace*{0.5mm}
		\item If $\riskt$ is \textbf{convex} and \textbf{differentiable}, and its gradient is Lipschitz continuous, GD is guaranteed to converge to the global minimum for small enough step-size.  
		% \vspace*{0.5mm}
		% \item However, if $\riskt$ has multiple local optima and/or saddle points, GD might only converge to a stationary point (other than the global optimum), depending on the starting point. 
	\end{itemize}
	
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{figure_man/gdes_1.png}
	\end{figure}



\framebreak
	
	Assume $f: \R^d \rightarrow \R$ convex and differentiable and assume that global minimum $\xv^\ast$ exists. Assume $\nabla f$ is Lipschitz continuous with $L > 0$: 

	\begin{equation*}
	|| \nabla \fx - \nabla f(\bm{y}) || \le L ||\fx - \bm{y} || \quad \text{ for all } \xv, y
	\end{equation*}
	
	(i.e., gradient can't change arbitrarily fast).
		
	\lz 
		
	\textbf{Convergence of GD:} GD with $k$ iterations with starting point $\xv^{[0]}$ and fixed step-size $\alpha \leq 1/L$ will yield a solution $f(\xv^{[k]})$, which satisfies

	$$
		f(\xv^{[k]}) - f(\xv^\ast) \leq \frac{|| \xv^{[0]} - {x}^\ast ||^2}{2\alpha k}
	$$
	
	This means, that GD converges with rate $\mathcal{O}(1/k)$.
	

	\framebreak 	
	
	\textbf{Proof: }
	From $\nabla f$ Lipschitz it follows that $\nabla^2 \fx \preccurlyeq L \cdot \mathbf{I}$ for all $\xv$. 

	\vspace*{0.2cm}

	\begin{footnotesize} NB: The generalized inequality $\nabla^2 \fx \preccurlyeq L I$ means that $L \cdot \mathbf{I} - \nabla^2 \fx$ is positive semidefinite. This means that $ \bm{v} ^\top \nabla^2 f(\bm{u}) \bm{v} \leq L || \bm{v} ||^2$ for any $\bm{u}$ and $ \bm{v} $. 
	\end{footnotesize}
	
	\vspace*{0.2cm}

	We perform a quadratic expansion of f around $\tilde{\mathbf{x}}$:

	\begin{eqnarray*}
		\fx &\approx& f(\tilde{\mathbf{x}}) + \nabla f(\tilde{\mathbf{x}})^\top (\xv - \tilde{\mathbf{x}}) + \textcolor{blue}{0.5  (\xv - \tilde{\mathbf{x}})^\top \nabla^2 f(\tilde{\mathbf{x}}) (\xv - \tilde{\mathbf{x}})} \\
		& \leq & f(\tilde{\mathbf{x}}) + \nabla f(\tilde{\mathbf{x}})^\top (\xv - \tilde{\mathbf{x}}) + 0.5 L ||\xv - \tilde{\mathbf{x}}||^2 \text{ (descent lemma)},
	\end{eqnarray*}		
	as the blue term is at most $0.5 \cdot L \cdot ||\xv - \tilde{\mathbf{x}}||^2$. 
	
	\lz 

	Now, do one GD update with step size $\alpha \leq 1/L$: 

	$$
	\xv^{[t+1]} = \xv^{[t]} - \alpha \nabla f\left(\xv^{[t]}\right)
	$$ 
	and plug this in the descent lemma.
		

	\framebreak 	

	\begin{footnotesize}
	
	We get
	\vspace*{-0.3cm}
	\begin{eqnarray*}
	f(\xv^{[t+1]}) &\leq& f(\xv^{[t]}) - \nabla f(\xv^{[t]})^\top(\xv^{[t+1]} - \xv^{[t]}) + \frac{1}{2}L ||\xv^{[t+1]} - \xv^{[t]}||^2 \\
	&=& f(\xv^{[t]}) + \nabla f(\xv^{[t]})^\top(\xv^{[t]} - \alpha \nabla f(\xv^{[t]}) - \xv^{[t]}) + \frac{1}{2}L ||\xv^{[t]} - \alpha \nabla f(\xv^{[t]}) - \xv^{[t]}||^2 \\
	& = & f(\xv^{[t]}) - \nabla f(\xv^{[t]})^\top \alpha  \nabla f(\xv^{[t]}) + \frac{1}{2}L ||\alpha \nabla f(\xv^{[t]})||^2 \\
	&=& f(\xv^{[t]}) - \alpha ||\nabla f(\xv^{[t]})||^2 + \frac{1}{2}L\alpha^2 ||\nabla f(\xv^{[t]})||^2 \\
	&=& f(\xv^{[t]}) - (1 - \frac{1}{2} L \alpha)\alpha  ||\nabla f(\xv^{[t]})||^2 \\
	&\le& f(\xv^{[t]}) - \frac{1}{2}\alpha ||\nabla f(\xv^{[t]})||^2, 
	\end{eqnarray*}

	where we used $\alpha \leq 1/L$ and therefore $- (1 - \frac{1}{2} L \alpha) \leq \frac{1}{2} L \frac{1}{L} -1 = -\frac{1}{2}$.
				
	Since $\frac{1}{2} \alpha ||\nabla f(\xv^{[t]})||^2$ is always positive unless $\nabla f(x) = 0$, it implies that $f$ strictly decreases with each iteration of GD until the optimal value is reached. So, it is a bound on guaranteed progress if $\alpha \leq 1/L$. 
	\end{footnotesize}
	
	\framebreak
	
			Now, we bound $\fx$ in terms of $f(\xv^\ast)$ using that $f$ is convex: 
			
			$$
			\fx \leq f(\xv^\ast) + \nabla f(\xv)^\top (\xv - \xv^\ast)
			$$ 
			
			When we combine this and the bound derived before, we get
			
			\begin{eqnarray*}
				f(\xv^{[t+1]}) - f(\xv^\ast) &\leq& \nabla \xv^\top (\xv-\xv^*) - \frac{\alpha}{2}||\nabla \fx||^2 \\
				&=& \frac{1}{2 \alpha} \left( ||\xv-\xv^\ast||^2 - || \xv - \xv^\ast - \alpha \nabla \fx||^2 \right) \\
				&=& \frac{1}{2 \alpha} \left( ||\xv-\xv^\ast||^2 - || \xv^{[t+1]} - \xv^\ast ||^2 \right)
			\end{eqnarray*}
		
		This holds for every iteration of GD. 
		
		\framebreak 
		
		Summing over iterations, we get: 
		
		\begin{eqnarray*}
			\sum_{t = 0}^{k} f(\xv^{[t+1]}) - f(\xv^\ast) &\leq& \sum_{t= 0}^{k} \frac{1}{2 \alpha} \left( ||\xv^{[t]}-\xv^\ast||^2 - || \xv^{[t+1]} - \xv^\ast ||^2 \right) \\
			&=& \frac{1}{2 \alpha}  \left( ||x^{[0]}-\xv^\ast||^2 - || x^{k} - \xv^\ast ||^2 \right) \\
			& \leq & \frac{1}{2 \alpha} \left( ||x^{[0]}-\xv^\ast||^2 \right),
		\end{eqnarray*}
	
		where we used that the LHS is a telescoping sum. In addition, we know that $f$ decreases on every iteration, so we can conclude that
		$$
		f(\xv^{[k]}) - f(\xv^\ast) \leq \frac{|| \xv^{[0]} - \xv^\ast ||^2}{2\alpha k}
		$$
%		\textbf{Note: } It might not be that bad if we do not find the global optimum:  
%		
%		\begin{itemize}
%			\item We do not optimize the actual quantity of interest, i.e. the (theoretical) risk, but only an approximate version, i.e. the empirical risk. 
%			\item If the model class is very flexible, it might be disadvantageous to optimize too aggressively and increase the risk of overfitting. 
%			\item Early-stopping the optimization might even increase generalization performance. 
%		\end{itemize}
		
	\end{vbframe}	
	
	\endlecture
\end{document}