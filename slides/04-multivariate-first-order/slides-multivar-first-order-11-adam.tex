\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/momentum_illustration_medium.png}
\newcommand{\learninggoals}{
\item Definition
\item Max. Likelihood 
\item Normal regression
\item Risk Minimization
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{First order methods: ADAM}
\lecture{Optimization in Machine Learning}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Adaptive learning rates}
	\begin{itemize}
		\item The learning rate is reliably one of the hyperparameters that is the most difficult to set because it has a significant impact on the models performance.
		\item Naturally, it might make sense to use a different learning rate for each parameter, and automatically adapt them throughout the training process.
	\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Adagrad}
	\begin{itemize}
		\item Adagrad adapts the learning rate to the parameters.
		\item In fact, Adagrad scales learning rates inversely proportional to the square root of the sum of the past squared derivatives.
		\begin{itemize}
			\item Parameters with large partial derivatives of the loss obtain a rapid decrease in their learning rate.
			\item Parameters with small partial derivatives on the other hand obtain a relatively small decrease in their learning rate.
		\end{itemize}
		\item For that reason, Adagrad might be well suited when dealing with sparse data. 
		\item Goodfellow et al. (2016) say that the accumulation of squared gradients can result in a premature and overly decrease in the learning rate.
	\end{itemize}
	
	\framebreak
	
	
	\begin{algorithm}[H]
		\small
		\caption{Adagrad}
		\begin{algorithmic}[1]
			\scriptsize 
			\State \textbf{require} Global learning rate $\alpha$ \strut
			\State \textbf{require} Initial parameter $\thetab$ \strut
			\State \textbf{require} Small constant $\beta$, perhaps $10^{-7}$, for numerical stability \strut
			\State \textbf{Initialize} gradient accumulation variable $\mathbf{r} = \mathbf{0} $
			\While{stopping criterion not met}
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
			\State Compute gradient estimate: $\hat{\mathbf{g}} \leftarrow \frac{1}{m} \nabla_{\thetab} \sum_{i} \Lxym$
			\State Accumulate squared gradient $\mathbf{r} \leftarrow \mathbf{r} + \hat{\mathbf{g}} \odot  \hat{\mathbf{g}}$
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla \thetab = - \frac{\alpha}{\beta + \sqrt\mathbf{r}} \odot \hat{\mathbf{g}}$ (division and square root applied element-wise) \strut}
			\State Apply update: $\thetab \leftarrow \thetab + \nabla\thetab$
			\EndWhile
		\end{algorithmic}
	\end{algorithm}
	\begin{itemize}
		\small
		\item \enquote{$\odot$} is called Hadamard or element-wise product.
		% \item Example:
		% \vspace{0.2cm}
		% \item[] $A =
		% \begin{bmatrix}
		% 1 & 2 \\
		% 3 & 4
		% \end{bmatrix}, \ 
		% B =
		% \begin{bmatrix}
		% 5 & 6 \\
		% 7 & 8
		% \end{bmatrix}, \ \text{ then } A \odot B =
		% \begin{bmatrix}
		% 1 \cdot 5 & 2 \cdot 6 \\
		% 3 \cdot 7 & 4 \cdot 8
		% \end{bmatrix}$
	\end{itemize}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{RMSProp}
	\begin{itemize}
		\item RMSprop is a modification of Adagrad.
		\item It's intention is to resolve Adagrad's radically diminishing learning rates.
		\item The gradient accumulation is replaced by an exponentially weighted moving average.
		\item Theoretically, that leads to performance gains in non-convex scenarios.
		\item Empirically, RMSProp is a very effective optimization algorithm. Particularly, it is employed routinely by deep learning practitioners.
	\end{itemize}
	
	\framebreak
	
	
	\begin{algorithm}[H]
		\small
		\caption{RMSProp}
		\begin{algorithmic}[1]
			\State \textbf{require} Global learning rate $\alpha$ and decay rate $\rho \in [0, 1)$ \strut
			\State \textbf{require} Initial parameter $\mathbf{\thetab}$ \strut
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require} Small constant $\beta$, perhaps $10^{-6}$, for numerical stability \strut}
			\State Initialize gradient accumulation variable $\mathbf{r} = \mathbf{0} $
			\While{stopping criterion not met}
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
			\State Compute gradient estimate: $\hat{\mathbf{g}} \leftarrow \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
			\State Accumulate squared gradient $\mathbf{r} \leftarrow \rho \mathbf{r} + (1 - \rho) \hat{\mathbf{g}} \odot  \hat{\mathbf{g}}$
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla\mathbf{\thetab} = - \frac{\alpha}{\beta + \sqrt\mathbf{r}} \odot \hat{\mathbf{g}}$ \strut}
			\State Apply update: $\mathbf{\thetab} \leftarrow \mathbf{\thetab} + \nabla\mathbf{\thetab}$
			\EndWhile
		\end{algorithmic}
	\end{algorithm}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Adam}
	\begin{itemize}
		\item Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter.
		\item Adam uses the first and the second moments of the gradients.
		\begin{itemize}
			\item Adam keeps an exponentially decaying average of past gradients (first moment).
			\item Like RMSProp it stores an exponentially decaying average of past squared gradients (second moment).
			\item Thus, it can be seen as a combination of RMSProp and momentum.
		\end{itemize}
		\item Basically Adam uses the combined averages of previous gradients at different moments to give it more \enquote{persuasive power} to adaptively update the parameters.
	\end{itemize}
	
	
	\framebreak
	
	\begin{algorithm}[H]
		\scriptsize 
		\caption{Adam}
		\begin{algorithmic}[1]
			\State \textbf{require} Step size $\alpha$ (suggested default: 0.001) \strut
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require} Exponential decay rates for moment estimates, $\rho_1$ and $\rho_2$ in $[0,1)$ (suggested defaults: 0.9 and 0.999 respectively)} \strut
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require} Small constant $\beta$ (suggested default $10^{-8}$) \strut}
			\State \textbf{require} Initial parameters $\thetab$ 
			\State Initialize time step $t = 0$
			\State Initialize 1st and 2nd moment variables $\mathbf{s}^{[0]} = 0, \mathbf{r}^{[0]} = 0$
			\While{stopping criterion not met}
			\State $t \leftarrow t + 1$
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
			\State Compute gradient estimate: $\hat{\mathbf{g}}^{[t]} \leftarrow \frac{1}{m} \nabla_{\thetab} \sum_{i} \Lxym$
			\State Update biased first moment estimate: $\mathbf{s}^{[t]} \leftarrow \rho_1 \mathbf{s}^{[t-1]}  + (1 - \rho_1) \hat{\mathbf{g}}^{[t]}$
			\State Update biased second moment estimate: $\mathbf{r}^{[t]} \leftarrow \rho_2 \mathbf{r}^{[t-1]}  + (1 - \rho_2) \hat{\mathbf{g}}^{[t]} \odot \hat{\mathbf{g}}^{[t]}$
			\State Correct bias in first moment: $\hat{\mathbf{s}} \leftarrow \frac{\mathbf{s}^{[t]} }{1-\rho_1^t}$
			\State Correct bias in second moment: $\hat{\mathbf{r}} \leftarrow \frac{\mathbf{r}^{[t]} }{1-\rho_2^t}$
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla\thetab = - \alpha \frac{\hat{\mathbf{s}}}{\sqrt{\hat{\mathbf{r}}} + \beta}$ \strut}
			\State Apply update: $\thetab \leftarrow \thetab + \nabla\thetab$
			
			\EndWhile
		\end{algorithmic}
	\end{algorithm}
	
	
	\framebreak
	
	
	\begin{itemize}
		\item Adam initializes the exponentially weighted moving averages $\mathbf{s}$ and $\mathbf{r}$ as $\mathbf{0}$ (zero) vectors.
		\item As a result, they are biased towards zero. 
		\item This means $\E[\mathbf{s}^{[t]}] \neq \E [\hat{\mathbf{g}}^{[t]}]$ and $\E[\mathbf{r}^{[t]}] \neq \E [\hat{\mathbf{g}}^{[t]} \odot \hat{\mathbf{g}}^{[t]}]$ (where the expectations are calculated over minibatches).
		\item To see this, let us unroll the computation of $\mathbf{s}^{[t]}$ for a few time-steps:
		\footnotesize
		\begin{gather*}
		\mathbf{s}^{[0]} = 0 \\
		\mathbf{s}^{[1]} = \rho_1\mathbf{s}^{[0]} + (1 - \rho_1) \hat{\mathbf{g}}^{[1]} = (1 - \rho_1) \hat{\mathbf{g}}^{[1]} \\
		\mathbf{s}^{[2]} = \rho_1\mathbf{s}^{[1]} + (1 - \rho_1) \hat{\mathbf{g}}^{[2]} = \rho_1 (1 - \rho_1) \hat{\mathbf{g}}^{[1]} + (1 - \rho_1) \hat{\mathbf{g}}^{[2]} \\
		\mathbf{s}^{[3]} = \rho_1\mathbf{s}^{[2]} + (1 - \rho_1) \hat{\mathbf{g}}^{[3]} = \rho_1^2 (1 - \rho_1) \hat{\mathbf{g}}^{[1]} + \rho_1 (1 - \rho_1) \hat{\mathbf{g}}^{[2]} + (1 - \rho_1) \hat{\mathbf{g}}^{[3]}
		\end{gather*}
		\normalsize
		\item Therefore, $\mathbf{s}^{[t]}  = (1 - \rho_1) \sum_{i=1}^t \rho_1^{t - i} \mathbf{g}^{[i]}$.
		\item Note that the contribution of the earlier $\hat{\mathbf{g}}^{[i]}$ to the moving average shrinks rapidly.
	\end{itemize}
	
	\framebreak
	
	
	\begin{itemize}
		\item The expected value  of $\mathbf{s}^{[t]}$ is:
		\footnotesize
		\begin{gather*}
		\E [\mathbf{s}^{[t]}] = \E [ (1 - \rho_1) \sum_{i=1}^t \rho_1^{t - i} \hat{\mathbf{g}}^{[i]}] \\
		= \E [\hat{\mathbf{g}}^{[t]}] (1 - \rho_1) \sum_{i=1}^t \rho_1^{t - i} + \zeta\\
		= \E [\hat{\mathbf{g}}^{[t]}] (1 - \rho_1^{t}) + \zeta
		\end{gather*}
		\normalsize
		where we approximate $\hat{\mathbf{g}}^{[i]}$ with $\hat{\mathbf{g}}^{[t]}$ which allows us to move it outside the sum. $\zeta$ is the error that results from this approximation.
		\item Therefore, $\mathbf{s}^{[t]}$ is a biased estimator of $\hat{\mathbf{g}}^{[t]}$ and the effect of the bias vanishes over the time-steps (because $\rho_1^t \rightarrow 0$ for $t \rightarrow \infty$).
		\item Ignoring $\zeta$ (as it can be kept small), we correct for the bias by setting $\hat{\mathbf{s}}^{[t]} = \frac{\mathbf{s}^{[t]}}{(1 - \rho_1^{t})}$.
		\item Similarly, we set $\hat{\mathbf{r}}^{[t]} = \frac{\mathbf{r}^{[t]}}{(1 - \rho_2^{t})}$.
	\end{itemize}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\frame{
	
	\frametitle{Comparison of optimizers: Animation}
	\vspace{1cm}
	\begin{figure}
		\begin{center}
			\vspace{-1cm}
			\href{https://giphy.com/embed/SJVFO3IcVC0M0}{\includegraphics[width = .49\textwidth]{figure_man/sattle_point2-50.png}}
			\href{https://giphy.com/embed/SJVFO3IcVC0M0}{\includegraphics[width = .49\textwidth]{figure_man/sattle_point2-150.png}}
			\tiny{\\Credits: Dettmers (2015) and Radford}
		\end{center}
		\footnotesize{Excerpts from an animation to compare the behavior of momentum and
			other methods compared to SGD for a saddle point. Left: After a few seconds; Right: A bit later. The animation shows that all showed methods accelerate optimization compared to the standard SGD. The highest acceleration is obtained using Rmsprop followed by Adagrad as learning rate strategies. You can find the animation \href{https://giphy.com/embed/SJVFO3IcVC0M0}{\textcolor{blue}{here}} or click on the images above.}
	\end{figure}
}

\endlecture
\end{document}


