\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/sattle_point2-50.png}
\newcommand{\learninggoals}{
\item Adaptive Step Sizes
\item Adagrad 
\item RMSProp
\item ADAM
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{First order methods: ADAM and friends}
\lecture{Optimization in Machine Learning}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Adaptive Step Sizes}
	\begin{itemize}
		\item Step size is probably the most important control param
        \item Has strong influence on performance
		\item Natural to use different SS for each input, 
            and to automatically adapt them
	\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Adagrad}
	\begin{itemize}
		\item Adagrad adapts SSs by
		scaling them inversely proportional to square root of the sum of the past squared derivatives
		\begin{itemize}
			\item Inputs with large partial derivatives get rapid decrease in SS
			\item Inputs with small PDs get small decrease in SS
		\end{itemize}
		%\item For that reason, Adagrad might be well suited when dealing with sparse data. 
		\item Goodfellow et al. (2016) say that the accumulation of squared gradients can result in premature decrease in SS
	\end{itemize}
	
	\framebreak
	
	
	\begin{algorithm}[H]
		\small
		\caption{Adagrad}
		\begin{algorithmic}[1]
			\scriptsize 
			\State \textbf{require} Global SS $\alpha$ \strut
			\State \textbf{require} Initial parameter $\thetab$ \strut
			\State \textbf{require} Small constant $\beta$, perhaps $10^{-7}$, for numerical stability \strut
			\State \textbf{Initialize} gradient accumulation variable $\mathbf{r} = \mathbf{0} $
			\While{stopping criterion not met}
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
			\State Compute gradient estimate: $\hat{\mathbf{g}} \leftarrow \frac{1}{m} \nabla_{\thetab} \sum_{i} \Lxym$
			\State Accumulate squared gradient $\mathbf{r} \leftarrow \mathbf{r} + \hat{\mathbf{g}} \odot  \hat{\mathbf{g}}$
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla \thetab = - \frac{\alpha}{\beta + \sqrt\mathbf{r}} \odot \hat{\mathbf{g}}$ (division and square root applied element-wise) \strut}
			\State Apply update: $\thetab \leftarrow \thetab + \nabla\thetab$
			\EndWhile
		\end{algorithmic}
	\end{algorithm}
	\begin{itemize}
		\small
		\item $\odot$: element-wise product (Hadamard)
		% \item Example:
		% \vspace{0.2cm}
		% \item[] $A =
		% \begin{bmatrix}
		% 1 & 2 \\
		% 3 & 4
		% \end{bmatrix}, \ 
		% B =
		% \begin{bmatrix}
		% 5 & 6 \\
		% 7 & 8
		% \end{bmatrix}, \ \text{ then } A \odot B =
		% \begin{bmatrix}
		% 1 \cdot 5 & 2 \cdot 6 \\
		% 3 \cdot 7 & 4 \cdot 8
		% \end{bmatrix}$
	\end{itemize}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{RMSProp}
	\begin{itemize}
		\item Modification of Adagrad
		\item Resolves Adagrad's radically diminishing SSs.
		\item Gradient accumulation is replaced by exponentially weighted moving average.
		\item Theoretically, leads to performance gains in non-convex scenarios.
		\item Empirically, RMSProp is a very effective optimization algorithm. Particularly, it is employed routinely by DL practitioners.
	\end{itemize}
	
	\framebreak
	
	
	\begin{algorithm}[H]
		\small
		\caption{RMSProp}
		\begin{algorithmic}[1]
			\State \textbf{require} Global SS $\alpha$ and decay rate $\rho \in [0, 1)$ \strut
			\State \textbf{require} Initial parameter $\mathbf{\thetab}$ \strut
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require} Small constant $\beta$, perhaps $10^{-6}$, for numerical stability \strut}
			\State Initialize gradient accumulation variable $\mathbf{r} = \mathbf{0} $
			\While{stopping criterion not met}
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
			\State Compute gradient estimate: $\hat{\mathbf{g}} \leftarrow \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
			\State Accumulate squared gradient $\mathbf{r} \leftarrow \rho \mathbf{r} + (1 - \rho) \hat{\mathbf{g}} \odot  \hat{\mathbf{g}}$
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla\mathbf{\thetab} = - \frac{\alpha}{\beta + \sqrt\mathbf{r}} \odot \hat{\mathbf{g}}$ \strut}
			\State Apply update: $\mathbf{\thetab} \leftarrow \mathbf{\thetab} + \nabla\mathbf{\thetab}$
			\EndWhile
		\end{algorithmic}
	\end{algorithm}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Adam}
	\begin{itemize}
		\item Adaptive Moment Estimation also has adaptive SSs
		\item Uses the 1st and 2nd moments of gradients
		\begin{itemize}
			\item Keeps an exponentially decaying average of past gradients (1st moment)
			\item Like RMSProp, stores an exp-decaying avg of past squared gradients (2nd moment)
			\item Can be seen as combo of RMSProp + momentum.
		\end{itemize}
		%\item Basically Adam uses the combined averages of previous gradients at different moments to give it more \enquote{persuasive power} to adaptively update the parameters.
	\end{itemize}
	
	
	\framebreak
	
	\begin{algorithm}[H]
		\scriptsize 
		\caption{Adam}
		\begin{algorithmic}[1]
			\State \textbf{require} Global step size $\alpha$ (suggested default: 0.001) \strut
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require} Exponential decay rates for moment estimates, $\rho_1$ and $\rho_2$ in $[0,1)$ (suggested defaults: 0.9 and 0.999 respectively)} \strut
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require} Small constant $\beta$ (suggested default $10^{-8}$) \strut}
			\State \textbf{require} Initial parameters $\thetab$ 
			\State Initialize time step $t = 0$
			\State Initialize 1st and 2nd moment variables $\mathbf{s}^{[0]} = 0, \mathbf{r}^{[0]} = 0$
			\While{stopping criterion not met}
			\State $t \leftarrow t + 1$
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
			\State Compute gradient estimate: $\hat{\mathbf{g}}^{[t]} \leftarrow \frac{1}{m} \nabla_{\thetab} \sum_{i} \Lxym$
			\State Update biased first moment estimate: $\mathbf{s}^{[t]} \leftarrow \rho_1 \mathbf{s}^{[t-1]}  + (1 - \rho_1) \hat{\mathbf{g}}^{[t]}$
			\State Update biased second moment estimate: $\mathbf{r}^{[t]} \leftarrow \rho_2 \mathbf{r}^{[t-1]}  + (1 - \rho_2) \hat{\mathbf{g}}^{[t]} \odot \hat{\mathbf{g}}^{[t]}$
			\State Correct bias in first moment: $\hat{\mathbf{s}} \leftarrow \frac{\mathbf{s}^{[t]} }{1-\rho_1^t}$
			\State Correct bias in second moment: $\hat{\mathbf{r}} \leftarrow \frac{\mathbf{r}^{[t]} }{1-\rho_2^t}$
			\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla\thetab = - \alpha \frac{\hat{\mathbf{s}}}{\sqrt{\hat{\mathbf{r}}} + \beta}$ \strut}
			\State Apply update: $\thetab \leftarrow \thetab + \nabla\thetab$
			
			\EndWhile
		\end{algorithmic}
	\end{algorithm}
	
	
	\framebreak
	
	
	\begin{itemize}
		\item Inits exp-weighted moving averages $\mathbf{s}$ and $\mathbf{r}$ as $\mathbf{0}$ (zero) vectors
		\item Hence, they are biased towards zero
		\item This means $\E[\mathbf{s}^{[t]}] \neq \E [\hat{\mathbf{g}}^{[t]}]$ and $\E[\mathbf{r}^{[t]}] \neq \E [\hat{\mathbf{g}}^{[t]} \odot \hat{\mathbf{g}}^{[t]}]$\\ (where the expectations are calculated over minibatches)
		\item To see, let's unroll $\mathbf{s}^{[t]}$:
		\footnotesize
		\begin{gather*}
		\mathbf{s}^{[0]} = 0 \\
		\mathbf{s}^{[1]} = \rho_1\mathbf{s}^{[0]} + (1 - \rho_1) \hat{\mathbf{g}}^{[1]} = (1 - \rho_1) \hat{\mathbf{g}}^{[1]} \\
		\mathbf{s}^{[2]} = \rho_1\mathbf{s}^{[1]} + (1 - \rho_1) \hat{\mathbf{g}}^{[2]} = \rho_1 (1 - \rho_1) \hat{\mathbf{g}}^{[1]} + (1 - \rho_1) \hat{\mathbf{g}}^{[2]} \\
		\mathbf{s}^{[3]} = \rho_1\mathbf{s}^{[2]} + (1 - \rho_1) \hat{\mathbf{g}}^{[3]} = \rho_1^2 (1 - \rho_1) \hat{\mathbf{g}}^{[1]} + \rho_1 (1 - \rho_1) \hat{\mathbf{g}}^{[2]} + (1 - \rho_1) \hat{\mathbf{g}}^{[3]}
		\end{gather*}
		\normalsize
		\item Therefore, $\mathbf{s}^{[t]}  = (1 - \rho_1) \sum_{i=1}^t \rho_1^{t - i} \mathbf{g}^{[i]}$.
		\item NB: contrib of earlier $\hat{\mathbf{g}}^{[i]}$ to moving average shrinks rapidly
	\end{itemize}
	
	\framebreak
	
	
	\begin{itemize}
		\item Now:
		\footnotesize
		\begin{gather*}
		\E [\mathbf{s}^{[t]}] = \E [ (1 - \rho_1) \sum_{i=1}^t \rho_1^{t - i} \hat{\mathbf{g}}^{[i]}] \\
		= \E [\hat{\mathbf{g}}^{[t]}] (1 - \rho_1) \sum_{i=1}^t \rho_1^{t - i} + \zeta\\
		= \E [\hat{\mathbf{g}}^{[t]}] (1 - \rho_1^{t}) + \zeta
		\end{gather*}
		\normalsize
		where we approximate $\hat{\mathbf{g}}^{[i]}$ with $\hat{\mathbf{g}}^{[t]}$ which allows us to move it outside the sum. $\zeta$ is the error that results from this approximation.
		\item Therefore, $\mathbf{s}^{[t]}$ is a biased estimator of $\hat{\mathbf{g}}^{[t]}$ and the effect of the bias vanishes over the time-steps (because $\rho_1^t \rightarrow 0$ for $t \rightarrow \infty$).
		\item Ignoring $\zeta$ (as it is small), we correct for the bias by setting $\hat{\mathbf{s}}^{[t]} = \frac{\mathbf{s}^{[t]}}{(1 - \rho_1^{t})}$.
		\item Similarly, we set $\hat{\mathbf{r}}^{[t]} = \frac{\mathbf{r}^{[t]}}{(1 - \rho_2^{t})}$.
	\end{itemize}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\frame{
	
	\frametitle{Comparison of optimizers: Animation}
	\vspace{1cm}
	\begin{figure}
		\begin{center}
			\vspace{-1cm}
			\href{https://giphy.com/embed/SJVFO3IcVC0M0}{\includegraphics[width = .49\textwidth]{figure_man/sattle_point2-50.png}}
			\href{https://giphy.com/embed/SJVFO3IcVC0M0}{\includegraphics[width = .49\textwidth]{slides/04-multivariate-first-order/figure_man/sattle_point2-100.png}}
			\tiny{\\Credits: Dettmers (2015) and Radford}
		\end{center}
  
  \vspace{1cm}
		\href{https://giphy.com/embed/SJVFO3IcVC0M0}{\textcolor{blue}{Comparison}} of SGD optimizers near saddle point. Left: After few secs; Right: Later. All methods accelerate compared to vanilla SGD. Best is Rmsprop, then Adagrad.
	\end{figure}
}

\begin{vbframe}{Comparison on quadratic form}

\begin{figure}
    \scalebox{0.8}{\includegraphics{slides/04-multivariate-first-order/figure_man/momentum/comparison_adam.png}} \\
    SGD vs. SGD with Momentum vs. ADAM on a quadratic form. 
\end{figure} 

\end{vbframe}

\endlecture
\end{document}


