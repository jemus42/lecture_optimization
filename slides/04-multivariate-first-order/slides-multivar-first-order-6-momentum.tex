\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/momentum_illustration_medium.png}
\newcommand{\learninggoals}{
\item Definition
\item Max. Likelihood 
\item Normal regression
\item Risk Minimization
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{First order methods: GD with Momentum}
\lecture{Optimization in Machine Learning}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Recap: Weaknesses of Gradient Descent}
\begin{itemize}
\item \textbf{Zig-zagging behavior:} For ill-conditioned problems, GD moves with a zig-zag course to the optimum, since the gradient points approximately orthogonal in the shortest direction to the minimum.
\item \textbf{Slow crawling:} may vanish rapidly close to stationary points (e.g. saddle points) and hence also slows down progress.
\item \textbf{Trapped in stationary points:} In some functions GD converges to stationary points (e.g. saddle points) since gradient on all sides is fairly flat and the step size is too small to pass this flat part.
\end{itemize}

\lz 

\textbf{Aim}: More efficient algorithms which quickly reach the minimum.

\end{vbframe}

% \item (Stochastic) Gradient Descent not optimal in areas with significantly stronger curvature in one direction than in the others (for example saddle point)
% \item SGD is influenced by strong curvature of the optimization function and moves slowly towards the minimum.



\begin{vbframe}{GD with momentum}

\begin{itemize}
\item \textbf{Idea: } \enquote{Velocity} $\bm{\nu}$. Velocity increases if successive gradients point in the same direction, but decreases if are opposite / different. 

\begin{figure}
	\includegraphics[width = 0.5\textwidth]{figure_man/momentum_illustration_medium.png} \\
	\begin{footnotesize}
	Source: H. Khandewal, \emph{Gradient Descent with Momentum, RMSprop And Adam Optimizer}, Medium 2020. 
	\end{footnotesize}
\end{figure}

\item $\bm{\nu}$ is the (weighted) moving average of the previous gradients:
% \begin{eqnarray*}
%         \bm{\nu}_{i+1} &\leftarrow& \varphi \bm{\nu}_{i} + \underbrace{\nabla_{\bm{\theta}_i} L(y^{(j)}, f_{\bm{\theta}_i}(x^{(j)}))}_{g(\bm{\theta})} \\
%         \bm{\theta}_{i + 1} &\leftarrow&  \bm{\theta}_{i} - \lambda \bm{\nu}_{i+1}
% \end{eqnarray*}
\begin{eqnarray*}
        \bm{\nu}^{[t+1]} &\leftarrow& \varphi \bm{\nu}^{[t]} - \alpha \nabla f(\bm{x}^{[t]}) \\
        \bm{x}^{[t + 1]} &\leftarrow&  \bm{x}^{[t]} + \bm{\nu}^{[t+1]}
\end{eqnarray*}
\item $\alpha$ is the step size and $\varphi \in [0,1]$ is an additional hyperparameter. 
\end{itemize}

\framebreak


\begin{itemize}
% \item Adaptive step size and therefore faster convergence than GD possible.
\item In GD: the step size is simply the gradient multiplied by the learning rate $\alpha$
\item Now, the step size depends on how large and how aligned a sequence of gradients is. The step size grows when many successive gradients point in the same direction.
\item $\varphi$ determines how strongly previous gradients are included in $\bm{\nu}$.
\item Common values for $\varphi$ are 0.5, 0.9 and even 0.99
\item In general, the larger $\varphi$ is relative to the learning rate $\alpha$, the more previous gradients affect the current direction. 
\item $\varphi = 0$ equals gradient descent.
\item Can be seen as GD with \enquote{short term memory} for the direction of motion.
\end{itemize}

\end{vbframe}

%<<eval = FALSE, echo = FALSE>>=
%# generate base data
%options(warn = -1)
%x0 = seq(-3L, 3L, length.out = 200L)
%y0 = cos(x0)
%x1 = x0
%y1 = y0 + rnorm(n = length(y0), sd = 0.3)


%# generate moving average
%ma1 = data.frame(x = x0[(1/(1-0.5)):length(x0)], y = rollapply(y1, FUN = mean, width = (1/(1-0.5))))
%ma1 = rbind(rep(NA, length.out = 2*(200L - length(ma1$x))), ma1)
%ma2 = data.frame(x = x0[(1/(1-0.95)):length(x0)], y = rollapply(y1, FUN = mean, width = (1/(1-0.95) + 1)))
%ma2 = rbind(data.frame(x = rep(NA, length.out = (200L - length(ma2$x))),
%                       y = rep(NA, length.out = (200L - length(ma2$x)))), ma2)
%ma3 = data.frame(x = x0[(1/(1-0.99)):length(x0)], y = rollapply(y1, FUN = mean, width = (1/(1-0.99) + 1)))
%ma3 = rbind(data.frame(x = rep(NA, length.out = (200L - length(ma3$x))),
%                       y = rep(NA, length.out = (200L - length(ma3$x)))), ma3)

%plotdata = data.frame(x0, y0, x1, y1, ma1x = ma1$x, ma1y = ma1$y, ma2x = ma2$x, ma2y = ma2$y,
%                      ma3x = ma3$x, ma3y = ma3$y)

%#plot data and moving average
%plot = ggplot(data = plotdata, aes(x0, y0, color = x))
%plot = plot + geom_line(aes(x0, y0, colour = "cos()"))
%plot = plot + geom_point(aes(x1, y1, colour = "cos() + noise"))
%plot = plot + xlab("x")
%plot = plot + ylab("y")
%plot = plot + xlim(-3, 3)
%plot = plot + scale_colour_manual(values = c("black", "red"))
%plot = plot + theme(legend.position = "bottom")
%plot = plot + theme(legend.title = element_blank())

%plot2 = ggplot(data = plotdata, aes(ma1x, ma1y, color = x))
%plot2 = plot2 + geom_line(aes(ma1x, ma1y, colour = paste('phi', " = 0.5")))
%plot2 = plot2 + geom_line(aes(ma2x, ma2y, colour = paste('phi', " = 0.95")))
%plot2 = plot2 + geom_line(aes(ma3x, ma3y, colour = paste('phi', " = 0.99")))
%plot2 = plot2 + geom_point(aes(x1, y1, colour = "cos() + noise"))
%plot2 = plot2 + xlab("x")
%plot2 = plot2 + ylab("y")
%plot2 = plot2 + xlim(-3, 3)
%plot2 = plot2 + scale_colour_manual(values = c("red", "green", "black", "orange", "yellow"))
%plot2 = plot2 + theme(legend.position = "bottom")
%plot2 = plot2 + theme(legend.title = element_blank())

%grid.arrange(plot, plot2, ncol = 2)

%options(warn = 0)
%@

% \scriptsize
% \begin{itemize}
% \item Kleines $\varphi$ (z.B. $\varphi = 0.5$) führt zu starker Fluktuation .
% \item Großes $\varphi$ (z.B. $\varphi = 0.99$) führt zu guter Glättung jedoch Verschiebung.
% \end{itemize}
% $\rightarrow$ Geringe Richtungsänderung: Erhöhe durch großes $\varphi$ die Geschwindigkeit des Algorithmus.
% \medskip
%
% $\rightarrow$ Starke Richtungsänderung: Verringere durch kleines $\varphi$ die Geschwindigkeit um die Krümmung besser abzubilden.
%
% \framebreak

%\textbf{Connection of velocity, momentum and gradient:}
%
%We consider the momentum in iteration $t$ depending on the previous iterations:
%
%\vspace*{-0.5cm}
%  \begin{eqnarray*}
%    \bm{\nu}^{[t]} &\leftarrow& \varphi \bm{\nu}^{[t-1]} + \nabla f(\bm{x}^{[t]}) \\
%    \bm{\nu}^{[t-1]} &\leftarrow& \varphi \bm{\nu}^{[t-2]} + \nabla f(\bm{x}^{[t-1]}) \\
%    \bm{\nu}^{[t-2]} &\leftarrow& \varphi \bm{\nu}^{[t-3]} + \nabla f(\bm{x}^{[t-2]}) \\
%    \\
%    \bm{\nu}^{[t]} &=& \varphi(\varphi(\varphi \bm{\nu}^{[t-3]} + \nabla f(\bm{x}^{[t-2]})) + \nabla f(\bm{x}^{[t-1]})) + \nabla f(\bm{x}^{[t]})\\
%    \bm{\nu}^{[t]} &=& \varphi^{3} \bm{\nu}^{[t-3]} + \varphi^{2} \nabla f(\bm{x}^{[t-2]}) +\varphi \nabla f(\bm{x}^{[t-1]}) + \nabla f(\bm{x}^{[t]})
%  \end{eqnarray*}
%  \vspace*{-0.6cm}
%\begin{itemize}
%% \item Da $\varphi \in [0,1]$ geht die Funktion $g(\theta_{t-n}), n\in \N$ mit geringerem Gewicht in $\bm{\nu}_{t}$ ein als $g(\theta_{t-(n-1)})$.
%\item Since $\varphi \in [0,1]$, the gradient of the iteration $t-1$ influences the velocity $\bm{\nu}^{[t]}$ more than the gradients of the previous iterations ($t-2, t-3,...$).
%\end{itemize}

%\framebreak

\begin{frame}{Momentum: Example}
	\footnotesize 
	\begin{eqnarray*}
		\bm{\nu}^{[1]} &\leftarrow& \textcolor{blue}{\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\bm{x}^{[0]})} \\[0.1cm]
		\bm{x}^{[1]} &\leftarrow& \bm{x}^{[0]} + \textcolor{blue}{\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\bm{x}^{[0]})} \\[0.1cm]
		\pause
		\bm{\nu}^{[2]} &\leftarrow& \textcolor{red}{\varphi \bm{\nu}^{[1]} - \alpha \nabla f(\bm{x}^{[1]})} \\
		&=& \textcolor{red}{\varphi} \textcolor{blue}{(\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\bm{x}^{[0]}))} - \textcolor{red}{\alpha \nabla f(\bm{x}^{[1]})} \\[0.1cm]
		\bm{x}^{[2]} &\leftarrow& \bm{x}^{[1]} +\textcolor{red}{\varphi} \textcolor{blue}{(\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\bm{x}^{[0]}))} - \textcolor{red}{\alpha \nabla f(\bm{x}^{[1]})} \\[0.1cm]
		\pause
		\bm{\nu}^{[3]} &\leftarrow& \textcolor{green}{\varphi \bm{\nu}^{[2]} - \alpha \nabla f(\bm{x}^{[2]})} \\
		&=& \textcolor{green}{\varphi} (\textcolor{red}{\varphi} \textcolor{blue}{(\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\bm{x}^{[0]}))} - \textcolor{red}{\alpha \nabla f(\bm{x}^{[1]})}) - \textcolor{green}{\alpha \nabla f(\bm{x}^{[2]})} \\[0.1cm]
		\bm{x}^{[3]} &\leftarrow& \bm{x}^{[2]} + \textcolor{green}{\varphi} (\textcolor{red}{\varphi} \textcolor{blue}{(\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\bm{x}^{[0]}))} - \textcolor{red}{\alpha \nabla f(\bm{x}^{[1]})}) - \textcolor{green}{\alpha \nabla f(\bm{x}^{[2]})}  \\
		&=& \bm{x}^{[2]} + \varphi^3\bm{\nu}^{[0]} - \varphi^2\alpha\nabla f(\bm{x}^{[0]}) - \varphi \alpha \nabla f(\bm{x}^{[1]}) - \alpha \nabla f(\bm{x}^{[2]}) \\
		&=& \bm{x}^{[2]} - \alpha(\varphi^2 \nabla f(\bm{x}^{[0]}) + \varphi^1 \nabla f(\bm{x}^{[1]}) + \varphi^0 \nabla f(\bm{x}^{[2]})) + \varphi^3 \bm{\nu}^{[0]} \\
		\pause
		\bm{x}^{[t+1]} &=& \bm{x}^{[t]} - \alpha \displaystyle\sum_{j = 0}^{t} \varphi^j \nabla f(\bm{x}^{[t-j]}) + \varphi^{t+1}\bm{\nu}^{[0]}
	\end{eqnarray*}
	\end{frame}

\begin{vbframe}{Momentum: Example}
	%\framebreak
	Suppose momentum always observes the same gradient $ \nabla f(\bm{x})$:
	\footnotesize 
	\begin{eqnarray*}
		\bm{x}^{[t+1]} &=& \bm{x}^{[t]} - \alpha \displaystyle\sum_{j = 0}^{t} \varphi^j  \nabla f(\bm{x}^{[j]}) + \varphi^{t+1}\bm{\nu}^{[0]} \\
		&=& \bm{x}^{[t]} - \alpha  \nabla f(\bm{x}) \displaystyle\sum_{j = 0}^{t} \varphi^j + \varphi^{t+1}\bm{\nu}^{[0]} \\
		&=& \bm{x}^{[t]} - \alpha  \nabla f(\bm{x}) \frac{1 - \varphi^{t+1}}{1 - \varphi} + \varphi^{t+1} \bm{\nu}^{[0]} \\
		&\to& \bm{x}^{[t]} - \alpha  \nabla f(\bm{x}) \frac{1}{1 - \varphi} \qquad \text{ for } t \to \infty. 
	\end{eqnarray*}
	
	Thus, momentum will accelerate in the direction of $- \nabla f(\bm{x})$ until reaching terminal velocity with step size: 
	$$-\alpha  \nabla f(\bm{x})(1 + \varphi + \varphi^2 + \varphi^3 + ...) = -\alpha  \nabla f(\bm{x}) \frac{1}{1 - \varphi}$$
	E.g. a momentum with $\varphi = 0.9$ corresponds to multiplying the maximum speed by 10 relative to the gradient descent algorithm. 

\framebreak
The vector $\bm{\nu}^{[3]}$ (for $\bm{\nu}^{[0]} = 0$): 
 	\begin{eqnarray*}
 	\bm{\nu}^{[3]} &=& \varphi (\varphi (\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\bm{x}^{[0]})) - \alpha \nabla f(\bm{x}^{[1]})) - \alpha \nabla f(\bm{x}^{[2]}) \\
 	&=& - \varphi^2\alpha\nabla f(\bm{x}^{[0]}) - \varphi \alpha \nabla f(\bm{x}^{[1]}) - \alpha \nabla f(\bm{x}^{[2]})
 	\end{eqnarray*}
 
 \vspace*{-0.2cm}
  \begin{figure}
  \centering
    \includegraphics[width = 0.7\textwidth]{figure_man/momentum_vectors.png} \\
    \begin{footnotesize}
    	If consecutive (negative) gradients point mostly in the same direction, the velocity "builds up". On the other hand, if consecutive (negative) gradients point in very different directions, the velocity "dies down". \\
    	Further geometric intuitions as well as a detailed explanation can be found on the following website:
      \url{https://distill.pub/2017/momentum/}
    \end{footnotesize}
  \end{figure}


\end{vbframe}


\begin{vbframe}{GD with momentum: Zig-zagging behavior}

Consider a 2D quadratic form 
$$
	\fx = \xv^\top \bm{C} \xv = \xv^\top \mat{0.5 & 0  \\ 0 & 10} \xv, \quad \xv^\ast = (0, 0)^\top. 
$$ 

Let $\xv^{[0]} = (10, 1)^\top$, and $\alpha = 0.1$. GD shows stronger zig-zagging behavior than GD with momentum. 
\vspace*{-0.2cm}

\begin{center}
\includegraphics[width = 0.6\textwidth]{figure_man/momentum/compare_gd_momentum.png}
\end{center}

\framebreak
\textbf{Be cautios:} If the momentum is too high, the optimum will probably be missed, rolling past it and back. We might swing back and forth between local optima.
\begin{center}
\includegraphics[width = 0.75\textwidth]{figure_man/momentum/comparison_momentum_overshoot.png}
\end{center}

\end{vbframe}


\begin{vbframe}{GD with momentum: Magnitude vanishing}

Consider the 2D quadratic function $f(\bm{x}) = x_{1}^{2} - x_{2}^{2}$with a saddle point at $(0, 0)^\top$. Let $\bm{x}^{[0]} = (-0.5, 0.001)^\intercal$, and $\alpha = 0.1$.

\vspace*{0.3cm}

 The GD is slowing down at the saddle point (vanishing magnitude of the gradient), while GD with momentum \enquote{breaks out} of the saddle point and converges towards the minimum. 

\begin{center}
\includegraphics[width = 0.6\textwidth]{figure_man/momentum/sgd_momentum_saddlepoint.png}
\end{center}

%\framebreak
%
%\textbf{Comparison of GD and GD with momentum}
%\medskip
%
%  \begin{figure}
%  \centering
%    \includegraphics[height = 5 cm, width = 9 cm]{figure_man/gd_vs_momentum.png}
%  \end{figure}
%  \vspace*{-0.4cm}
%  \begin{footnotesize}
%  \begin{itemize}
%  \item GD (left) requires many steps and terminates in local minimum.
%  \item Momentum (right) terminates in global minimum.
%  \end{itemize}
%  \end{footnotesize}
\end{vbframe}


\begin{vbframe}{Nesterov's accelerated gradient}
	A slightly modified version is Nesterov momentum with stronger theoretical convergence guarantees for convex functions. Here, the gradient is computed at updated position 
	$$
	\bm{\Tilde{x}} =  \bm{x}^{[t]} + \varphi \bm{\nu}^{[t]}
	$$
	\begin{center}
		\includegraphics[width = 0.8\textwidth]{figure_man/nesterov.jpeg}
		\footnotesize{Instead of evaluating the gradient at the current position, with Nesterov momentum we evaluate the gradient at the "looked-ahead" position. \\ Source:https://cs231n.github.io/neural-networks-3/}
	\end{center}
\end{vbframe}

\begin{vbframe}{Momentum vs. Nesterov Momentum}
	\begin{figure}
		\vspace{-0.3cm}
		
		\centering
		\scalebox{0.90}{\includegraphics{figure_man/nesterov_momentum.png}}
		\tiny{\\Source: Chandra (2015) \\}
		\footnotesize{Comparison GD with momentum (left) and GD with Nesterov momentum (right) for one parameter $\theta$. The first three updates of $\theta$ are very similar in both cases and the updates become larger due to momentum (accumulation of previous negative gradients). Update 4 is different. In case of momentum, the update overshoots as it makes an even bigger step due to the gradient history. In contrast, Nesterov momentum first evaluates a "look-ahead" point $\theta_{\text{look\_ahead}}$, detects that it overshoots, and slightly reduces the overall magnitude of the fourth update. Thus, Nesterov momentum reduces overshooting and leads to smaller oscillations than momentum. }
	\end{figure}
\end{vbframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
\end{document}

