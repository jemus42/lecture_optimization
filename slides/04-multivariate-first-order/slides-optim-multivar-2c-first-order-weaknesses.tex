\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/ml_linreg_example_2.pdf}
\newcommand{\learninggoals}{
\item Definition
\item Max. Likelihood 
\item Normal regression
\item Risk Minimization
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{First order methods: \\Weaknesses of GD}
\lecture{Optimization in Machine Learning}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Reminder: Local quadratic geometry}

Every function can be locally approximated by a quadratic function via Taylor approximation: 

\vspace*{-0.3cm}

$$
f(\xv) \approx f(\bm{\tilde{x}}) + \nabla f(\bm{\tilde{x}})^\top(\xv-\bm{\tilde{x}}) +
\frac 12(\xv-\bm{\tilde{x}})^\top\nabla^2 f(\bm{\tilde{x}})(\xv-\bm{\tilde{x}})
$$

\begin{figure}
	\includegraphics[width=0.3\textwidth]{figure_man/taylor_2D_quadratic.png} \\
	\begin{footnotesize} 
	$f$ is shown as the hollow grid and its second-order approximation at $(0, 0)$ as a continuous surface. Source: \url{daniloroccatano.blog}.
	\end{footnotesize}
\end{figure}

\framebreak 



We will therefore look at the Hessian $\bm{H} = \nabla^2 f(\xv^{[t]})$ at a given iteration of gradient descent and discuss weaknesses of GD depending on the local curvature of a function. 

\vspace{0.2cm} 

\textbf{Recall}:
\begin{itemize}
	\item The eigenvector $\textbf{v}_\text{max}$ ($\textbf{v}_\text{min}$) belonging to the largest (smallest) eigenvalue $\lambda_\text{max}$ ($\lambda_\text{min}$) is the direction of max (min) curvature. 
	\item We call the Hessian ill-conditioned if the ratio $\kappa(\Amat) = \frac{|\lambda_\text{max}|}{|\lambda_\text{min}|}$ is high. 
\end{itemize}

	\begin{center}
		\includegraphics[width=0.7\textwidth]{figure_man/ill-con.png}
	\end{center}

\end{vbframe}


\begin{frame} {Effects of curvature}

	Intuitively, the curvature of a function determines the outcome of a GD step\dots
	
	\vspace*{-0.5cm}
	\begin{figure}
		\begin{center}
			\includegraphics[width=.7\textwidth]{figure_man/curvature.png}
		\end{center}
		\tiny{Source: Goodfellow \emph{et al.}, (2016), ch. 4} \\
		\footnotesize{Quadratic objective function $\fx$ with various curvatures.
			The dashed line indicates the first order Taylor approximation. Left: The cost function decreases faster than the gradient predicts; Middle: The gradient predicts the decrease correctly; Right: The function decreases more slowly than expected and begins to increase. }
	\end{figure}
\end{frame}

% \begin{vbframe}{Second derivative and curvature}

% 	To understand better how the curvature of a function influences the outcome of a gradient descent step, let us recall how curvature is described mathematically: 
	
% 	\begin{itemize}
% 		\item The second derivative corresponds to the curvature of the graph of a function. 
% 		\item The \textbf{Hessian} matrix of a function $\riskt: \R^m \to \R$ is the matrix of second-order partial derivatives
% 		$$
% 		H_{ij} = \frac{\partial^2}{\partial \theta_i \partial \theta_j} \riskt.
% 		$$
% 	\end{itemize}

% \framebreak 

% 	\begin{itemize}
% 		\item The second derivative in a direction $\mathbf{d}$, %of length $1$
% 		with $\|\mathbf{d}\| = 1$, is given by $\mathbf{d}^\top \!\Hess\,\mathbf{d}$.
% 		\item What is the direction of the highest curvature (red direction), and what is the direction of the lowest curvature (blue)?
% 	\end{itemize}
	
% 	\vspace*{-0.5cm}
	
% 	\begin{figure}
% 		\begin{center}
% 			\includegraphics{figure_man/curvature2.png}
% 		\end{center}
% 	\end{figure}

% \framebreak

% 	\begin{itemize}
% 		\item Since $\Hess$ is real and symmetric, eigendecomposition yields
% 		$\Hess = \mathbf{V} \mathbf{\diag(\boldsymbol{\lambda})} \mathbf{V}^{-1}$
% 		with $\mathbf{V}$ and $\boldsymbol{\lambda}$ collecting eigenvectors and eigenvalues, respectively.
% 		\item It can be shown, that the eigenvector $\textcolor{red}{\bm{v}_{\text{max}}}$ with the max. eigenvalue $\lambda_{\text{max}}$ points into the direction of highest curvature ($\bm{v}_{\text{max}}^\top \bm{H} \bm{v}_{\text{max}} = \lambda_{\text{max}}$), while the eigenvector $\textcolor{blue}{\bm{v}_{\text{min}}}$ with the min. eigenvalue $\lambda_{\text{min}}$ points into the direction of least curvature. 
% 	\end{itemize}
	
% 	\vspace*{-0.5cm}
	
% 	\begin{figure}
% 		\begin{center}
% 			\includegraphics[width=.7\textwidth]{figure_man/curvature2.png}
% 		\end{center}
% 	\end{figure}

% \framebreak 

% 	\begin{itemize}
% 		\item At a stationary point $\thetab$, where the gradient is 0, we can examine the eigenvalues of the Hessian to determine whether the $\thetab$ is a local maximum, minimum or saddle point: 
% 		\vspace{-0.3cm}
% 		\begin{align*} 
% 		\quad\forall i: \lambda_i > 0  \, (\Hess \text{ positive definite at $\thetab$}) &\quad\Rightarrow\quad \text{minimum at $\thetab$} \\
% 		\quad\forall i: \lambda_i < 0 \, (\Hess \text{ negative definite at $\thetab$}) &\quad\Rightarrow\quad \text{maximum at $\thetab$}\\
% 		\exists\, i: \lambda_i < 0 \land  \exists i: \lambda_i > 0  \,(\Hess \text{ indefinit at $\thetab$}) &\quad\Rightarrow\quad \mbox{saddle point at $\thetab$}
% 		\end{align*}
% 	\end{itemize}
% 	\begin{figure}
% 		\begin{center}
% 			\includegraphics[width=.7\textwidth]{figure_man/3dim_curvature.png}
% 		\end{center}
% 		\tiny{Source: Rong Ge (2016)}
% 	\end{figure}

% \end{vbframe}

\begin{vbframe}{Curvature and Step-size in GD}
	
	In the worst case, the Hessian is ill-conditioned. What does this mean for GD?

	\begin{itemize}
		\item Let us consider the second-order Taylor approximation of $f(\xv)$ around $\bm{\tilde{x}}$ (with gradient $\mathbf{g}$)
		$$
		f(\xv) \approx f(\bm{\tilde{x}}) + (\xv-\bm{\tilde{x}})^\top \bm{g} +
		\frac 12(\xv-\bm{\tilde{x}})^\top \bm{H}(\xv-\bm{\tilde{x}})
		$$
		\item One GD step with a learning rate $\alpha$ yields new parameters $\bm{\tilde{x}}-\alpha \mathbf{g}$ and a new approximated function value
		$$
		f(\bm{\tilde{x}}-\alpha \mathbf{g}) \approx f(\bm{\tilde{x}}) - \alpha \mathbf{g}^\top\mathbf{g} + \frac{1}{2}	\alpha^2 \mathbf{g}^\top\!\Hess\,\mathbf{g}  \,.
		$$ 

		\item Theoretically, if $\mathbf{g}^\top \Hess \mathbf{g}$ is positive, we can solve the equation above for the optimal step size which corresponds to 
		$$
		\alpha^* = \frac{\mathbf{g}^\top \mathbf{g}}{\mathbf{g}^\top \!\Hess\, \mathbf{g}}  \,.
		$$

		\framebreak 
		
		\item Let us assume the gradient $\mathbf{g}$ points into the direction of $\bm{v}_{\text{max}}$ (i.e. the direction of highest curvature), the optimal step size is given by 
		$$
		\alpha^* = \frac{\mathbf{g}^\top \mathbf{g}}{\mathbf{g}^\top \!\Hess\, \mathbf{g}} = \frac{\mathbf{g}^\top \mathbf{g}}{\lambda_{\text{max}} \mathbf{g}^\top   \mathbf{g}} = \frac{1}{\lambda_{\text{max}}}, 
		$$ 
		which is very small. Choosing a too large step-size is bad, as it will make us \enquote{overshoot} the stationary point.
		\item If, on the other hand, $\mathbf{g}$ points into the direction of the lowest curvature, the optimal step size is 
		$$
		\alpha^* = \frac{1}{\lambda_{\text{min}}}, 
		$$
		which corresponds to the largest possible optimal step-size.
		\item We summarize: We want to perform big steps in directions of low curvature, but small steps in directions of high curvature.

		
		\framebreak 
		
		\item But what if the gradient does not point into the direction of one of the eigenvectors? 
		\item Let us consider the 2-dimensional case: We can decompose the direction of $\bm{g}$ (black) into the two eigenvectors $\textcolor{red}{\bm{v}_{\text{max}}}$ and $\textcolor{blue}{\bm{v}_{\text{min}}}$
		\item It would be optimal to perform a \textbf{big} step into the direction of the smallest curvature $\textcolor{blue}{\bm{v}_{\text{min}}}$, but a \textbf{small} step into the direction of $\textcolor{red}{\bm{v}_{\text{max}}}$, but the gradient points into a completely different direction. 
	\end{itemize}
	
	\vspace*{-0.4cm}
	
	\begin{figure}
		\begin{center}
			\includegraphics[width=.7\textwidth]{figure_man/curvature3.png}
		\end{center}
	\end{figure}
	
\framebreak 

\begin{itemize}
%   \item The new risk after a \textbf{gradient descent} step is 
%   $$
%   \risk(\boldsymbol{\theta}^0-\alpha \mathbf{g}) \approx \risk(\boldsymbol{\theta}^0) \underbrace{- \alpha \mathbf{g}^\top\mathbf{g} + \frac{1}{2}	\alpha^2 \mathbf{g}^\top\!\Hess\,\mathbf{g}}_{:=\nu}  \,.
%   $$ 
% The term  $\nu$ is added to the risk $\risk$ in each gradient descent step. 
%   \item Ill-conditioning of the Hessian matrix $\Hess$ becomes a problem, when $$\frac{1}{2}	\alpha^2 \mathbf{g}^\top\!\Hess\,\mathbf{g}^\top > \alpha \mathbf{g}^\top\mathbf{g}$$
%   \item Ill-conditioning occurrs, if the second derivatives for a specific point differ a lot. This can be measured by the condition number. A condition number of 5, for example, means that the direction of the highest curvature has five times more curvature than the direction of the least  curvature. 

% \framebreak 
\item GD is unaware of large differences in curvature, and can only walk into the direction of the gradient.  
\item Choosing a too large step-size will then cause the descent direction change frequently (\enquote{jumping around}).
\item $\alpha$ needs to be small enough, which results in a low progress.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.65\textwidth]{figure_man/big_small_stepsize.png}
	\end{center}
	\tiny{The contour lines show a quadratic risk function with a poorly conditioned Hessian matrix. The plot shows the progress of gradient descent with a small step-size vs. larger step-size. In both cases, convergence to the global optimum is rather slow. \par}
\end{figure}

\framebreak
\item In the worst case, ill-conditioning of the Hessian matrix and a too big step-size will cause the risk to increase

$$
	f(\bm{\tilde{x}}-\alpha \mathbf{g}) \approx f(\bm{\tilde{x}}) - \alpha \mathbf{g}^\top\mathbf{g} + \frac{1}{2}	\alpha^2 \mathbf{g}^\top\!\Hess\,\mathbf{g}  \,.
$$ 

which happens if 

$$
\frac{1}{2} \alpha^2 \mathbf{g}^\top\!\Hess\,\mathbf{g} > \alpha \mathbf{g}^\top\mathbf{g}.
$$
\item To determine whether ill-conditioning is detrimental to the training, the squared gradient norm $\mathbf{g}^\top \mathbf{g}$ and the risk %and the term $\mathbf{g}^\top\Hess\mathbf{g}$ 
can be monitored.

\begin{figure}
	\centering
	\scalebox{0.8}{\includegraphics{figure_man/no_critical.png}}
	\tiny{\\Source: Goodfellow, ch. 6}
\end{figure}

\vspace*{-0.1cm}

\item Gradient norms \textbf{increase} over time, showing that the training process is not converging to a stationary point $\mathbf{g} = 0$. 
\item At the same time, we observe that the risk is approx. constant, but the gradient norm increases
\vspace*{-0.2cm}
$$
\underbrace{f(\bm{\tilde{x}}-\alpha \mathbf{g})}_{\text{approx. constant}} \approx f(\bm{\tilde{x}}) - \underbrace{\alpha \mathbf{g}^\top\mathbf{g}}_{\text{increase}}+ \frac{1}{2}  \underbrace{\alpha^2 \mathbf{g}^\top\!\Hess\,\mathbf{g}}_{\to \text{increase}}  \,. 
$$ 
\end{itemize}
\end{vbframe}


% \section{Ill-Conditioning}
% \begin{vbframe}{Ill-conditioned Hessian matrix} 
	
	
% 	The condition number of a symmetric matrix $\bm{A}$ is given by the ratio of its min/max eigenvalues $\kappa(\bm{A}) = \frac{|\lambda_{\text{max}}|}{|\lambda_{\text{min}}|}$. A matrix is called ill-conditioned, if the condition number $\kappa(\bm{A})$ is very high. 
	
% 	\lz 
	
% 	An \textbf{ill-conditioned} Hessian matrix means that the ratio of max. / min. curvature is high, as in the example below: 
	
% 	\begin{center}
% 		\includegraphics[width=0.7\textwidth]{figure_man/ill-con.png}
% 	\end{center}
	
	
% \end{vbframe}



%\begin{vbframe}{Ill-conditioned problems}
%
%For ill-conditioned problems, the gradient moves with a zig-zag course to the optimum, since the gradient points approximately orthogonal in the shortest direction to the minimum.
%\lz
%
%Let
%
%$$
%f(\xv) = \xv^\intercal \mathbf{C} \xv
%$$
%
%be a 2-dimensional quadratic of the form with $\mathbf{C} = \mat{0.5 & 0  \\ 0 & 10}$ and a global optimum in the origin.\\
%\vspace*{0.1cm}
%Let $\bm{x}^{[0]} = (10, 1)^\intercal$ be the initialization and $\alpha = 0.1$ then the gradient descent steps are showing a zig-zagging behavior in the contour plot for the first 10 iterations.
%
%\framebreak
%\begin{itemize}
%\item Negative gradient always points perpendicular to the contour of a function which may cause a rapid change in negative gradient direction in each optimization step and hence leads to the zig-zagging behavior shown in the figure
%\item Slows down optimization process and hence requires many steps for convergence 
%\end{itemize}
%\vspace*{-0.2cm}
%\begin{center}
%\includegraphics[width = 0.6\textwidth]{figure_man/momentum/sgd_without_momentum.png}
%\end{center}

%\framebreak
%\textbf{Slow crawling:} may vanish rapidly close to stationary points (e.g. saddle points) and hence also slows down progress at these points
%\textbf{Example for slow crawling and getting trapped in saddle point}\\
%\lz
%
%Let
%$$
%f(\xv) = x_{1}^{2} - x_{2}^{2}
%$$
%
%be a 2-dimensional quadratic with a saddle point in the origin.\\
%\vspace*{0.3cm}
%Let $\bm{x}^{[0]} = (-0.5, 0.001)^\intercal$ be the initialization and $\alpha = 0.1$ then the gradient descent steps are slowing down at the saddle point with vanishing magnitude of the gradient.
%
%\framebreak
%
%\begin{center}
%\includegraphics[width = 0.68\textwidth, height = 0.5\textheight]{figure_man/momentum/sgd_saddlepoint.png}
%\end{center}
%\vspace*{-0.2cm}
%\begin{itemize}
%\item Slow crawling: a gradient descent step is proportional to the magnitude of the gradient $\rightarrow$ SGD starts with large steps and slows down near minimum or optimum
%\item For some functions (like here) this behavior can also cause that gradient descent gets stuck near saddle points.
%\end{itemize}
%

%\end{vbframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






% \begin{frame} {Effects of curvature}
% \begin{itemize}
% \item The second-order Taylor approximation (with gradient $\mathbf{g}$) around current point $\boldsymbol{\theta}^0$ is
% \begin{equation*}
% \risk(\boldsymbol{\theta}) = \risk(\boldsymbol{\theta}^0) + (\boldsymbol{\theta}-\boldsymbol{\theta}^0)^\top \mathbf{g} + \frac{1}{2}(\boldsymbol{\theta}-\boldsymbol{\theta}^0)^\top \!\Hess\, (\boldsymbol{\theta}-\boldsymbol{\theta}^0)
% \end{equation*}
% \item SGD with learning rate $\alpha$ yields new parameters $\boldsymbol{\theta}^0-\alpha g$ and new loss value
% $$
% \risk(\boldsymbol{\theta}^0-\alpha \mathbf{g}) = \risk(\boldsymbol{\theta}^0) - \alpha \mathbf{g}^\top\mathbf{g} + \frac{1}{2}	\alpha^2 \mathbf{g}^\top\!\Hess\,\mathbf{g}  \,.
% $$
% \item When $g^\top H g$ is to large, we might get $\risk(\boldsymbol{\theta}^0-\alpha \mathbf{g}) > \risk(\boldsymbol{\theta}^0)$.
% \item If $\mathbf{g}^\top \Hess \mathbf{g}$ is positive, optimal step size is
% $$
% \alpha^* = \frac{\mathbf{g}^\top \mathbf{g}}{\mathbf{g}^\top \!\Hess\, \mathbf{g}} \ge \frac{1}{\lambda_{\text{max}}}  \,.
% $$
% \end{itemize}
% \end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{vbframe}{GD at saddle points}
	
	\begin{columns}
		\begin{column}{0.48\textwidth}
			\textbf{Example: }
			\begin{center}
				$f(x_1, x_2) = x_1^2 - x_2^2$
			\end{center}
			Along $x_1$, the function curves upwards (eigenvector of the Hessian with positive eigenvalue). Along $x_2$, the function curves downwards (eigenvector of the Hessian with negative eigenvalue).
			
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=4cm]{figure_man/saddlepoint.png}
			\end{figure} 
		\end{column}	
	\end{columns}
	%\item Second-order algorithms experience even greater problems when dealing with saddle points. Newtons method for example actively searches for a region with zero gradient. That might be another reason why second-order methods have not succeeded in replacing gradient descent for neural network training. 
	
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	
	\frametitle{Example: Saddle point with GD}
	\begin{itemize}
		\item So how do saddle points impair optimization?
		\item First-order algorithms that use only gradient information \textbf{might} get stuck in saddle points.
	\end{itemize}
	\center
	\only<1>{\includegraphics[width=9cm]{figure_man/opt1.png}}%
	\only<2>{\includegraphics[width=9cm]{figure_man/opt2.png}}%
	\only<3>{\includegraphics[width=9cm]{figure_man/opt3.png}}%
	\only<4>{\includegraphics[width=9cm]{figure_man/opt10.png}}%
	
	\begin{itemize}
		
		\only<1>{\item[] \small{Red dot: Starting location}}
		\only<2>{\item[] \small{First step...}}
		\only<3>{\item[] \small{...second step...}}
		\only<4>{\item[] \small{...tenth step got stuck and cannot escape the saddle point!}}
		
	\end{itemize}
	
}

\begin{vbframe}{Unimodal vs. Multimodal loss surfaces}
\begin{figure}
\centering
\includegraphics[width=12cm]{figure_man/difficult_vs_easy.png}
\footnotesize{Left: Multimodal loss surface with saddle points; Right: (Nearly) unimodal loss surface (Hao Li et al. (2017))}
\centering
\vspace{1cm}
\includegraphics[width=.9\textwidth]{figure_man/multimodal.png}
\footnotesize{Potential snippet from a loss surface with many local minima}
\end{figure}

\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Only locally optimal moves}
\begin{itemize}
\small{
\item If the training algorithm makes only \textit{locally} optimal moves (as in gradient descent), it may move away from regions of \textit{much} lower cost.
\begin{figure}
	\centering
	\scalebox{0.65}{\includegraphics{figure_man/local_hill.png}}
	\tiny{\\Source: Goodfellow, Ch. 8}
\end{figure}
\item In the figure above, initializing the parameter on the ``wrong'' side of the hill will result in suboptimal performance.
\item In higher dimensions, however, it may be possible for gradient descent to go around the hill but such a trajectory might be very long and result in excessive training time.}
\end{itemize}
\end{frame}

\begin{vbframe} {Local minima}

\begin{itemize}
\item In practice only local minima with a high value compared to the global minimium are problematic.
\begin{figure}
\begin{center}
	\includegraphics[width=.6\textwidth]{figure_man/minima.png}
\end{center}
\tiny{Source: Goodfellow, Ch. 4}
\end{figure}
\item In DL, literature suspects that most local minima have low empirical risk. (\href{https://arxiv.org/abs/1406.2572}{Y. Dauphin et al. (2014)})
\item Simple test: Norm of gradient should get close to zero.
\end{itemize}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Cliffs and Exploding Gradients}
%\begin{vbframe}{Cliffs and exploding gradients}
%\begin{itemize}
%\item As a result from the multiplication of several parameters, the emprirical risk for highly nonlinear deep neural networks often contain sharp nonlinearities.
%\begin{itemize}
%\item That may result in very high derivatives in some places.
%\item As the parameters get close to such cliff regions, a gradient descent update can catapult the parameters very far.
%\item Such an occurrence can lead to losing most of the optimization work that had been done.
%\end{itemize}
%\item However, serious consequences can be easily avoided using a technique called \textbf{gradient
%clipping}.
%\item The gradient does not specify the optimal step size, but only the optimal direction
%within an infinitesimal region.
%\framebreak 
%\item Gradient clipping simply caps the step size to be small enough that it is less likely to go outside the region where the gradient indicates the direction of steepest descent.
%\item We simply \enquote{prune} the norm of the gradient at some threshold $h$:
%$$\text{if  } ||\nabla \thetab|| > \text h: \nabla \thetab \leftarrow \frac{h}{||\nabla \thetab||} \nabla \thetab $$
%\end{itemize}
%\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{vbframe}{Example: cliffs and exploding gradients}
%\begin{figure}
%\centering
%\includegraphics[width=8cm]{figure_man/cliff2.png}
%\caption{\enquote{The objective function for highly nonlinear deep neural networks or for
%	recurrent neural networks often contains sharp nonlinearities in parameter space resulting
%	from the multiplication of several parameters. These nonlinearities give rise to very
%	high derivatives in some places. When the parameters get close to such a cliff region, a
%	gradient descent update can catapult the parameters very far, possibly losing most of the
%	optimization work that had been done} (Goodfellow et al. (2016)).}
%\end{figure}
%\end{vbframe}

\endlecture
\end{document}

