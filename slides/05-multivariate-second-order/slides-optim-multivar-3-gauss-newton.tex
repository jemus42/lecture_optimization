\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/squares.png}
\newcommand{\learninggoals}{
\item Least squares
\item Gauss-Newton
\item Levenberg-Marquardt
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Second order methods: Gauss-Newton}
\lecture{Optimization in Machine Learning}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Least squares problem}


Consider the problem of minimizing a sum of squares

\begin{eqnarray*}
	\min_{\bm{\theta}} &&g(\thetab)  \\ \text{ with } && g(\thetab) = \|r(\bm{\theta})\|_2^2 = \sum_{i = 1}^n \left[r_i(\bm{\theta})\right]^2 = r(\thetab)^\top r(\thetab). 
\end{eqnarray*}

$r$: map $\thetab$ to residuals

\begin{eqnarray*}
	r: \R^d &\to& \R^n, \\
	\thetab &\mapsto& r(\thetab) = \begin{pmatrix} r_1(\thetab) \\ ... \\ r_n(\thetab)\end{pmatrix}
\end{eqnarray*}

\framebreak 

\textbf{Risk minimization with squared loss} $\Lxy = \left(y - \fx\right)^2$

$$
	\risket = \sumin \Lxyit = \sumin \underbrace{\left(\yi - \fxit \right)^2}_{[r_i(\thetab)]^2} % = \sumin\left[r_i(\bm{\theta})\right]^2
$$
also known as least squares regression is a least squares problem. $\fxit$ might be a nonlinear function. The $r_i$ are commonly referred to as residuals.

\vspace*{0.3cm} 

\begin{columns}
\begin{column}{0.55\textwidth}
\textbf{Example:} 
%Consider, for example, a regression problem with data

\begin{footnotesize}
\begin{eqnarray*}
\D &=& \left(\left(\xi, \yi\right)\right)_{i = 1, ..., 5} \\ &=& \left((1,3),(2,7),(4,12),(5,13),(7,20)\right)
\end{eqnarray*}
\end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
	\vspace*{-0.5cm}  
    \begin{center}
     \includegraphics[width=1\textwidth]{figure_man/squares.png}
     \end{center}
\end{column}
\end{columns}


% <<echo = F, out.width = '50%', fig.align='center'>>=
% x = c(1, 2, 4, 5, 8)
% y = c(3, 5, 6, 13, 20)
% mod = nls(y ~ a*exp(b*x), start = list(a = 1, b = 0.2))

% d = data.frame(x = x, y = y, pred = predict(mod))

% plot = ggplot(data = d, aes(x = x, y = y)) + geom_point()
% plot = plot + geom_smooth(method = "nls", formula = y ~ a*exp(b*x),method.args = list(start = c(a = 1, b = 0.2)), se = FALSE, color = "black" )
% plot = plot + geom_point(aes(x = x, y = pred))
% plot = plot + geom_segment(aes(x = x, y = y, xend = x, yend = pred), color = "red")
% plot = plot + theme_bw()
% plot
% @


% Allgemein ist die Modellfunktion beschrieben durch:
% $$
% \bm{y} = f(\bm{x_1},...,\bm{x_n})
% $$
% und hat $p \leq k$ Parameter $\bm{\theta} = (\theta_1,...,\theta_p)$ die nun so bestimmt werden sollen dass die Modellfunktion die tatsächlichen Werte $\bm{y}$ möglichst gut beschreibt.

\framebreak

Suppose we suspect an exponential relationship between $x$ and $y$ 
$$
\fxt = \theta_1 \cdot \exp(\theta_2 \cdot x), \quad \theta_1, \theta_2 \in \R.
$$

% Die Residuen lauten also:
% $$
% r^{(i)}(\bm{\theta}) := y^{(i)} - f(x^{(i)}) = y^{(i)} - \theta_1 exp(\theta_2 x^{(i)})
% $$
% \framebreak

% Sei $|| \cdot ||$ die euklidische Norm. Da bei der KQ-Methode der quadrierte vertikale Abstand zwischen beobachtung $y^{(i)}$ und der Modellfunktion $f(x)$ minimiert wird, können wir das Optimierungsproblem schreiben als:
% \footnotesize
% $$
% \min_{\bm{\theta} \in \R^n} \; g(\bm{\theta}) = \min_{\bm{\theta} \in \R^n} \; \frac{1}{2} ||r(\bm{\theta})||^2 = \min_{\bm{\theta} \in \R^n} \; \frac{1}{2} \sum_{i=1}^{n} (r^{(i)})^2 (\bm{\theta}) =  \min_{\bm{\theta} \in \R^n} \; \frac{1}{2} r(\bm{\theta})^{\top}r(\bm{\theta})
% $$

% Dieses Optimierungsproblem möchten wir nun mit Hilfe des Newton Verfahrens lösen. Hierfür beginnen wir mit der Berechnung der Jakobi- und Hessematrix.


Residuals:
\begin{footnotesize}
$$
r(\bm{\theta}) = \mat{\theta_1 exp(\theta_2 x^{(1)}) - y^{(1)} \\\theta_1 exp(\theta_2 x^{(2)}) - y^{(2)}\\ \theta_1 exp(\theta_2 x^{(3)}) - y^{(3)} \\ \theta_1 exp(\theta_2 x^{(4)}) - y^{(4)} \\ \theta_1 exp(\theta_2 x^{(5)}) - y^{(5)} } = \mat{
\theta_1 exp(1 \theta_2) - 3 \\\theta_1 exp(2 \theta_2) - 7\\ \theta_1 exp(4 \theta_2) - 12 \\ \theta_1 exp(5 \theta_2) - 13 \\ \theta_1 exp(7 \theta_2) - 20
}.
$$
\end{footnotesize}

LS problem:

$$
g(\thetab) =  r(\thetab)^\top r(\thetab) = \sum_{i=1}^{5} \left(\yi - \theta_1 \exp\left(\theta_2 x^{(i)}\right)\right)^2.
$$



\end{vbframe}

\begin{vbframe}{Newton-Raphson Idea}


\textbf{Approach:} Calculate NR update direction by solving:

$$
\nabla^2 g(\bm{\theta}^{[t]}) \bm{d}^{[t]} = - \nabla g(\thetab^{[t]}).
$$

The gradient is calculated by applying the chain rule

$$
	\nabla_\theta g(\thetab) = \nabla_\theta \left[r(\thetab)^\top r(\thetab)\right] = 2 \cdot  \nabla r(\thetab)^\top r(\thetab)
$$

with $\nabla r(\bm{\theta})$ the Jacobian matrix of $r(\cdot)$.

\lz

In our example

\begin{footnotesize}
$$
\nabla r(\thetab) = \mat{\frac{\partial r_1(\theta)}{\partial \theta_1} & \frac{\partial r_1(\theta)}{\partial \theta_2}\\
\frac{\partial r_2(\theta)}{\partial \theta_1} & \frac{\partial r_2(\theta)}{\partial \theta_2}\\
\vdots & \vdots \\
\frac{\partial r_5(\theta)}{\partial \theta_1} & \frac{\partial r_5(\theta)}{\partial \theta_2}} 
= \mat{exp(\theta_2 x^{(1)}) & x^{(1)} \theta_ 1 exp(\theta_2 x^{(1)}) \\ exp(\theta_2 x^{(2)}) & x^{(2)} \theta_ 1 exp(\theta_2 x^{(2)})\\ exp(\theta_2 x^{(3)}) & x^{(3)} \theta_1 exp(\theta_2 x^{(3)}) \\ exp(\theta_2 x^{(4)}) & x^{(4)} \theta_1 exp(\theta_2 x^{(4)}) \\ exp(\theta_2 x^{(5)}) & x^{(5)} \theta_1 exp(\theta_2 x^{(5)})} 
% = \mat{exp(1 \theta_2) & 1 \theta_1 exp(1 \theta_2 ) \\ exp(2 \theta_2) & 2 \theta_1 exp(2 \theta_2 )\\ exp(4 \theta_2) & 4 \theta_1 exp(4 \theta_2) \\ exp(5 \theta_2) & 5 \theta_1 exp(5 \theta_2) \\ exp(7 \theta_2) & 7 \theta_1 exp(7 \theta_2)}
$$
\end{footnotesize}

\framebreak 

Hessian is obtained by applying product rule and has elements

\begin{eqnarray*}
	H_{jk} &=& 2 \sumin \left(\frac{\partial r_i}{\partial \thetab_j}\frac{\partial r_i}{\partial \thetab_k} + r_i \frac{\partial^2 r_i}{\partial \thetab_j \partial \thetab_k}\right)
\end{eqnarray*}

% \begin{eqnarray*}
% \nabla^2_\theta \left(r(\thetab)^\top r(\thetab)\right) &=& \nabla_\theta \left(\nabla_\theta \left(r(\thetab)^\top(\thetab)\right)\right) = \nabla_\theta \left[2 \cdot  \nabla r(\thetab)^\top r(\thetab)\right] \\ 
% &=& 2 \nabla_\theta r(\thetab)^\top \nabla_\theta r(\thetab) + 
% \end{eqnarray*}


% The Hessian matrix $\nabla^{2} f(\bm{\theta})$ is obtained by applying the chain rule:

% % muss eine p x p - Matrix sein
% % r ist ein n x p Vektor
% % nabla r_i ist p x 1
% \begin{align*}
% \nabla^{2} \left(\frac{1}{2}\cdot r(\bm{\theta})^\top r(\bm{\theta}) \right) &= \nabla r(\bm{\theta})^\top \nabla r(\bm{\theta}) + \sum_{i=1}^{n} r^{(i)}(\bm{\theta}) (\nabla^2)^{(i)}(\bm{\theta})
% % &= J(\bm{\theta})^\top J(\bm{\theta}) + W(\bm{\theta})
% \end{align*}

\textbf{Problem with NR:} 2nd derivatives can be challenging to compute! 

\end{vbframe}

\begin{vbframe}{Gauss Newton for least squares}

GN approximates H by dropping its second part:

\begin{eqnarray*}
H_{jk} &=& 2 \sumin \left(\frac{\partial r_i}{\partial \thetab_j}\frac{\partial r_i}{\partial \thetab_k} + r_i \frac{\partial^2 r_i}{\partial \thetab_j \partial \thetab_k}\right) \\
&\approx&  2 \sumin \left(\frac{\partial r_i}{\partial \thetab_j}\frac{\partial r_i}{\partial \thetab_k}\right) = 2 \nabla r^\top \nabla r.  
\end{eqnarray*}

assuming for all $i$ that 
$$
	 \left|\frac{\partial r_i}{\partial \thetab_j}\frac{\partial r_i}{\partial \thetab_k}\right| \gg \left|r_i \frac{\partial^2 r_i}{\partial \thetab_j \partial \thetab_k}\right|.
$$

This assumption may be valid if: 

\begin{itemize}
	\item Residuals $r_i$ are small in magnitude
	\item Functions are only \enquote{mildly} nonlinear and $\frac{\partial^2 r_i}{\partial \thetab_j \partial \thetab_k}$ is small. 
\end{itemize}

\framebreak 

If $\nabla r(\thetab)^\top \nabla r(\thetab)$ is invertible, the Gauss-Newton update direction is 

\begin{eqnarray*}
\bm{d}^{[t]} &=& - \left[\nabla^2 g(\bm{\theta}^{[t]})\right]^{-1} \nabla g(\thetab^{[t]}) \\
&=& - \left[\nabla r(\thetab)^\top \nabla r(\thetab)\right]^{-1} \nabla r(\thetab)^\top r(\thetab),
\end{eqnarray*}

\lz

\textbf{Advantage}: Reduced computational complexity because Hessian does not have to be computed. 


% \textbf{Solution:} Gauss-Newton algorithm
% \begin{itemize}
% \item If residuals $r^{(i)}(\bm{\theta})$ small, approximation of the Hessian matrix:
% \begin{align*}
% \nabla^{2} f(\bm{\theta}) &= \nabla r(\bm{\theta})^\top \nabla r(\bm{\theta}) + \underbrace{\sum_{i=1}^{n} r^{(i)}(\bm{\theta}) \nabla^{2}r^{(i)}(\bm{\theta})}_{\approx 0} \approx \nabla r(\bm{\theta})^{\top} \nabla r(\bm{\theta})
% \end{align*}

% \item Hessian matrix is therefore not explicitly calculated \\ $\rightarrow$ less effort than Newton's method.
% % \item Quadratische Konvergenz bei Startwert nahe des Optimums
% % \item Algorithmus iteriert durch die Parameter $\bm{\theta}$.
% \end{itemize}

% \textbf{Prerequisites for Gauss Newton:}
% \begin{itemize}
% \item Residuals must be sufficiently small so that approximation is not too bad
% % \item initial value of the parameters $\bm{\theta}$ must be sufficiently close to the optimal solution (otherwise procedure does not converge)
% \item Full Rank of the Jacobian matrix $\nabla r(\bm{\theta})$ (so that solution of the LES possible)
% \end{itemize}

% \framebreak

% Instead of using the exact Newton-Raphson search direction

% \begin{eqnarray*}
% \nabla^2 f(\bm{\theta}^{[t]}) \bm{d}^{[t]} &=& - \nabla f(\bm{x}^{[t]}),
% \end{eqnarray*}

% we are using a simplified Newton-Raphson search direction:

% \begin{eqnarray*}
% \nabla r(\bm{\theta}^{[t]})^\top\nabla r(\bm{\theta}^{[t]}) \bm{d}^{[t]} &=& - \nabla  r(\bm{\theta}^{[t]})^\top r(\bm{\theta}^{[t]})
% \end{eqnarray*}

% Dieses Gleichungssystem ist widerum ein überbestimmtes Gleichungssystem, das wir durch die analytische Lösung des Optimierungsproblems

% \begin{eqnarray*}
% \text{arg } \min_{\bm{d}_i} \frac{1}{2} \|\nabla r(\bm{\theta}_i) \bm{d}_i + r(\bm{\theta}_i)\| \\
% \bm{d}_i = \left(\nabla r(\bm{\theta}_i)^\top \nabla r(\bm{\theta}_i)\right)^{-1} \nabla r(\bm{\theta}_i)^\top  \left(r(\bm{\theta}_i\right)
% \end{eqnarray*}

% ersetzen.

% \lz


% $$
% \nabla^{2} g(\bm{\theta}) = J(\bm{\theta})^\top J(\bm{\theta})
% $$

% Die Gauss-Newton-Suchrichtung $\bm{d}_i$ lautet somit

% $$
% \nabla^{2} g(\bm{\theta}) d_{G} = -\nabla g(\bm{\theta})
% $$
% $$
% J(\bm{\theta})^{\top} J(\bm{\theta}) d_{GN} = J(\bm{\theta})^{\top} r(\bm{\theta})
% $$
% \medskip

% Die Iterationsvorschrift des Gauß-Newton Verfahrens lautet:
% $$
% \bm{\theta}_{i+1} = \bm{\theta}_{k} + \bm{d}_{i}
% $$
% wobei $\bm{\delta}_k$ die Lösung des folgenden Optimierungsproblems in der $k$-ten Iterations ist:
% $$
% \min_{d} \; \frac{1}{2} ||J_k d + r_k||^2 = \min_{d} \; \frac{1}{2} ||J_k d -(-r(\bm{\theta}))||^2
% $$
% Die Lösung ist dann durch die Normalengleichung bestimmt:
% $$
% J_{k}^\top J_{k} d = -J_{k}^\top r_k \Leftrightarrow d = (J_{k}^\top J_{k})^{-1} J_{k}^\top(-r_k)
% $$
% \medskip

% Das Minimum entspricht also der Gauß-Newton-Suchrichtung $d_{GN} = d $
% \lz

% \textbf{Note:} In \texttt{R} LS estimators for non-linear relationships can be determined using the \texttt{nls()} function. This function uses the Gauss Newton algorithm by default.

\end{vbframe}
%
% \framebreak
%
% \normalsize
% \textbf{Allgemein: }
%
%
%
% \begin{itemize}
% \item Sei $r$ Funktion eines n-dimensionalen Vektors
%
% \vspace*{-0.2cm}
%
% $$
%   g: \; \R^p \rightarrow \R^n.
% $$
%
% In vorherigem Beispiel bildet die Funktion $g$ den $p$-dimensionalen Parametervektor $\bm{\theta}$ auf die $n$ Residuen ab.
%
% \item \textbf{Ziel}: $g(x)$ so nah wie möglich an Null, z.B. bezüglich quadratischem Abstand
%
% \vspace*{-0.2cm}
% $$
% \min_{x\in\R^p} f(x) = \min_{x\in\R^p} \; \frac{1}{2} ||g(x)||_2^2 = \min_{x\in\R^p} \; \frac{1}{2} \sum_{i=1}^{n} g_i^2(x).
% $$
% \item Kann als elementweise Penalty der Werte von $g(x)$ angesehen werden (auch andere Penalty vorstellbar, z.B. L1-Norm $\min_{x\in\R^p}\; \sum_{i=1}^n |g_i(x)|$).
% \end{itemize}
%
% <<echo=FALSE, fig.align='center'>>=
% int = seq(-2,2,l=300)
% plot(int,  int^2/2, type = "l", xlab = expression(g[i](x)), ylab = "Penalty")
% points(int, abs(int), type="l")
% @
% %
% % \framebreak
% %
% % \begin{itemize}
% % \item Allgemein lässt sich Penalty darstellen als:
% % $$
% % f(x)=\sum_{i=1}^{m} \phi(g_i(x))
% % $$
% % % \lz
% % % \item Dadurch auch auf z.B. Constrained Optimierung übertragbar
% % \end{itemize}
%
% \framebreak
%
% Allgemein lässt sich Penalty darstellen als:
%
% \lz
% $$
% f(x)=\sum_{i=1}^{n} \phi(g_i(x))
% $$
% \lz
%
% \textbf{Ableitungen von $f(x)$}:
% \begin{itemize}
% \lz
% \item $\nabla f(x) = \sum_{i} \phi'(g_i(x)) \nabla g_i(x)$
% \lz
% \item $\nabla^2 f(x) = \sum_{i} [\phi''(g_i(x)) \nabla g_i(x)] \nabla g_i(x)^T + \phi'(g_i(x)) \nabla^2g_i(x)$
% \end{itemize}
% \framebreak
%
% \textbf{Matrixform:}
% \lz
% \begin{itemize}
% \item $\nabla f(x)= \nabla g(x) \phi'(g(x))$
% \lz
% \item $\nabla^2 f(x)= \nabla g(x) \Phi''  \nabla^{\top} g(x) + \sum_i \phi'(g_i(x)) \nabla^2 g_i(x),$
% \end{itemize}
%
% mit
% $$
% \nabla g(x) = [\nabla g_1(x), \nabla g_2(x), \ldots, \nabla g_n(x)] \text{ (Jacobi-Matrix)},
% $$
% $$
% \phi'(g(x)) = [\phi'(g_1(x)), \ldots, \phi'(g_n(x))]^{\top},
% $$
% $$
% \Phi'' = \diag(\phi''(g_1(x)), \ldots, \phi''(g_n(x)))
% $$
%
% \begin{footnotesize}
% Wegen Blockmatrix-Produkt:
% $$
%   [\nabla g_1(x), \ldots, \nabla g_n(x)] \Phi'' [\nabla g_1(x), \ldots, \nabla g_n(x)]^{\top}
%    = \sum_i \Phi'' \nabla g_i(x) \nabla^{\top} g_i(x)
% $$
% \end{footnotesize}
%
% \framebreak
%
% \textbf{Im Least Squares Fall:} \\
% \lz
% \begin{itemize}
% \item Penalty: $\phi(t) = \frac{1}{2} t^2$,  $\phi'(t) =t$, $\phi''(t) =1$, $\Phi''= I$
% \lz
% \item Setze in Formeln von vorheriger Slide ein:
% \begin{itemize}
% \lz
% \item $\nabla f(x) = \nabla g(x) g(x)$
% \lz
% \item $\nabla^2 f(x) = \nabla g(x) \nabla^{\top}g(x) + \sum_{i} g_i(x)\nabla^2 g_i(x)$
% \end{itemize}
% \end{itemize}

% \framebreak
%
%
%
% \textbf{Idee Gauss-Newton Methode:} \\
% \begin{itemize}
% \item Was passiert mit $\nabla^2 f(x)$ wenn nahe an Minimum und damit $g_i$ klein?
% $$
% \nabla^2 f(x)  \approx \nabla g(x) (\nabla g(x))^{\top} = H(x),
% $$
% H(x) positiv (semi-)definit.
%
% \item Gauss-Newton ähnlich Newton Methode:
% $$
% x^{(i+1)} =  x^{(i)} - \lambda H^{-1}(x^{(i)}) \nabla g(x^{(i)}) g(x^{(i)}),
% $$
% mit $\lambda$ Schrittweite und $\nabla g(x^{(i)}) g(x^{(i)})$ Gradient von f an der Stelle $x^{(i)}$.
% \end{itemize}
\begin{vbframe}{Levenberg-Marquardt algorithm}

If $\nabla r(\bm{\theta}^{[t]})^\top\nabla r(\bm{\theta}^{[t]})$ singular, use $\nabla r(\bm{\theta}^{[t]})^\top\nabla r(\bm{\theta}^{[t]})+\Delta$ with $\Delta$ non-negative diagonal matrix.
\lz
$$
\Delta = \epsilon \cdot I
$$
or
$$
\Delta = \epsilon \cdot \diag\left(\nabla r(\bm{\theta}^{[t]})^\top\nabla r(\bm{\theta}^{[t]})\right)
$$

LMA is an efficient and popular method for solving nonlinear optimization problems.

\lz

Note: The diag elements of a pd matrix are always $\geq 0$

\end{vbframe}

\endlecture
\end{document}


